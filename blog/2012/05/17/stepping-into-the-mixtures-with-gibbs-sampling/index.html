
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Stepping into the Mixtures with Gibbs Sampling - Wordsi by Fozzie The Beat</title>
  <meta name="author" content="Keith Stevens">

  
  <meta name="description" content="Refresher As stated before here and elsewhere, mixture models are pretty cool.
But rather than diving head first into a complicated mixture model &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://fozziethebeat.github.com/blog/2012/05/17/stepping-into-the-mixtures-with-gibbs-sampling">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="Wordsi by Fozzie The Beat" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href='http://fonts.googleapis.com/css?family=Voces' rel='stylesheet' type='text/css'>

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-315689840']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>



  <script type="text/javascript"
          src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Wordsi by Fozzie The Beat</a></h1>
  
    <h2>A blog about computational semantics, life, and anything else.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:fozziethebeat.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
  <li><a href="/papers.html">Research</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Stepping Into the Mixtures With Gibbs Sampling</h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-05-17T16:07:00-07:00" pubdate data-updated="true">May 17<span>th</span>, 2012</time>
        
      </p>
    
  </header>


<div class="entry-content"><script src="/javascripts/raphael-min.js"></script>


<script src="/javascripts/raphael-extra.js"></script>


<script src="/javascripts/graphing-resize.js"></script>


<h3>Refresher</h3>

<p><img class="right" src="/images/empty_pool.jpg" width="300" height="600" title="Empty Pools = Not fun for diving" >
As stated before <a href="blog/2012/05/04/getting-to-the-mixtures-in-dirichlet-process-mixture-models/">here</a> and <a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">elsewhere</a>, mixture models are pretty cool.
But rather than diving head first into a complicated mixture model like the
<a href="http://en.wikipedia.org/wiki/Dirichlet_process">Dirichlet Process Mixture Model</a> or the <a href="http://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process">Hierarchical Dirichlet Process</a>,
we&#8217;re going to ease our way into these models, just like you&#8217;d learn to accept
the horrors of a pool of water before you try deep sea diving.  First, we&#8217;ll
figure out how to apply the <a href="http://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs Sampling Method</a> with the <a href="http://en.wikipedia.org/wiki/Linked_list">Linked List</a>
of machine learning methods, <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a>.  Then, after <a href="http://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs Sampling</a>
feels comfy, we&#8217;ll tackle a <a href="http://en.wikipedia.org/wiki/Mixture_model">Finite Mixture Model</a> with <a href="http://en.wikipedia.org/wiki/Multivariate_normal_distribution">Gaussian</a>
components.  And with the magic of <a href="https://github.com/scalala/Scalala">Scalala</a> and <a href="http://www.scalanlp.org/">Scalanlp</a> the code&#8217;ll be
short, sweet, and easy to understand.</p>

<h3>The Toy of Machine Learning Methods</h3>

<p><img class="left" src="/images/Single_linked_list.png" width="400" height="200" title="Linked Lists = Tool for Learning" >
Simple but instructive data structures like a <a href="http://en.wikipedia.org/wiki/Linked_list">Linked List</a> work well as an
instructive tool.  Good libraries provide solid and efficient implementations,
but it&#8217;s straightforward enough for beginners to implement in a variety of
manners.  For our goals, <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a> will serve the same purpose.  As simple
as the model is, it&#8217;s reasonably powerful.</p>

<p>So what is the <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a> model?  Let&#8217;s say we want to group a bundle of
emails into two groups: spam and not spam.  And all you have to go on are the
contents of the emails.  In this situation, we&#8217;d like to just use the words in a
single email to decide whether or not it&#8217;s spamtastic or spamfree.  <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a>
assumes that you can make this decision based on each word in the
email individually and then combine these decisions.  It&#8217;s <a href="http://en.wikipedia.org/wiki/Bayes'_rule">Bayesian</a>
because it&#8217;s a simple <a href="http://en.wikipedia.org/wiki/Bayesian_network">Bayesian Model</a> and it&#8217;s Naive because this approach
assumes the words are totally independent and have no statistical relation
whatsoever.</p>

<h3>Model Representations</h3>

<p>Due to it&#8217;s simplicity, and that naive assumption, Naive Bayes makes for an easy
model to understand and describe using a variety of models.  here&#8217;s some of the
most frequent descriptions:</p>

<h4>A Bayesian Network</h4>

<table>
<td>
<div id='naivebayes'></div>
<script type='text/javascript'>
(function() {

  function draw() {
        var paper = Raphael('naivebayes', 200, 300);
        paper.circle(60, 20, 20);
        paper.text(60, 20, "α")
             .attr({"font-size": 25});

        paper.arrow(60, 40, 60, 80, 10);

        paper.rect(10,60, 100, 150);

        paper.circle(60, 100, 20);
        paper.text(60, 100, "K")
             .attr({"font-size": 25});


        paper.circle(150, 100, 20);
        paper.text(150, 100, "γ")
             .attr({"font-size": 25});

        paper.arrow(150, 120, 150, 150, 10);

        paper.rect(120, 140, 60, 60);
        paper.circle(150, 170, 20);
        paper.text(150, 170, "θ")
             .attr({"font-size": 25});

        paper.arrow(130, 170, 80, 170, 10);

        paper.rect(29, 140, 60, 60);

        paper.arrow(60, 120, 60, 150, 10);

        paper.circle(60, 170, 20);
        paper.text(60, 170, "X")
             .attr({"font-size": 25});
    }

    draw();

    $(window).resize(function() {
        draw();
    });

})();
</script>
</td>
<td>

<script type="math/tex; mode=display">
\begin{array}{lll}
\phi & \sim & Dirichlet(\alpha) \\
\theta_{s} & \sim & Dirichlet(\gamma_s) \\
\theta_{n} & \sim & Dirichlet(\gamma_n) \\
K_{j} & \sim & Multinomial(\phi) \\
X_j & \sim & Multinomial(\theta_{K_{j}}) 
\end{array}
</script>

</td>
</table>


<p>These two views of Naive Bayes describe the same model, but different ways
of computing it.  For the model on the right, if we work backwards, it says that
a data point <script type="math/tex"> X_j </script> comes from some multinomial distribution <script type="math/tex"> \theta_{K_j} </script>.  <script type="math/tex"> K_j </script> is a latent variable that encode which
group, i.e. spam or not spam, the data point came from, and that itself comes
from a multinomial <script type="math/tex"> \phi </script>.  The two <script type="math/tex"> \theta_s, \theta_n </script> distributions describe our knowledge about spam and non-spam emails,
respectively.  And finally, all three multinomial distributions come from
<a href="http://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet</a> distributions.  The added magic of the <a href="http://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet</a>
distrubtion is that it <strong>generates</strong> multinomial distributions, this
relationship is more formally known as a <a href="http://en.wikipedia.org/wiki/Conjugate_prior">Conjugate Prior</a>.  The plate
diagram on the left says <strong>nearly</strong> the same story except it <em>collapses</em> out the <script type="math/tex"> \phi </script> distribution and links <script type="math/tex"> \alpha </script> directly to
the compnent labels.  We could also in theory collapse the <script type="math/tex"> \theta </script> distrubtions, but we&#8217;ll not do that
for the moment.</p>

<h4>A Graphical Model</h4>

<table>
<td>
<div id="nb-graph"></div>
</td>
<td>
<div id="nb-factor"></div>
</td>
</table>




<script type='text/javascript'>
(function() {

  function draw() {
        var paper = Raphael('nb-graph', 200, 200);

        paper.circle(100, 30, 20);
        paper.text(100, 30, "Y")
             .attr({"font-size": 20});

        paper.circle(30, 160, 20);
        paper.text(30, 160, "w_1")
             .attr({"font-size": 20});

        paper.circle(80, 160, 20);
        paper.text(80, 160, "w_2")
             .attr({"font-size": 20});

        paper.text(120, 160, "...")
             .attr({"font-size": 25});

        paper.circle(170, 160, 20);
        paper.text(170, 160, "w_v")
             .attr({"font-size": 20});

        paper.arrow(100, 50, 30, 140, 10);
        paper.arrow(100, 50, 80, 140, 10);
        paper.arrow(100, 50, 170, 140, 10);
    }

    draw();

    $(window).resize(function() {
        draw();
    });

})();
</script>




<script type='text/javascript'>
(function() {

  function draw() {
        var paper = Raphael('nb-fail', 200, 200);

        paper.circle(100, 30, 20);
        paper.text(100, 30, "Y")
             .attr({"font-size": 20});

        paper.circle(30, 160, 20);
        paper.text(30, 160, "w_1")
             .attr({"font-size": 20});

        paper.circle(80, 160, 20);
        paper.text(80, 160, "w_2")
             .attr({"font-size": 20});

        paper.text(120, 160, "...")
             .attr({"font-size": 25});

        paper.circle(170, 160, 20);
        paper.text(170, 160, "w_v")
             .attr({"font-size": 20});

        paper.factor(100, 50, 30, 140, 10);
        paper.factor(100, 50, 80, 140, 10);
        paper.factor(100, 50, 170, 140, 10);
    }

    draw();

    $(window).resize(function() {
        draw();
    });

})();
</script>


<p>This graphical view tells an alternative story.  Instead of probability
distrubutions making other distributions and then finally making data points.
this says that the class of a data point, <script type="math/tex"> Y </script> generates the
features of the data point, <script type="math/tex">, w_1, w_2, \cdots, w_v </script>.  This is
where the naive part comes from, each &#8220;feature&#8221;, i.e. word in the email, is
completely disconnected from the other words and only depends on the class of
the email.</p>

<p><img class="right" src="/images/thomas_bayes.gif" title="Thomas Bayes, not so Naive" ></p>

<h4>A Probability Distribution</h4>

<p>Finally, there&#8217;s the written way to understand Naive Bayes, as a conditional
probability distrubtion.  Let&#8217;s write out the distribution for just the spam
class:</p>

<script type="math/tex; mode=display">
p(spam | email) \propto p(spam) \prod_{word \in email} p(word | spam)
</script>


<p>This looks pretty much the same for non-spam.  Simple, no? If we combine
together the different views above with this written form, then we can think of
the class <script type="math/tex">spam</script> as a <a href="http://en.wikipedia.org/wiki/Multinomial_distribution">Multinomial</a> distribution over words and the
probability of a word given the class spam is really just the probability of
picking the word based on how many times it&#8217;s shown up in spammy emails.
Similarly, the probability of seeing spam is just the number of times you&#8217;ve
seen a delcious piece of spam, be it on your phone, at home, or in the frying
pan.  Using that intuition and some Bayesian math, the above definition gets
boild down to:</p>

<script type="math/tex; mode=display">
p(s|e) \propto \frac{C_{s} + \alpha_s - 1}{N + \alpha_s + \alpha_n- 1}
                \prod_{w \in e} \theta_{s,w}^{C_{w,e}}
</script>


<p>Let&#8217;s get some definitions out of the way.</p>

<script type="math/tex; mode=display">
\begin{array}{lll}
C_s &=& \text{number of times spam's been seen} \\
C_{w,e} &=& \text{number of times seeing word w in email e} \\
N &=& \text{number of data points seen} \\
\alpha_{s,n} &=& \text{priors for each class} \\
\theta_{s,w} &=& \text{the wight given to word w in class spam} \\
\end{array}
</script>


<p>Let&#8217;s tease this appart into two pieces, <script type="math/tex"> p(spam) </script> and <script type="math/tex"> p(email|spam) </script>.  The first part is:</p>

<script type="math/tex; mode=display">
p(s) \propto \frac{C_{s} + \alpha_s - 1}{N + \alpha_s + \alpha_n- 1}
</script>


<p>This basically says that the probability of spam is the number of time&#8217;s we&#8217;ve
seen it, plus some smoothing factors so that both span and non-spam have some
positive possibility of existing.  Amazingly! this is also equivalent to the
<em>collapsed</em> <script type="math/tex">\phi</script> distribution we mentioned in the bayesian
description of Naive Bayes, by doing just a little bit of math and realizing
that the Dirichlet is a conjugate prior of a multinomial, we get the above
distribution for each class.</p>

<p>Next,</p>

<script type="math/tex; mode=display">
p(email|s) = \prod_{w \in e} \theta_{s,w}^{C_{w,e}}
</script>


<p>If <script type="math/tex">\theta_s</script> describes the probability of each word occuring in a
spammy email, then <script type="math/tex">\theta_{w,s}<sup>{C_{w,e}}</script></sup> becomes the likelihood of
seeing that word as many times as observed in the email.  By computing the
product of all these words, we <em>naively</em> get the probability of the full email.
Slam the two distributions together and we get the full likelihood of the email
given the class spam.</p>

<h3>Don&#8217;t forget Gibbs!</h3>

<p>Now that Naive Bayes makes some more sense in a variety of descriptive flavours
<img class="right" src="/images/Josiah_Willard_Gibbs_-from_MMS-.jpg" width="300" height="600" title="The man who made sampling cool before HipHop" >
and we&#8217;ve defined our distrubutions and hyper parameters, it&#8217;s time to throw in
gibbs sampling.  To quickly refresh our memories, the recipe for gibbs at this
stage is:</p>

<ol>
<li>for each data point <script type="math/tex"> X_j </script>

<ol>
<li>Forget about <script type="math/tex"> X_j </script></li>
<li>Compute the likelihood that each component, <script type="math/tex">\theta_*</script>, generated <script type="math/tex">X_j</script></li>
<li>Sample and jot down a new component, <script type="math/tex">K_j</script>, for <script type="math/tex">X_J</script></li>
<li>Remember <script type="math/tex"> X_J </script></li>
</ol>
</li>
<li>Restimate our component parameters, <script type="math/tex">\theta_{*} </script></li>
<li>Repeat</li>
</ol>


<p>You might be wondering &#8220;why forget about the current data point?&#8221;  The reason is
that we&#8217;re trying to decide what to do with a <strong>new data point</strong>.  Gibbs
sampling only really works because the distrubtions we&#8217;re working with are
<a href="http://en.wikipedia.org/wiki/Exchangeable_random_variables">exchangeable</a>, i.e. the ordering of our data doesn&#8217;t matter.  So we can
just pretend any data point is new by just forgetting it ever existed, Note,
this is why <script type="math/tex">p(spam)</script> has a <script type="math/tex">-1</script> in the denominator, that count
has been &#8220;forgotten&#8221;.  Other than that tidbit, we just pretend everything else
is the same.</p>

<h3>Writing out the code</h3>

<p><img class="left" src="/images/spam_sampling.jpg" width="150" height="150" title="Tasty Samples of Spam" >
With Gibbs sampling fresh in our minds, and our probability distributions clear,
let&#8217;s see what some real code looks like.  To do this, I&#8217;m going to use
<a href="https://github.com/scalala/Scalala">Scalala</a> and <a href="http://www.scalanlp.org/">Scalanlp</a>, which take care of the linear algebra and
probability distrubtions for us really concisely.  And while it may not be the
fastest library around, it&#8217;s super easy to read and understand.  For faster
implementations, you can just translate this code into whatever beast you desire
and optimize away.</p>

<h4>Setting up before sampling</h4>

<p>Before we go on a spam sampling binge, we gotta setup our initial groupings.
Gibbs sampling works pretty well with a random starting point, so let&#8217;s go with
that:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">def</span> <span class="n">sampleThetas</span><span class="o">(</span><span class="n">labelStats</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">DenseVectorRow</span><span class="o">[</span><span class="kt">Double</span><span class="o">]])</span> <span class="k">=</span>
</span><span class='line'>    <span class="n">labelStats</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">n_c</span> <span class="k">=&gt;</span> <span class="k">new</span> <span class="nc">Dirichlet</span><span class="o">(</span><span class="n">n_c</span><span class="o">+</span><span class="n">gamma</span><span class="o">).</span><span class="n">sample</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="k">val</span> <span class="n">k</span> <span class="k">=</span> <span class="n">numComponents</span>
</span><span class='line'><span class="k">val</span> <span class="n">v</span> <span class="k">=</span> <span class="n">numFeatures</span>
</span><span class='line'><span class="k">val</span> <span class="n">n</span> <span class="k">=</span> <span class="n">numDataPoints</span>
</span><span class='line'><span class="k">var</span> <span class="n">labelStats</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">.</span><span class="n">fill</span><span class="o">(</span><span class="n">k</span><span class="o">)(</span><span class="k">new</span> <span class="nc">DenseVectorRow</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span>
</span><span class='line'>    <span class="nc">Array</span><span class="o">.</span><span class="n">fill</span><span class="o">(</span><span class="n">v</span><span class="o">)(</span><span class="mi">0</span><span class="n">d</span><span class="o">)))</span>
</span><span class='line'><span class="k">var</span> <span class="n">thetas</span> <span class="k">=</span> <span class="n">sampleThetas</span><span class="o">(</span><span class="n">labelStats</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="k">val</span> <span class="n">labels</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Multinomial</span><span class="o">(</span><span class="k">new</span> <span class="nc">Dirichlet</span><span class="o">(</span><span class="n">alpha</span><span class="o">).</span><span class="n">sample</span><span class="o">.</span><span class="n">data</span><span class="o">).</span><span class="n">sample</span><span class="o">(</span><span class="n">n</span><span class="o">).</span><span class="n">toArray</span>
</span><span class='line'><span class="k">var</span> <span class="n">labelCounts</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DenseVectorRow</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="nc">Array</span><span class="o">.</span><span class="n">fill</span><span class="o">(</span><span class="n">k</span><span class="o">)(</span><span class="mi">0</span><span class="n">d</span><span class="o">))</span>
</span><span class='line'><span class="n">data</span><span class="o">.</span><span class="n">zip</span><span class="o">(</span><span class="n">labels</span><span class="o">).</span><span class="n">foreach</span><span class="o">{</span>
</span><span class='line'>    <span class="k">case</span> <span class="o">(</span><span class="n">x_j</span><span class="o">,</span> <span class="n">j</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">{</span><span class="n">labelStats</span><span class="o">(</span><span class="n">j</span><span class="o">)</span> <span class="o">+=</span> <span class="n">x_j</span><span class="o">;</span> <span class="n">labelCounts</span><span class="o">(</span><span class="n">j</span><span class="o">)</span> <span class="o">+=</span> <span class="mi">1</span><span class="o">}</span> <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>So what&#8217;s going on here?  Well, first, we just get some basic stats about our
data such as the size, the number of features and the number of components we
want, in this case, 2.  Then we make a data structure that will hold the number
of times we see each feature within each component, <code>labelStats</code>; the
number of times we&#8217;ve seen each component overall <code>labelCounts</code>; and the
parameters for each compnent, <code>thetas</code>.  Finally, randomly assign each point to
a random component and update the <code>labelStats</code> and <code>labelCounts</code>.   Note that we
get the <code>thetas</code> from a Dirichlet distribution that uses the <code>labelStats</code> and a
smoothing parameter <code>gamma</code>, just like we noted in the Bayesian Plate diagram
above.</p>

<p>Now for the real sampling meat:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">for</span> <span class="o">(</span><span class="n">i</span> <span class="k">&lt;-</span> <span class="mi">0</span> <span class="n">until</span> <span class="n">numIterations</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">for</span> <span class="o">(</span> <span class="o">(</span><span class="n">x_j</span><span class="o">,</span> <span class="n">j</span><span class="o">)</span> <span class="k">&lt;-</span> <span class="n">data</span><span class="o">.</span><span class="n">zipWithIndex</span> <span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">label</span> <span class="k">=</span> <span class="n">labels</span><span class="o">(</span><span class="n">j</span><span class="o">)</span>
</span><span class='line'>        <span class="n">labelStats</span><span class="o">(</span><span class="n">label</span><span class="o">)</span> <span class="o">-=</span> <span class="n">x_j</span>
</span><span class='line'>        <span class="n">labelCounts</span><span class="o">(</span><span class="n">label</span><span class="o">)</span> <span class="o">-=</span> <span class="mi">1</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">val</span> <span class="n">prior</span> <span class="k">=</span> <span class="n">labelCounts</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="o">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="o">+</span><span class="n">alpha</span><span class="o">.</span><span class="n">sum</span><span class="o">)</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">likelihood</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DenseVectorRow</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="n">thetas</span><span class="o">.</span><span class="n">map</span><span class="o">(</span> <span class="n">theta</span> <span class="k">=&gt;</span>
</span><span class='line'>            <span class="o">(</span><span class="n">theta</span> <span class="o">:^</span> <span class="n">x_j</span><span class="o">).</span><span class="n">iterator</span><span class="o">.</span><span class="n">product</span><span class="o">).</span><span class="n">toArray</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">val</span> <span class="n">probs</span> <span class="k">=</span> <span class="n">prior</span> <span class="o">:*</span> <span class="n">likelihood</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">new_label</span><span class="k">=</span> <span class="k">new</span> <span class="nc">Multinomial</span><span class="o">(</span><span class="n">probs</span> <span class="o">/</span> <span class="n">probs</span><span class="o">.</span><span class="n">sum</span><span class="o">).</span><span class="n">sample</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">labelCounts</span><span class="o">(</span><span class="n">new_label</span><span class="o">)</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class='line'>        <span class="n">labelStats</span><span class="o">(</span><span class="n">new_label</span><span class="o">)</span> <span class="o">+=</span> <span class="n">x_j</span>
</span><span class='line'>        <span class="n">labels</span><span class="o">(</span><span class="n">j</span><span class="o">)</span> <span class="k">=</span> <span class="n">new_label</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="n">thetas</span> <span class="k">=</span> <span class="n">sampleThetas</span><span class="o">(</span><span class="n">labelStats</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The first three lines in the loop get the current label for <code>x_j</code> and then
forget about it&#8217;s data.  <code>prior</code> represents the likelihood of selecting each
component just based on it&#8217;s frequency, i.e. <script type="math/tex">p(spam)</script>.  <code>likelihood</code>
represents the likelihood of each component generating <code>x_j</code>, i.e. <script type="math/tex"> p(e|s)</script>.
To finish off the sampling procedure, we sample a new label by turning these
likelihoods into a multinomial distrubtion and sample from it.  With the new
label we then add in the data for <code>x_j</code> to the that component&#8217;s stats.  Once
we&#8217;ve made a full sweep through the data set, we update our parmeters for each
component by resampling from a Dirichlet distribution by using the our
<code>labelStats</code>.</p>

<p>And Voila! We have a simple gibbs sampler for Naive Bayes.</p>

<h3>Running it on some data</h3>

<p>Let&#8217;s see how this unsupervised version of Naive Bayes handles a really generic
looking dataset: globs of data points from three different Gaussian
distributions, each with a different mean and variance.  And let&#8217;s start off
with just trying to find 2 groups, as we&#8217;ve been working along with so far.</p>

<p><img class="right" src="/images/test.nb.groups.png" title="Splitting apart some simple groups" ></p>

<p>Looks pretty neat huh?  Even with a terrible and completely random starting
point, our learner manages to split up the data in a sensible way by finding a
dividing plane between two groups.  What&#8217;s even cooler, is that notice how after
iteration 1 completes, nearly all the data is assigned to one class.  By somehow
by iteration 10, we&#8217;ve managed to finagle our way out of that terrible solution
and into a <strong>significantly</strong> better solution.  And by the 100th iteration, we&#8217;ve
stayed there in a stable state.</p>

<p>Now what happens if we try finding 3 components in the data set?</p>

<p><img class="right" src="/images/test.nb.3.groups.png" title="Splitting apart some simple groups" ></p>

<p>Here we get a similar split, but not what we&#8217;re looking for.  Our mixture model
gets all befuddled with the group on the bottom left and tries to flailingly
split into two smaller groups.  If you run this a couple times, you&#8217;ll see that
it does the same thing with the two green groups, it can&#8217;t split them evenly as
you&#8217;d expect.</p>

<h3>The Secret no one mentions</h3>

<p>This simple model i&#8217;ve walked through is the unsupervised equivalent to the
supervised Naive Bayes model.  It does a pretty reasonable job of splitting
apart some groups of data, but there are clearly datasets it has some issues
dealing with.  But what&#8217;s not been said so far is that this model is actually
not just Naive Bayes, it&#8217;s a <a href="http://en.wikipedia.org/wiki/Mixture_model">Finite Mixture Model</a> with Multionomial
components.  The code, probabilities, and graphs i&#8217;ve layed out all work for
more than two components.  So next, i&#8217;ll show two cool ways to extend this
model:</p>

<ol>
<li>With Gaussian components, which can better handle this toy data set</li>
<li>With an infinite number of components, which figure out that paramter <strong>for
you</strong> like a <em>magician</em>.</li>
</ol>


<p><img class="center" src="/images/magician.jpg" title="Magicians figure out their own parameters" ></p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Keith Stevens</span></span>

      








  


<time datetime="2012-05-17T16:07:00-07:00" pubdate data-updated="true">May 17<span>th</span>, 2012</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/bayesian-modeling/'>bayesian modeling</a>, <a class='category' href='/blog/categories/clustering/'>clustering</a>, <a class='category' href='/blog/categories/scala/'>scala</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://fozziethebeat.github.com/blog/2012/05/17/stepping-into-the-mixtures-with-gibbs-sampling/" data-via="fozziethebeat" data-counturl="http://fozziethebeat.github.com/blog/2012/05/17/stepping-into-the-mixtures-with-gibbs-sampling/" >Tweet</a>
  
  
  <div class="g-plusone" data-size="medium"></div>
  
  
    <div class="fb-like" data-send="true" data-width="450" data-show-faces="false"></div>
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2012/05/04/getting-to-the-mixtures-in-dirichlet-process-mixture-models/" title="Previous Post: Getting to the mixtures in Dirichlet Process Mixture Models">&laquo; Getting to the mixtures in Dirichlet Process Mixture Models</a>
      
      
        <a class="basic-alignment right" href="/blog/2012/06/02/sonatype-and-scala-a-relationship-of-trickery/" title="Next Post: Sonatype and Scala: A relationship of trickery">Sonatype and Scala: A relationship of trickery &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Categories</h1>
    <ul id="category-list"><li><a href='/blog/categories/bayesian'>Bayesian (1)</a></li><li><a href='/blog/categories/bayesian-modeling'>Bayesian modeling (2)</a></li><li><a href='/blog/categories/clustering'>Clustering (3)</a></li><li><a href='/blog/categories/computational'>Computational (1)</a></li><li><a href='/blog/categories/computational-linguistics'>Computational linguistics (1)</a></li><li><a href='/blog/categories/conferences'>Conferences (1)</a></li><li><a href='/blog/categories/dilemmas'>Dilemmas (1)</a></li><li><a href='/blog/categories/experiments'>Experiments (1)</a></li><li><a href='/blog/categories/linguistics'>Linguistics (2)</a></li><li><a href='/blog/categories/maven'>Maven (1)</a></li><li><a href='/blog/categories/morality'>Morality (1)</a></li><li><a href='/blog/categories/natural-language-processing'>Natural language processing (1)</a></li><li><a href='/blog/categories/open-source'>Open source (1)</a></li><li><a href='/blog/categories/scala'>Scala (6)</a></li><li><a href='/blog/categories/statistics'>Statistics (1)</a></li><li><a href='/blog/categories/topic-models'>Topic models (1)</a></li></ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2012/07/13/acl-and-emnlp-2012-debrief/">ACL and EMNLP 2012 Debrief</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/06/25/gamma-ramma/">Gamma Ramma</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/06/04/topic-model-comparisons-how-to-replicate-an-experiment/">Topic Model Comparisons: how to replicate an experiment</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/06/02/sonatype-and-scala-a-relationship-of-trickery/">Sonatype and Scala: A relationship of trickery</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/05/17/stepping-into-the-mixtures-with-gibbs-sampling/">Stepping into the Mixtures with Gibbs Sampling</a>
      </li>
    
  </ul>
</section>

  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - Keith Stevens -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'fozziethebeat';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://fozziethebeat.github.com/blog/2012/05/17/stepping-into-the-mixtures-with-gibbs-sampling/';
        var disqus_url = 'http://fozziethebeat.github.com/blog/2012/05/17/stepping-into-the-mixtures-with-gibbs-sampling/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>



  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
