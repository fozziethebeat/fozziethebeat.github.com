<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: clustering | Wordsi by Fozzie The Beat]]></title>
  <link href="http://fozziethebeat.github.com/blog/categories/clustering/atom.xml" rel="self"/>
  <link href="http://fozziethebeat.github.com/"/>
  <updated>2012-05-04T15:07:14-07:00</updated>
  <id>http://fozziethebeat.github.com/</id>
  <author>
    <name><![CDATA[Keith Stevens]]></name>
    <email><![CDATA[fozziethebeat@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Getting to the mixtures in Dirichlet Process Mixture Models]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/05/04/getting-to-the-mixtures-in-dirichlet-process-mixture-models/"/>
    <updated>2012-05-04T13:01:00-07:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/05/04/getting-to-the-mixtures-in-dirichlet-process-mixture-models</id>
    <content type="html"><![CDATA[<p>The <a href="http://en.wikipedia.org/wiki/Dirichlet_process">Dirichlet Process</a> is pretty sweet as Edwin Chen and numerous others
have <a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">pointed out</a>.  When combined with a mixture of Bayesian models, it's
really effective for finding an accurate number of models needed to represent
your bundle of data, especially useful when you have no idea how many bundles of
related words you may have.  Edwin Chen does a really great job of
<a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">explaining</a> how the Dirichlet Process works, how to implement it, and how it
can break down the McDonald's menu, but he leaves out details on how to combine
it with mixture models, which is by far the coolest part.</p>

<p>The short description for how to merge the Dirichlet Process with <a href="http://en.wikipedia.org/wiki/Mixture_model">Nonparametric
Mixture Modeling</a> (and infer the model) looks like this:</p>

<ol>
<li>Define probability distributions for your mixture model</li>
<li>Rewrite your distributions with Bayes rule so that parameters depend on data
and hyper parameters</li>
<li>Repeat step 2 for hyper parameters as desired (it's turtles all the way down, as Kevin
Knight <a href="http://www.isi.edu/natural-language/people/bayes-with-tears.pdf">said</a>)</li>
<li>For each data point

<ol>
<li>Forget about the current data point</li>
<li>Compute the likelihood that each mixture made this data point</li>
<li>Sampling a new label for this data point using these likelihoods</li>
<li>Jot down this new label</li>
<li>Remember this data point</li>
</ol>
</li>
<li>After seeing all the data, re-estimate all your parameters based on the new
assignments</li>
<li>Repeat steps 4 and 5 ad nausea.</li>
</ol>


<p>In short, this is the <a href="http://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs Sampling</a> approach.  If you already know what
your distributions look like, and this paltry description made perfect sense,
Hurray! Please go bake a cake for everyone that feels a little underwhelmed with
this description.</p>

<p>{% img center /images/cake.jpg Delicious cake for people still reading %}</p>

<p>Now that someone is off baking us cake, let's dive down into what the
distributions look like.  Taking a look at a number of papers describing
variations of the Dirichlet Process Mixture Model, like <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.9111">The Infinite Gaussian Mixture Model</a>
and <a href="http://www.cs.princeton.edu/~blei/papers/BleiJordan2004.pdf">Variational Inference for Dirichlet Process Mixture Models</a>, they first
make things seem pretty straightforward.  To start, they generally throw down
this simple equation:</p>

<p>{% math %}
p(y|\theta_1, \cdots, \theta_k, \pi_1, \cdots, \pi_k) = \sum_j<sup>k</sup> \pi_j p(y|\theta_j)
{% endmath %}</p>

<p>Simple, no?  If you want mixtures of <a href="http://www.umiacs.umd.edu/~resnik/pubs/gibbs.pdf">Gaussian</a> models, then {%m%} \theta_j{%em%} has a mean and a variance, we'll call these {%m%} \mu_j{%em%} and {%m%}\sigma_j<sup>2{%em%}.</sup>   The next step is then to rewrite this equation in a couple of ways so that
you define the parameters, i.e. {% m %} \theta_j {% em %} in terms of the data
and other parameters.  One example: {% m %} \mu_j {% em %} looks like this:</p>

<p>{% math %}
p(\mu_j|c,y,s_j, \lambda, r) \sim \mathcal{N}\left( \frac{\bar{y}n_js_j + \lambda
r}{n_js_j + r},\frac{1}{n_js_j+r} \right)
{% endmath %}</p>

<p>Not <em>too</em> bad after reading through the definition for all the parameters; the
first bundle describes the mean of the mixture and the second bundle describes
the variance of the mixture.  You may have noticed that our means for each
mixture come from a Gaussian distribution themselves (I did say it was turtles
all the way down).  If you're using a nice math coding environment, like
<a href="https://github.com/scalala/Scalala">Scalala</a> and <a href="http://www.scalanlp.org/">Scalanlp</a>, you can easily sample from this distribution like
this</p>

<p><code>scala
val theta_j = new Gamma((y_mean*n_j*s_j + lambda*r)/(n_j*s_j+r), 1d/(n_j*s_j +r)).sample
</code></p>

<p>This goes on for most of the parameters for the model until you get to one of
the core tear inducing rewrites.  This lovely gem describes the probably
distribution function for one of the hyper parameters in the <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.9111">Infinite Gaussian Mixture Model</a>:</p>

<p>{% math %}
p(\alpha|k,n) \propto \frac{\alpha<sup>{k-3/2}</sup> exp(-1/(2\alpha))\Gamma(\alpha)}</p>

<pre><code>                       {\Gamma(\alpha / k)}
</code></pre>

<p>{% endmath %}</p>

<p><em>This is not easy to sample from</em> especially if you're new to sophisticated
sampling methods.  And sadly, just about every academic publication describing
these kinds of models gets to a point where they assume you know what they're
talking about and can throw down <a href="http://en.wikipedia.org/wiki/Mixture_model">Monte Carlo Markov Chain</a> sampling
procedures like they're rice at a wedding.</p>

<p>{% img center /images/wedding-rice.jpg MCMC sampling methods for the newlyweds %}</p>

<p>So what's a lowly non-statistician data modeller to do? Start with a well
written description of a similar, but significantly simpler model.  In this
case, <a href="http://www.umiacs.umd.edu/~resnik/pubs/gibbs.pdf">Gibbs Sampling for the Uninitiated</a>, which describes how to apply
Gibbs to the Naive Bayes model.  I'll summarize this awesome paper in the next
part of this series and then tweak it a little to make a <a href="http://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model">Finite Gaussian Mixture Model</a>.<br/>
After that's said and done, I'll show how to extend the finite version into the
Dirichlet Process Mixture Model (with some simplifications).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Making Agglomerative Clustering run in N^2 Time]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/03/16/making-agglomerative-clustering-run-in-n-2-time/"/>
    <updated>2012-03-16T17:21:00-07:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/03/16/making-agglomerative-clustering-run-in-n-2-time</id>
    <content type="html"><![CDATA[<p>Let's say you're an amateur zoologist and you've got a bunch of data describing
<a href="http://en.wikipedia.org/wiki/Owl">parliaments of owls</a>, <a href="http://en.wikipedia.org/wiki/Goose">gaggles of geese</a>, <a href="http://en.wikipedia.org/wiki/Chicken">peeps of chickens</a>, and a
<a href="http://en.wikipedia.org/wiki/Jackdaw">train of jackdaws</a> but you don't really know that you have these bird types.
All you really have are descriptive features describing each bird like feature
type, beak type, conservation status, feeding preferences, etc.   Using just
this data, you'd like to find out how many bird species you have and how similar
each group is to the others in a nice graphical fashion like down below.  How
would you do it?</p>

<p>{% img center /images/circularDendrogram.png %}</p>

<p>The classic solution to the problem would be to use <a href="http://en.wikipedia.org/wiki/Hierarchical_clustering">Hierarchical Agglomerative
Clustering</a>.  In this case, Agglomerative Clustering would group together
birds that have similar features into hopefully distinct species of birds.  And
there's a lot of packages out there that kind of do this for you like <a href="http://www.cs.waikato.ac.nz/ml/weka/">Weka</a>,
<a href="http://scikit-learn.org/stable/">Scikit-Learn</a>, or plain old <a href="http://www.statmethods.net/advstats/cluster.html">R</a>.  However, you've got a <em><em>lot</em></em> of birds
to deal with and these standard packages are just taking <em><em>way</em></em> too long.  Why
are they taking way too long?  Because they're doing agglomerative clustering
the slow ways.</p>

<p>So what do they slow ways look like?  Well, pretty much all ways of doing
agglomerative clustering start by building an Affinity Matrix that simply
measures how similar two birds are to each other:</p>

<p>``` scala Building the Affinity Matrix</p>

<pre><code>val m = Array.fill(numBirds, numBirds)(0)
for ((bird1, i) &lt;- birds.zipWithIndex;
     (bird2, j) &lt;- birds.zipWithIndex)
    m(i)(j) = similarityBetween(bird1, bird2)
</code></pre>

<p>```</p>

<p>This little snippet just compares each bird against all other birds and stores
how "similar" they are to each other in terms of their descriptive features.
What next?  Well, you need another data structure to keep track of your bird
groups.  We'll do this with just a map from group identifiers to sets of bird
identifiers.  And since this algorithm is named ``agglomerative'', we gotta
agglomerate things, so we'll start by putting every bird in their own bird
group:</p>

<p>``` scala Starting off the bird groups</p>

<pre><code>// First get each bird and it's id, then create a tuple holding the bird id
// and a set with the bird as the only element.  Then turn this list of
// tuples into a map.
var groupMap = birds.zipWithIndex.map( birdIndex =&gt; (birdIndex._2, Set(birdIndex._1))).toMap
</code></pre>

<p>```</p>

<p>So that was simple.  Now comes the complicated bits that either lead you to a
slow version, a fairly slow version, or a <em>super</em> slow version of agglomerative
clustering.  Before we do either of these options, let's make two
simplifications: let's assume we just want the final groups that our bird show
up in and we know how many bird groups we want to find.  Agglomerative
Clustering can give you not only this information, but a whole tree showing how
birds are linked together, but the book-keeping for doing this is tricky and
doesn't really affect the issues we're focusing on here.  With that out of the
way, what does the <em>super</em> slow method look like?</p>

<p>``` scala The Super Slow Agglomerative Clustering</p>

<pre><code>while (groupMap.size &gt; numClusters) {
    val groupSet = groupMap.toSet
    // Find the two most similar groups in our map.
    var bestSim = 0.0
    var bestGroups = (-1, -1)
    for (Array((id1, birds1), (id2, birds2)) &lt;- groupSet.subsets(2).map(_.toArray)) {
        // Get the similarity between the two bird groups using the raw
        // values in m.
        val sim = groupSim(id1, birds1, id2, birds2, m)
        if (sim &gt; bestSim) {
            bestSim = sim
            bestGroups = (id1, id2)
        }
    }

    // Now merge the two groups together into a new group
    val newGroup = groupMap(bestGroups._1) ++ groupMap(bestGroups._2)
    // Now remove the two groups from the map
    groupMap = groupMap - bestGroups._1
    groupMap = groupMap - bestGroups._2
    // Update the map to store the new group.  Since the old id's are
    // removed, we can just re-use one of them without any change.
    groupMap = groupMap ++ Map((bestGroups._1 -&gt; newGroup))
}
</code></pre>

<p>```</p>

<p>That's it!  It's <em>super</em> simple and <em>super</em> slow.  Why is it so slow?  Well, in
general, you'll do the big while loop <code>O(N)</code> times, where N is your number of
birds.  Then the next block of code compares each possible pairing of bird
groups.  Since in general the number of groups is proportional to the number of
birds, this will be <code>O(N^2)</code> comparisons.  Throw these two bits together
and you get a runtime of <code>O(N3)</code>!  When you have 100,000 birds,
<em>that's</em> <strong>super</strong> slow.</p>

<p>So what can you do to hasten that up?  Well, the main goal of the big loop
through pairs of groups is to find the most <em>similar</em> pair of bird groups, so an
obvious choice would be to create a priority queue, so let's see how that looks:</p>

<p>``` scala Agglomerative Clustering, Priority Queue Style</p>

<pre><code>// Create a new priority queue that tracks the similarity of two groups and
// their id's
val groupQueue = new PriorityQueue[(double, int, int)]()
val groupSet = groupMap.toSet
for (Array((id1, birds1), (id2, birds2)) &lt;- groupMap.subsets(2).map(_.toArray)) {
    // Get the similarity between the two bird groups using the raw
    // values in m.
    val sim = groupSim(id1, birds1, id2, birds2, m)
    groupQueue.enque((sim, id1, id2))
}

var nextId = groupMap.size
while (groupMap.size &gt; numClusters) {
    var best = (0, -1, -1)
    do {
        best = p.dequeue
    } while (groupMap.contains(best._2) &amp;&amp; groupMap.contains(best._3))

    // Now merge the two groups together into a new group
    val newGroup = groupMap(best._1) ++ groupMap(best._2)
    // Now remove the two groups from the map
    groupMap = groupMap - best._1
    groupMap = groupMap - best._2

    // Create a new id for this group.
    val newId = nextId

    // Next, add in the similarity between the new group and all existing
    // groups to the queue.
    for ( (id, group) &lt;- groupMap )
        groupQueue.enqueue((groupSim(newId, newGroup, id, group, m),
                            newId, id))

    // Finally, update the map to store the new group.
    nextId += 1
    groupMap = groupMap ++ (newId-&gt; newGroup)
}
</code></pre>

<p><code>`
Now this approach is definitely _faster_ but it's also horribly memory
inefficient.  Even if you have an array based priority queue that doesn't
allocate any extra memory for each entry other than the tuple being stored, this
approach has one major problem: after every merge step</code>O(N)`` elements
immediately become invalidated.  Since the two merged groups no longer exist,
any comparison between them and other groups is moot.  However the priority
queue doesn't easily permit removing them so they just float around.  This is
precisely why lines 14-16 are in a do while loop that ensures the returned
groups exist.  It's highly likely that they wont.  I think we can do better,
both in terms of speed and in terms of memory.  So let's blaze through a faster
method in couple of steps.</p>

<p>First, we need to change the setup of the algorithm.  The previous two method
just depended on some simple data structures.  This approach requires two
additional structures for bookkeeping: a way to track chains of nearest
neighbors and a way to track the set of clusters not already a part of the
chain.</p>

<p>``` scala Agglomerative Clustering, Blazingly Fast Style Step 1</p>

<pre><code>// A mapping from cluster id's to their point sets.
val clusterMap = new HashMap[Int, HashSet[Int]]()

// A set of clusters to be considered for merging.
val remaining = new HashSet[Int]()

// Create a cluster for every data point and add it to the cluster map
// and to the examine set.
for (r &lt;- 0 until adj.size) {
    remaining.add(r)
    clusterMap(r) = HashSet(r)
}

// Create a stack to represent the nearest neighbor.  The real source of
// matic
val chain = new Stack[(Double, Int)]()

// Add in a random node from remaining to start the neighbor chain.
initializeChain(chain, remaining);
</code></pre>

<p>```</p>

<p>The <code>clusterMap</code> structure replaces our <code>birdMap</code> but does the same thing, but
the remaining set and the chain stack hold the crux of this approach.  Instead
of trying to find the best link between two clusters, we're going to depend on
chains of nearest neighbors, so the first node in the chain could be anything,
but the second node is simply the nearest neighbor to the first node.  To add
the third node, we find the nearest neighbor out of any nodes not in the chain.
We'll keep doing this until two nodes in the chain represent reciprocal nearest
neighbors, that is two nodes that are most similar to each other, and no other
nodes in or outside of the chain.  Upon finding these two nodes, we merge them,
immediately.  Then we just repeat the process until we have the desired number
of clusters.  In scala, this turns out to be pretty simple to do:</p>

<p>``` scala Agglomerative Clustering, Blazingly Fast Style Step 2</p>

<pre><code>// Find the nearest neighbors and merge as soon as recursive nearest
// neighbors are found.
while (clusterMap.size &gt; numClusters) {
    // Get the last link in the chain.
    val (parentSim, current) = chain.top

    // Find the nearest neighbor using the clusters not in the chain
    // already.
    val (linkSim, next) = findBest(remaining, adj, current)

    // Check the similarity for the best neighbor and compare it to that of
    // the current node in the chain.  If the neighbor sim is larger, then
    // the current node and it's parent aren't RNNs.  Otherwise, the current
    // node is RNNs with it's parent.
    if (linkSim &gt; parentSim) {
        // The current node to push is more similar to the last node in the
        // chain, so the last node and the next to last nodes can't be
        // reciprocal nearest neighbors.
        chain.push((linkSim, next))
        remaining.remove(next)
    } else {
        // The current node is less similar to the last node than the last
        // node is to it's predecesor in the chain, so the last two nodes in
        // the chain are just the kind of nodes we're looking to merge.

        // Pop the current node from the top. 
        chain.pop
        // Pop the parent of the best node.
        val (_, parent) = chain.pop

        // These are the two nodes we'll be merging.  The node we
        // found above is left in the remaining set and is essentially
        // forgotten.

        // Remove the current and parent clusters from the cluster map
        // and extract the sizes.
        val (c1Points, c1Size) = removeCluster(clusterMap, current)
        val (c2Points, c2Size) = removeCluster(clusterMap, parent)
        val total = c1Size + c2Size

        // Update the similarity between the new merged cluster and all
        // other existing clusters.  
        for (key &lt;- clusterMap.keys) 
            adj(current)(key) = updatedSimilarity(
                adj, current, c1Size, parent, c2Size, key)

        // Replace the mapping from current to now point to the merged
        // cluster and add current back into the set of remaining
        // clusters so that it's compared to nodes in the chain.
        clusterMap(current) = c1Points ++ c2Points
        remaining.add(current)

        // If the chain is now empty, re-initialize it.
        if (chain.size == 0)
            initializeChain(chain, remaining)
    }
}
</code></pre>

<p>```</p>

<p>And that's it!  By focusing on reciprocal nearest neighbors, the algorithm
merges together clusters that will always be merged, no matter how you find
them.  Furthermore, it's remarkably easy and fast to find these nodes.  By
building the chain, the number of things that can go on the chain gets smaller
and smaller.</p>

<p>Oh, but there's one other magic trick to making this super fast, and it depends
on how you compute <code>updateSimilarity</code>.  The silly way to do it would be to
traverse all the pairings between nodes in the new cluster and the each other
remaining cluster, but that in itself gets really slow as the clusters grow.
But rather than doing that, <a href="http://onlinelibrary.wiley.com/doi/10.1002/widm.53/full">these folks</a> suggest some recurrence relations
that can be computed in <em>constant time</em>, for any of the existing agglomerative
criteria methods.  Crazy right?  But just crazy enough to work correctly and be
too fast to believe.</p>
]]></content>
  </entry>
  
</feed>
