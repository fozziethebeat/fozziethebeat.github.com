<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: research | Wordsi by Fozzie The Beat]]></title>
  <link href="http://fozziethebeat.github.com/blog/categories/research/atom.xml" rel="self"/>
  <link href="http://fozziethebeat.github.com/"/>
  <updated>2012-09-12T23:07:16-07:00</updated>
  <id>http://fozziethebeat.github.com/</id>
  <author>
    <name><![CDATA[Keith Stevens]]></name>
    <email><![CDATA[fozziethebeat@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Papers I've really enjoyed reading]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/09/12/papers-ive-really-enjoyed-reading/"/>
    <updated>2012-09-12T22:36:00-07:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/09/12/papers-ive-really-enjoyed-reading</id>
    <content type="html"><![CDATA[<p>As any good grad student would, I've read a <em>lot</em> of research papers.  And my
reading interest have swayed far and wide across the Natural Language Processing
spectrum and even expanded into more general Machine Learning methods and
Bayesian modelling.  Now that i'm transitioning from a full time grad student
focused on research to being a full time software developer at Google, I want to
just briefly jot down a list of papers that have stuck out the most for me this
last week as I transition.  So in no particular order, and in no way totally
inclusive, here they are:</p>

<ul>
<li><a href="http://www.cs.berkeley.edu/~jordan/papers/hdp.pdf">Hierarchical Dirichlet Processes</a>: This paper really helped me figure out
both how Dirichlet Processes worked, the multiple interpretations of these
models, and inference procedures.  On top of that, it introduced an even cooler
hierarchical extension to Dirichlet Processes.  A must read for non-parametric
Bayesian models.</li>
<li><a href="http://ba.stat.cmu.edu/journal/2010/vol05/issue04/carvalho.pdf">Particle Learning for General Mixtures</a>: I still don't <em>fully</em> understand
how to design good particle filters, but this paper pushed my mind in a lot of
ways and got me excited about Sequential Markov Models quite a while ago.
It's event got a cool surprise: It's written by people in Business schools!</li>
<li><a href="http://faculty.haas.berkeley.edu/eandrade/p12_LaranDaltonAndrade_JCR2010.pdf">The Curious Case of Behavioral Backlash: Why Brands Produce Priming Effects and Slogans Produce Reverse Priming Effects</a>:
I in generally love anything to do with psychical priming, so this paper
caught my attention when I came across it.  And the horribly long title says
what it's all about.  They go through a battery of experiments testing the
impact upon people created by both branding and slogans.  Their results are
kinda surprising between</li>
<li><a href="http://www.aclweb.org/anthology-new/D/D11/D11-1130.pdf">Cross-Cutting Models of Lexical Semantics</a>: This paper is both really
simple and really complicated at the same time.  They glue together two really
great ideas: Latent Dirichlet Allocation and Dirichlet Process Mixture Models
to build a nicely thought out Cross Cutting, or multiple clustering under
multiple views, model for processing text documents.  I only wish I'd thought
of this idea first.</li>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.13.7133">An Introduction to MCMC for Machine Learning</a>: If you love topic models,
Dirichlet Process Mixture Models, or any other sophisticated Bayesian model,
then this paper is a must read.  Written by some of the big hitters in MCMC
learning, they cover all the major Monte Carlo Markov Chain approaches in
sweet sweet detail.</li>
<li><a href="http://www.gatsby.ucl.ac.uk/~edward/pub/inf.mix.nips.99.pdf">The Infinite Gaussian Mixture Model</a>: Yet another paper based on Dirichlet
Process Mixture Models.  It's a totally non-parametric Bayesian clustering
model with deep layers of hierarchy to smooth out the mixtures nicely.</li>
<li><a href="http://jmlr.csail.mit.edu/proceedings/papers/v22/niu12/niu12.pdf">A Nonparametric Bayesian Model for Multiple Clusterings with Overlapping Feature Views</a>:
 This was my first introduction to cross cutting clustering models.  They take
 a <em>totally</em> different approach than the original multi-clustering approach
 and but as complicated as it looks at first, it makes a nice amount of sense
 after a while.</li>
<li><a href="http://ai.stanford.edu/~rion/papers/semtax_acl06.pdf">Semantic Taxonomy Induction from Heterogeneous Evidence</a>: Yet another
paper I wish I'd though of first.  How do you extend an ontology
automatically?  You just use the knowledge you already have encoded to train a
classifier for the kind of relationships you want to add and let it fly over
new data.  Final product? A bigger and better taxonomy with oodles of new
information.</li>
<li><a href="http://www.aclweb.org/anthology/E/E09/E09-1005.pdf">Personalizing PageRank for Word Sense Disambiguation</a>: This is one of the
earlier really great graph based and fully unsupervised Word Sense
Disambiguation papers.  It glues together the magic of PageRank and intuitions
about word senses to get a general model that's pretty hard to beat event
today.</li>
<li><a href="http://homepages.inf.ed.ac.uk/mlap/Papers/PAMI_2010_Navigli_Lapata-1.pdf">An Experimental Study of Graph Connectivity for Unsupervised Word Sense Disambiguation</a>:
Another really good paper covering all the ways you can build fully
unsupervised Word Sense Disambiguation models using graphs of word senses.
The best of the models used still get used pretty often in WSD.</li>
<li><a href="http://www.aclweb.org/anthology/P/P12/P12-1092.pdf">Improving Word Representations via Global Context and Multiple Word Prototypes</a>:
Andrew Socher is a badass.  Simple as that.  I never though I'd see a totally
legitimate and awesome lexical semantics model use a neural network, but hey
made it work beautifully.  It's a good competitor to the Cross-Cutting model
of semantics too.</li>
<li><a href="http://www.ncbi.nlm.nih.gov/pubmed/21377146">A probabilistic model of cross-categorization</a>: Probably the best
starting point for learning about cross categorization (cross-cutting) models.
It gives a good bit of theory of how they work, how to build them, and a
<em>huge</em> swath of examples of how they can be applied to approximate human
decision making.  Really impressive stuff.</li>
</ul>

]]></content>
  </entry>
  
</feed>
