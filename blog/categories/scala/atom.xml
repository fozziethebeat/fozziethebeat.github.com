<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: scala | Wordsi by Fozzie The Beat]]></title>
  <link href="http://fozziethebeat.github.com/blog/categories/scala/atom.xml" rel="self"/>
  <link href="http://fozziethebeat.github.com/"/>
  <updated>2012-05-22T14:31:42-07:00</updated>
  <id>http://fozziethebeat.github.com/</id>
  <author>
    <name><![CDATA[Keith Stevens]]></name>
    <email><![CDATA[fozziethebeat@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Stepping into the Mixtures with Gibbs Sampling]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/05/17/stepping-into-the-mixtures-with-gibbs-sampling/"/>
    <updated>2012-05-17T16:07:00-07:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/05/17/stepping-into-the-mixtures-with-gibbs-sampling</id>
    <content type="html"><![CDATA[<script src="http://fozziethebeat.github.com/javascripts/raphael-min.js"></script>


<script src="http://fozziethebeat.github.com/javascripts/raphael-extra.js"></script>


<script src="http://fozziethebeat.github.com/javascripts/graphing-resize.js"></script>


<h3>Refresher</h3>

<p><img class="right" src="/images/empty_pool.jpg" width="300" height="600" title="Empty Pools = Not fun for diving" >
As stated before <a href="blog/2012/05/04/getting-to-the-mixtures-in-dirichlet-process-mixture-models/">here</a> and <a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">elsewhere</a>, mixture models are pretty cool.
But rather than diving head first into a complicated mixture model like the
<a href="http://en.wikipedia.org/wiki/Dirichlet_process">Dirichlet Process Mixture Model</a> or the <a href="http://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process">Hierarchical Dirichlet Process</a>,
we're going to ease our way into these models, just like you'd learn to accept
the horrors of a pool of water before you try deep sea diving.  First, we'll
figure out how to apply the <a href="http://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs Sampling Method</a> with the <a href="http://en.wikipedia.org/wiki/Linked_list">Linked List</a>
of machine learning methods, <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a>.  Then, after <a href="http://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs Sampling</a>
feels comfy, we'll tackle a <a href="http://en.wikipedia.org/wiki/Mixture_model">Finite Mixture Model</a> with <a href="http://en.wikipedia.org/wiki/Multivariate_normal_distribution">Gaussian</a>
components.  And with the magic of <a href="https://github.com/scalala/Scalala">Scalala</a> and <a href="http://www.scalanlp.org/">Scalanlp</a> the code'll be
short, sweet, and easy to understand.</p>

<h3>The Toy of Machine Learning Methods</h3>

<p><img class="left" src="/images/Single_linked_list.png" width="400" height="200" title="Linked Lists = Tool for Learning" >
Simple but instructive data structures like a <a href="http://en.wikipedia.org/wiki/Linked_list">Linked List</a> work well as an
instructive tool.  Good libraries provide solid and efficient implementations,
but it's straightforward enough for beginners to implement in a variety of
manners.  For our goals, <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a> will serve the same purpose.  As simple
as the model is, it's reasonably powerful.</p>

<p>So what is the <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a> model?  Let's say we want to group a bundle of
emails into two groups: spam and not spam.  And all you have to go on are the
contents of the emails.  In this situation, we'd like to just use the words in a
single email to decide whether or not it's spamtastic or spamfree.  <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a>
assumes that you can make this decision based on each word in the
email individually and then combine these decisions.  It's <a href="http://en.wikipedia.org/wiki/Bayes'_rule">Bayesian</a>
because it's a simple <a href="http://en.wikipedia.org/wiki/Bayesian_network">Bayesian Model</a> and it's Naive because this approach
assumes the words are totally independent and have no statistical relation
whatsoever.</p>

<h3>Model Representations</h3>

<p>Due to it's simplicity, and that naive assumption, Naive Bayes makes for an easy
model to understand and describe using a variety of models.  here's some of the
most frequent descriptions:</p>

<h4>A Bayesian Network</h4>

<table>
<td>
<div id='naivebayes'></div>
<script type='text/javascript'>
(function() {

  function draw() {
        var paper = Raphael('naivebayes', 200, 300);
        paper.circle(60, 20, 20);
        paper.text(60, 20, "α")
             .attr({"font-size": 25});

        paper.arrow(60, 40, 60, 80, 10);

        paper.rect(10,60, 100, 150);

        paper.circle(60, 100, 20);
        paper.text(60, 100, "K")
             .attr({"font-size": 25});


        paper.circle(150, 100, 20);
        paper.text(150, 100, "γ")
             .attr({"font-size": 25});

        paper.arrow(150, 120, 150, 150, 10);

        paper.rect(120, 140, 60, 60);
        paper.circle(150, 170, 20);
        paper.text(150, 170, "θ")
             .attr({"font-size": 25});

        paper.arrow(130, 170, 80, 170, 10);

        paper.rect(29, 140, 60, 60);

        paper.arrow(60, 120, 60, 150, 10);

        paper.circle(60, 170, 20);
        paper.text(60, 170, "X")
             .attr({"font-size": 25});
    }

    draw();

    $(window).resize(function() {
        draw();
    });

})();
</script>
</td>
<td>

<script type="math/tex; mode=display">
\begin{array}{lll}
\phi & \sim & Dirichlet(\alpha) \\
\theta_{s} & \sim & Dirichlet(\gamma_s) \\
\theta_{n} & \sim & Dirichlet(\gamma_n) \\
K_{j} & \sim & Multinomial(\phi) \\
X_j & \sim & Multinomial(\theta_{K_{j}}) 
\end{array}
</script>

</td>
</table>


<p>These two views of Naive Bayes describe the same model, but different ways
of computing it.  For the model on the right, if we work backwards, it says that
a data point <script type="math/tex"> X_j </script> comes from some multinomial distribution <script type="math/tex"> \theta_{K_j} </script>.  <script type="math/tex"> K_j </script> is a latent variable that encode which
group, i.e. spam or not spam, the data point came from, and that itself comes
from a multinomial <script type="math/tex"> \phi </script>.  The two <script type="math/tex"> \theta_s, \theta_n </script> distributions describe our knowledge about spam and non-spam emails,
respectively.  And finally, all three multinomial distributions come from
<a href="http://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet</a> distributions.  The added magic of the <a href="http://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet</a>
distrubtion is that it <strong>generates</strong> multinomial distributions, this
relationship is more formally known as a <a href="http://en.wikipedia.org/wiki/Conjugate_prior">Conjugate Prior</a>.  The plate
diagram on the left says <strong>nearly</strong> the same story except it <em>collapses</em> out the <script type="math/tex"> \phi </script> distribution and links <script type="math/tex"> \alpha </script> directly to
the compnent labels.  We could also in theory collapse the <script type="math/tex"> \theta </script> distrubtions, but we'll not do that
for the moment.</p>

<h4>A Graphical Model</h4>

<table>
<td>
<div id="nb-graph"></div>
</td>
<td>
<div id="nb-factor"></div>
</td>
</table>




<script type='text/javascript'>
(function() {

  function draw() {
        var paper = Raphael('nb-graph', 200, 200);

        paper.circle(100, 30, 20);
        paper.text(100, 30, "Y")
             .attr({"font-size": 20});

        paper.circle(30, 160, 20);
        paper.text(30, 160, "w_1")
             .attr({"font-size": 20});

        paper.circle(80, 160, 20);
        paper.text(80, 160, "w_2")
             .attr({"font-size": 20});

        paper.text(120, 160, "...")
             .attr({"font-size": 25});

        paper.circle(170, 160, 20);
        paper.text(170, 160, "w_v")
             .attr({"font-size": 20});

        paper.arrow(100, 50, 30, 140, 10);
        paper.arrow(100, 50, 80, 140, 10);
        paper.arrow(100, 50, 170, 140, 10);
    }

    draw();

    $(window).resize(function() {
        draw();
    });

})();
</script>




<script type='text/javascript'>
(function() {

  function draw() {
        var paper = Raphael('nb-fail', 200, 200);

        paper.circle(100, 30, 20);
        paper.text(100, 30, "Y")
             .attr({"font-size": 20});

        paper.circle(30, 160, 20);
        paper.text(30, 160, "w_1")
             .attr({"font-size": 20});

        paper.circle(80, 160, 20);
        paper.text(80, 160, "w_2")
             .attr({"font-size": 20});

        paper.text(120, 160, "...")
             .attr({"font-size": 25});

        paper.circle(170, 160, 20);
        paper.text(170, 160, "w_v")
             .attr({"font-size": 20});

        paper.factor(100, 50, 30, 140, 10);
        paper.factor(100, 50, 80, 140, 10);
        paper.factor(100, 50, 170, 140, 10);
    }

    draw();

    $(window).resize(function() {
        draw();
    });

})();
</script>


<p>This graphical view tells an alternative story.  Instead of probability
distrubutions making other distributions and then finally making data points.
this says that the class of a data point, <script type="math/tex"> Y </script> generates the
features of the data point, <script type="math/tex">, w_1, w_2, \cdots, w_v </script>.  This is
where the naive part comes from, each "feature", i.e. word in the email, is
completely disconnected from the other words and only depends on the class of
the email.</p>

<p><img class="right" src="/images/thomas_bayes.gif" title="Thomas Bayes, not so Naive" ></p>

<h4>A Probability Distribution</h4>

<p>Finally, there's the written way to understand Naive Bayes, as a conditional
probability distrubtion.  Let's write out the distribution for just the spam
class:</p>

<p><script type="math/tex; mode=display">
p(spam | email) \propto p(spam) \prod_{word \in email} p(word | spam)
</script></p>

<p>This looks pretty much the same for non-spam.  Simple, no? If we combine
together the different views above with this written form, then we can think of
the class <script type="math/tex">spam</script> as a <a href="http://en.wikipedia.org/wiki/Multinomial_distribution">Multinomial</a> distribution over words and the
probability of a word given the class spam is really just the probability of
picking the word based on how many times it's shown up in spammy emails.
Similarly, the probability of seeing spam is just the number of times you've
seen a delcious piece of spam, be it on your phone, at home, or in the frying
pan.  Using that intuition and some Bayesian math, the above definition gets
boild down to:</p>

<p><script type="math/tex; mode=display">
p(s|e) \propto \frac{C_{s} + \alpha_s - 1}{N + \alpha_s + \alpha_n- 1}</p>

<pre><code>            \prod_{w \in e} \theta_{s,w}^{C_{w,e}}
</code></pre>

<p></script></p>

<p>Let's get some definitions out of the way.</p>

<p><script type="math/tex; mode=display">
\begin{array}{lll}
C_s &amp;=&amp; \text{number of times spam's been seen} \
C<em>{w,e} &amp;=&amp; \text{number of times seeing word w in email e} \
N &amp;=&amp; \text{number of data points seen} \
\alpha</em>{s,n} &amp;=&amp; \text{priors for each class} \
\theta_{s,w} &amp;=&amp; \text{the wight given to word w in class spam} \
\end{array}
</script></p>

<p>Let's tease this appart into two pieces, <script type="math/tex"> p(spam) </script> and <script type="math/tex"> p(email|spam) </script>.  The first part is:</p>

<p><script type="math/tex; mode=display">
p(s) \propto \frac{C<em>{s} + \alpha_s - 1}{N + \alpha_s + \alpha_n- 1}
</script>
This basically says that the probability of spam is the number of time's we've
seen it, plus some smoothing factors so that both span and non-spam have some
positive possibility of existing.  Amazingly! this is also equivalent to the
</em>collapsed_ <script type="math/tex">\phi</script> distribution we mentioned in the bayesian
description of Naive Bayes, by doing just a little bit of math and realizing
that the Dirichlet is a conjugate prior of a multinomial, we get the above
distribution for each class.</p>

<p>Next,
<script type="math/tex; mode=display">
p(email|s) = \prod<em>{w \in e} \theta</em>{s,w}<sup>{C_{w,e}}</sup>
</script></p>

<p>If <script type="math/tex">\theta_s</script> describes the probability of each word occuring in a
spammy email, then <script type="math/tex">\theta_{w,s}<sup>{C_{w,e}}</script></sup> becomes the likelihood of
seeing that word as many times as observed in the email.  By computing the
product of all these words, we <em>naively</em> get the probability of the full email.
Slam the two distributions together and we get the full likelihood of the email
given the class spam.</p>

<h3>Don't forget Gibbs!</h3>

<p>Now that Naive Bayes makes some more sense in a variety of descriptive flavours
<img class="right" src="/images/Josiah_Willard_Gibbs_-from_MMS-.jpg" width="300" height="600" title="The man who made sampling cool before HipHop" >
and we've defined our distrubutions and hyper parameters, it's time to throw in
gibbs sampling.  To quickly refresh our memories, the recipe for gibbs at this
stage is:</p>

<ol>
<li>for each data point <script type="math/tex"> X_j </script>

<ol>
<li>Forget about <script type="math/tex"> X_j </script></li>
<li>Compute the likelihood that each component, <script type="math/tex">\theta_*</script>, generated <script type="math/tex">X_j</script></li>
<li>Sample and jot down a new component, <script type="math/tex">K_j</script>, for <script type="math/tex">X_J</script></li>
<li>Remember <script type="math/tex"> X_J </script></li>
</ol>
</li>
<li>Restimate our component parameters, <script type="math/tex">\theta_{*} </script></li>
<li>Repeat</li>
</ol>


<p>You might be wondering "why forget about the current data point?"  The reason is
that we're trying to decide what to do with a <strong>new data point</strong>.  Gibbs
sampling only really works because the distrubtions we're working with are
<a href="http://en.wikipedia.org/wiki/Exchangeable_random_variables">exchangeable</a>, i.e. the ordering of our data doesn't matter.  So we can
just pretend any data point is new by just forgetting it ever existed, Note,
this is why <script type="math/tex">p(spam)</script> has a <script type="math/tex">-1</script> in the denominator, that count
has been "forgotten".  Other than that tidbit, we just pretend everything else
is the same.</p>

<h3>Writing out the code</h3>

<p><img class="left" src="/images/spam_sampling.jpg" width="150" height="150" title="Tasty Samples of Spam" >
With Gibbs sampling fresh in our minds, and our probability distributions clear,
let's see what some real code looks like.  To do this, I'm going to use
<a href="https://github.com/scalala/Scalala">Scalala</a> and <a href="http://www.scalanlp.org/">Scalanlp</a>, which take care of the linear algebra and
probability distrubtions for us really concisely.  And while it may not be the
fastest library around, it's super easy to read and understand.  For faster
implementations, you can just translate this code into whatever beast you desire
and optimize away.</p>

<h4>Setting up before sampling</h4>

<p>Before we go on a spam sampling binge, we gotta setup our initial groupings.
Gibbs sampling works pretty well with a random starting point, so let's go with
that:</p>

<p>``` scala</p>

<pre><code>def sampleThetas(labelStats: Array[DenseVectorRow[Double]]) =
    labelStats.map(n_c =&gt; new Dirichlet(n_c+gamma).sample)

val k = numComponents
val v = numFeatures
val n = numDataPoints
var labelStats = Array.fill(k)(new DenseVectorRow[Double](
    Array.fill(v)(0d)))
var thetas = sampleThetas(labelStats)

val labels = new Multinomial(new Dirichlet(alpha).sample.data).sample(n).toArray
var labelCounts = new DenseVectorRow[Double](Array.fill(k)(0d))
data.zip(labels).foreach{
    case (x_j, j) =&gt; {labelStats(j) += x_j; labelCounts(j) += 1} }
</code></pre>

<p>```</p>

<p>So what's going on here?  Well, first, we just get some basic stats about our
data such as the size, the number of features and the number of components we
want, in this case, 2.  Then we make a data structure that will hold the number
of times we see each feature within each component, <code>labelStats</code>; the
number of times we've seen each component overall <code>labelCounts</code>; and the
parameters for each compnent, <code>thetas</code>.  Finally, randomly assign each point to
a random component and update the <code>labelStats</code> and <code>labelCounts</code>.   Note that we
get the <code>thetas</code> from a Dirichlet distribution that uses the <code>labelStats</code> and a
smoothing parameter <code>gamma</code>, just like we noted in the Bayesian Plate diagram
above.</p>

<p>Now for the real sampling meat:</p>

<p>``` scala</p>

<pre><code>for (i &lt;- 0 until numIterations) {
    for ( (x_j, j) &lt;- data.zipWithIndex ) {
        val label = labels(j)
        labelStats(label) -= x_j
        labelCounts(label) -= 1

        val prior = labelCounts + alpha - 1 / (n-1+alpha.sum)
        val posterior = new DenseVectorRow[Double](thetas.map( theta =&gt;
            (theta :^ x_j).iterator.product).toArray)

        val probs = prior :* posterior
        val new_label= new Multinomial(probs / probs.sum).sample

        labelCounts(new_label) += 1
        labelStats(new_label) += x_j
        labels(j) = new_label
    }
    thetas = sampleThetas(labelStats)
}
</code></pre>

<p>```</p>

<p>The first three lines in the loop get the current label for <code>x_j</code> and then
forget about it's data.  <code>prior</code> represents the likelihood of selecting each
component just based on it's frequency, i.e. <script type="math/tex">p(spam)</script>.  <code>posterior</code>
represents the likelihood of each component generating <code>x_j</code>, i.e. <script type="math/tex"> p(e|s)</script>.
To finish off the sampling procedure, we sample a new label by turning these
likelihoods into a multinomial distrubtion and sample from it.  With the new
label we then add in the data for <code>x_j</code> to the that component's stats.  Once
we've made a full sweep through the data set, we update our parmeters for each
component by resampling from a Dirichlet distribution by using the our
<code>labelStats</code>.</p>

<p>And Voila! We have a simple gibbs sampler for Naive Bayes.</p>

<h3>Running it on some data</h3>

<p>Let's see how this unsupervised version of Naive Bayes handles a really generic
looking dataset: globs of data points from three different Gaussian
distributions, each with a different mean and variance.  And let's start off
with just trying to find 2 groups, as we've been working along with so far.</p>

<p><img class="right" src="/images/test.nb.groups.png" title="Splitting apart some simple groups" ></p>

<p>Looks pretty neat huh?  Even with a terrible and completely random starting
point, our learner manages to split up the data in a sensible way by finding a
dividing plane between two groups.  What's even cooler, is that notice how after
iteration 1 completes, nearly all the data is assigned to one class.  By somehow
by iteration 10, we've managed to finagle our way out of that terrible solution
and into a <strong>significantly</strong> better solution.  And by the 100th iteration, we've
stayed there in a stable state.</p>

<p>Now what happens if we try finding 3 components in the data set?</p>

<p><img class="right" src="/images/test.nb.3.groups.png" title="Splitting apart some simple groups" ></p>

<p>Here we get a similar split, but not what we're looking for.  Our mixture model
gets all befuddled with the group on the bottom left and tries to flailingly
split into two smaller groups.  If you run this a couple times, you'll see that
it does the same thing with the two green groups, it can't split them evenly as
you'd expect.</p>

<h3>The Secret no one mentions</h3>

<p>This simple model i've walked through is the unsupervised equivalent to the
supervised Naive Bayes model.  It does a pretty reasonable job of splitting
apart some groups of data, but there are clearly datasets it has some issues
dealing with.  But what's not been said so far is that this model is actually
not just Naive Bayes, it's a [Finite Mixture Model][] with Multionomial
components.  The code, probabilities, and graphs i've layed out all work for
more than two components.  So next, i'll show two cool ways to extend this
model:</p>

<ol>
<li>With Gaussian components, which can better handle this toy data set</li>
<li>With an infinite number of components, which figure out that paramter <strong>for
you</strong> like a <em>magician</em>.</li>
</ol>


<p><img class="center" src="/images/magician.jpg" title="Magicians figure out their own parameters" ></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting to the mixtures in Dirichlet Process Mixture Models]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/05/04/getting-to-the-mixtures-in-dirichlet-process-mixture-models/"/>
    <updated>2012-05-04T13:01:00-07:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/05/04/getting-to-the-mixtures-in-dirichlet-process-mixture-models</id>
    <content type="html"><![CDATA[<p>The <a href="http://en.wikipedia.org/wiki/Dirichlet_process">Dirichlet Process</a> is pretty sweet as Edwin Chen and numerous others
have <a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">pointed out</a>.  When combined with a mixture of Bayesian models, it's
really effective for finding an accurate number of models needed to represent
your bundle of data, especially useful when you have no idea how many bundles of
related words you may have.  Edwin Chen does a really great job of
<a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">explaining</a> how the Dirichlet Process works, how to implement it, and how it
can break down the McDonald's menu, but he leaves out details on how to combine
it with mixture models, which is by far the coolest part.</p>

<p>The short description for how to merge the Dirichlet Process with <a href="http://en.wikipedia.org/wiki/Mixture_model">Nonparametric
Mixture Modeling</a> (and infer the model) looks like this:</p>

<ol>
<li>Define probability distributions for your mixture model</li>
<li>Rewrite your distributions with Bayes rule so that parameters depend on data
and hyper parameters</li>
<li>Repeat step 2 for hyper parameters as desired (it's turtles all the way down, as Kevin
Knight <a href="http://www.isi.edu/natural-language/people/bayes-with-tears.pdf">said</a>)</li>
<li>For each data point

<ol>
<li>Forget about the current data point</li>
<li>Compute the likelihood that each mixture made this data point</li>
<li>Sampling a new label for this data point using these likelihoods</li>
<li>Jot down this new label</li>
<li>Remember this data point</li>
</ol>
</li>
<li>After seeing all the data, re-estimate all your parameters based on the new
assignments</li>
<li>Repeat steps 4 and 5 ad nausea.</li>
</ol>


<p>In short, this is the <a href="http://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs Sampling</a> approach.  If you already know what
your distributions look like, and this paltry description made perfect sense,
Hurray! Please go bake a cake for everyone that feels a little underwhelmed with
this description.</p>

<p><img class="center" src="/images/cake.jpg" title="Delicious cake for people still reading" ></p>

<p>Now that someone is off baking us cake, let's dive down into what the
distributions look like.  Taking a look at a number of papers describing
variations of the Dirichlet Process Mixture Model, like <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.9111">The Infinite Gaussian Mixture Model</a>
and <a href="http://www.cs.princeton.edu/~blei/papers/BleiJordan2004.pdf">Variational Inference for Dirichlet Process Mixture Models</a>, they first
make things seem pretty straightforward.  To start, they generally throw down
this simple equation:</p>

<p><script type="math/tex; mode=display">
p(y|\theta_1, \cdots, \theta_k, \pi_1, \cdots, \pi_k) = \sum_j<sup>k</sup> \pi_j p(y|\theta_j)
</script></p>

<p>Simple, no?  If you want mixtures of <a href="http://www.umiacs.umd.edu/~resnik/pubs/gibbs.pdf">Gaussian</a> models, then <script type="math/tex"> \theta_j</script> has a mean and a variance, we'll call these <script type="math/tex"> \mu_j</script> and <script type="math/tex">\sigma_j<sup>2</script>.</sup>   The next step is then to rewrite this equation in a couple of ways so that
you define the parameters, i.e. <script type="math/tex"> \theta_j </script> in terms of the data
and other parameters.  One example: <script type="math/tex"> \mu_j </script> looks like this:</p>

<p><script type="math/tex; mode=display">
p(\mu_j|c,y,s_j, \lambda, r) \sim \mathcal{N}\left( \frac{\bar{y}n_js_j + \lambda
r}{n_js_j + r},\frac{1}{n_js_j+r} \right)
</script></p>

<p>Not <em>too</em> bad after reading through the definition for all the parameters; the
first bundle describes the mean of the mixture and the second bundle describes
the variance of the mixture.  You may have noticed that our means for each
mixture come from a Gaussian distribution themselves (I did say it was turtles
all the way down).  If you're using a nice math coding environment, like
<a href="https://github.com/scalala/Scalala">Scalala</a> and <a href="http://www.scalanlp.org/">Scalanlp</a>, you can easily sample from this distribution like
this</p>

<p><code>scala
val theta_j = new Gamma((y_mean*n_j*s_j + lambda*r)/(n_j*s_j+r), 1d/(n_j*s_j +r)).sample
</code></p>

<p>This goes on for most of the parameters for the model until you get to one of
the core tear inducing rewrites.  This lovely gem describes the probably
distribution function for one of the hyper parameters in the <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.9111">Infinite Gaussian Mixture Model</a>:</p>

<p><script type="math/tex; mode=display">
p(\alpha|k,n) \propto \frac{\alpha<sup>{k-3/2}</sup> exp(-1/(2\alpha))\Gamma(\alpha)}</p>

<pre><code>                       {\Gamma(\alpha / k)}
</code></pre>

<p></script></p>

<p><em>This is not easy to sample from</em> especially if you're new to sophisticated
sampling methods.  And sadly, just about every academic publication describing
these kinds of models gets to a point where they assume you know what they're
talking about and can throw down <a href="http://en.wikipedia.org/wiki/Mixture_model">Monte Carlo Markov Chain</a> sampling
procedures like they're rice at a wedding.</p>

<p><img class="center" src="/images/wedding-rice.jpg" title="MCMC sampling methods for the newlyweds" ></p>

<p>So what's a lowly non-statistician data modeller to do? Start with a well
written description of a similar, but significantly simpler model.  In this
case, <a href="http://www.umiacs.umd.edu/~resnik/pubs/gibbs.pdf">Gibbs Sampling for the Uninitiated</a>, which describes how to apply
Gibbs to the Naive Bayes model.  I'll summarize this awesome paper in the next
part of this series and then tweak it a little to make a <a href="http://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model">Finite Gaussian Mixture Model</a>.<br/>
After that's said and done, I'll show how to extend the finite version into the
Dirichlet Process Mixture Model (with some simplifications).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Puzzle Time: Transforming Names]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/04/23/puzzle-time-transforming-names/"/>
    <updated>2012-04-23T22:18:00-07:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/04/23/puzzle-time-transforming-names</id>
    <content type="html"><![CDATA[<p>This post starts what will hopefully be a new trend: computational solutions to
[NPR's Sunday Puzzle][1].  In this weeks puzzle, we have to</p>

<p><blockquote><p>Think of a common man's name in four letters, one syllable. Move each letter exactly halfway around the alphabet. For example, A would become N, N would become A, and B would become O. The result will be a common woman's name in two syllables. What names are these?</p></blockquote></p>

<p>Word puzzles can often be pretty hard for automated computer programs.  This is
in part what made [IBM's Watson][2] so darn cool.  But thankfully this week's
puzzle is amazingly simple for a program, in fact, i'd even say it's ideal for a
program.  To solve it, just need two simple steps:</p>

<ol>
<li>Gather a bunch of male and female names</li>
<li>Process the male names as noted to see if we have any matching female names</li>
</ol>


<p>Step 1 is perhaps the hardest since it requires gathering not only names, but
common names in english.  However, [this][3] handy site has done the work for
us.  Here they have listings of common English names based on a variety of
categories: region, ranking, gender, alphabetical, etc.  For our purposes, we'll
just grab the a long list of the most frequent male and female names from
[here][4] and [here][5].  All the names are nicely laid out in a table with a
pretty regular format.  In fact the format is so regular we can write a [<em>regular expression</em>][6] for it.  Here's an example of the table:</p>

<p>``` html</p>

<table align="left" CELLPADDING="3" CELLSPACING="3" style="table1">
<tr bgcolor="F5FDD9"><td><b>Name&nbsp;&nbsp;</td><td><b>% Frequency&nbsp;&nbsp;</td><td><b>Approx Number&nbsp;&nbsp;</td><td><b>Rank&nbsp;&nbsp;</td></tr>
<tr bgcolor="white"><td>AARON</td><td>0.24</td><td> 350,151 </td><td>77</td></tr>
<tr bgcolor="white"><td>ABDUL</td><td>0.007</td><td> 10,213 </td><td>831</td></tr>
<tr bgcolor="F5FDD9"><td>ABE</td><td>0.006</td><td> 8,754 </td><td>854</td></tr>
<tr bgcolor="white"><td>ABEL</td><td>0.019</td><td> 27,720 </td><td>485</td></tr>
<tr bgcolor="white"><td>ABRAHAM</td><td>0.035</td><td> 51,064 </td><td>347</td></tr>
<tr bgcolor="white"><td>ABRAM</td><td>0.005</td><td> 7,295 </td><td>1053</td></tr>
<tr bgcolor="F5FDD9"><td>ADALBERTO</td><td>0.005</td><td> 7,295 </td><td>1040</td></tr>
<tr bgcolor="white"><td>ADAM</td><td>0.259</td><td> 377,871 </td><td>69</td></tr>
```

Regular right?  It's so darn simple we can use some simple command line tools to
clean it up:

```
cat female_names_alpha.htm | grep '<tr bgcolor="' | sed "s/.*<td>\([A-Z]\+\).*/\1/" | tail -n +2 > female_names.txt
```

If we downloaded the female names into `female_names_alpha.htm`, the first part
will simply print out the contents of the html page to standard out.  The second
part, the grep command, will catch all rows in the table, which thankfully have
the same start prefix.  Part three extracts the names nestled in a row element.
And finally the tail part eliminates the first row of the table, e.g. the head
row, which grep accidentally catches.  This'll grab us a ton of names.

So what's next?  Well, we have a bunch of male and female names.  But we've got
too many, namely we have names that clearly won't work as they have too many or
too few letters.  So let's so some more cleaning, but this time in scala:

``` scala
val maleNames = Source.fromFile(args(0)).getLines.filter(_.size == 4)
```

This dandy line reads the text from the file and filters out any lines that have
more than four characters.  Since our initial processing placed each name on a
line, this in effect removes any names with more than four letters.  If we do
this with the female names, we'll have lists of common four letter names for the
two genders.  For later use, we'll also turn the female names into a set by
calling `.toSet` on the list.

Last, we can do the transformation on each male name to see if it's a valid
female name and then print out any combinations that match.  The next three
lines do this slickly:

``` scala
maleNames.map(w => (w, w.map(l => (((l-'A'+13) % 26) + 'A').toChar).toString))
         .filter( bg => femaleNames.contains(bg._2))
         .foreach(println)
```

This works in three steps.  First, the `map` call transforms each male name into
a `tuple` which consists of the original male name, and then a _potential_
female name by sliding up each letter in the name by 13 letters.  With ascii
characters, this is done just by first grounding the letter to 0, by subtracting
the char value for `A`, adding 13, and then rolling over the value to be back
in the range of the 26 letter characters by taking the modulo (`%`) and finally
bumping the character value into the range of real ascii characters by adding
`A`.   That's all done in the first line.  Step two is to simply throw out any
generated female names that don't show up in our list of female names, done by
line 3.  And the final line just prints out any matches we get.  With our name
lists, this turns out to be:

```
(ERIN,REVA)
(EVAN,RINA)
(GLEN,TYRA)
(IVAN,VINA)
(RYAN,ELNA)
```

The rules of the game stipulated that the male name had to have one syllable and
the female name has to have two.  We didn't code for this at all since that
parts slightly more complicated, and since our pre-processing reduced the set of
options down to five, we can easily pick out a valid answer.  In this case,
"Glen" and "Tyra" or "Ryan" and "Elna" look like valid responses.  I'd vote for
the first pair since I haven't heard "Elna" in forever.


  [1]: http://www.npr.org/2012/04/22/151120151/a-puzzle-worthy-of-don-draper
  [2]: http://www-03.ibm.com/innovation/us/watson/index.html
  [3]: http://names.mongabay.com
  [4]: http://names.mongabay.com/male_names.htm
  [5]: http://names.mongabay.com/female_names.htm
  [6]: http://en.wikipedia.org/wiki/Regular_expression

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Making Agglomerative Clustering run in N^2 Time]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/03/16/making-agglomerative-clustering-run-in-n-2-time/"/>
    <updated>2012-03-16T17:21:00-07:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/03/16/making-agglomerative-clustering-run-in-n-2-time</id>
    <content type="html"><![CDATA[<p>Let's say you're an amateur zoologist and you've got a bunch of data describing
<a href="http://en.wikipedia.org/wiki/Owl">parliaments of owls</a>, <a href="http://en.wikipedia.org/wiki/Goose">gaggles of geese</a>, <a href="http://en.wikipedia.org/wiki/Chicken">peeps of chickens</a>, and a
<a href="http://en.wikipedia.org/wiki/Jackdaw">train of jackdaws</a> but you don't really know that you have these bird types.
All you really have are descriptive features describing each bird like feature
type, beak type, conservation status, feeding preferences, etc.   Using just
this data, you'd like to find out how many bird species you have and how similar
each group is to the others in a nice graphical fashion like down below.  How
would you do it?</p>

<p><img class="center" src="/images/circularDendrogram.png"></p>

<p>The classic solution to the problem would be to use <a href="http://en.wikipedia.org/wiki/Hierarchical_clustering">Hierarchical Agglomerative
Clustering</a>.  In this case, Agglomerative Clustering would group together
birds that have similar features into hopefully distinct species of birds.  And
there's a lot of packages out there that kind of do this for you like <a href="http://www.cs.waikato.ac.nz/ml/weka/">Weka</a>,
<a href="http://scikit-learn.org/stable/">Scikit-Learn</a>, or plain old <a href="http://www.statmethods.net/advstats/cluster.html">R</a>.  However, you've got a <em><em>lot</em></em> of birds
to deal with and these standard packages are just taking <em><em>way</em></em> too long.  Why
are they taking way too long?  Because they're doing agglomerative clustering
the slow ways.</p>

<p>So what do they slow ways look like?  Well, pretty much all ways of doing
agglomerative clustering start by building an Affinity Matrix that simply
measures how similar two birds are to each other:</p>

<p>``` scala Building the Affinity Matrix</p>

<pre><code>val m = Array.fill(numBirds, numBirds)(0)
for ((bird1, i) &lt;- birds.zipWithIndex;
     (bird2, j) &lt;- birds.zipWithIndex)
    m(i)(j) = similarityBetween(bird1, bird2)
</code></pre>

<p>```</p>

<p>This little snippet just compares each bird against all other birds and stores
how "similar" they are to each other in terms of their descriptive features.
What next?  Well, you need another data structure to keep track of your bird
groups.  We'll do this with just a map from group identifiers to sets of bird
identifiers.  And since this algorithm is named ``agglomerative'', we gotta
agglomerate things, so we'll start by putting every bird in their own bird
group:</p>

<p>``` scala Starting off the bird groups</p>

<pre><code>// First get each bird and it's id, then create a tuple holding the bird id
// and a set with the bird as the only element.  Then turn this list of
// tuples into a map.
var groupMap = birds.zipWithIndex.map( birdIndex =&gt; (birdIndex._2, Set(birdIndex._1))).toMap
</code></pre>

<p>```</p>

<p>So that was simple.  Now comes the complicated bits that either lead you to a
slow version, a fairly slow version, or a <em>super</em> slow version of agglomerative
clustering.  Before we do either of these options, let's make two
simplifications: let's assume we just want the final groups that our bird show
up in and we know how many bird groups we want to find.  Agglomerative
Clustering can give you not only this information, but a whole tree showing how
birds are linked together, but the book-keeping for doing this is tricky and
doesn't really affect the issues we're focusing on here.  With that out of the
way, what does the <em>super</em> slow method look like?</p>

<p>``` scala The Super Slow Agglomerative Clustering</p>

<pre><code>while (groupMap.size &gt; numClusters) {
    val groupSet = groupMap.toSet
    // Find the two most similar groups in our map.
    var bestSim = 0.0
    var bestGroups = (-1, -1)
    for (Array((id1, birds1), (id2, birds2)) &lt;- groupSet.subsets(2).map(_.toArray)) {
        // Get the similarity between the two bird groups using the raw
        // values in m.
        val sim = groupSim(id1, birds1, id2, birds2, m)
        if (sim &gt; bestSim) {
            bestSim = sim
            bestGroups = (id1, id2)
        }
    }

    // Now merge the two groups together into a new group
    val newGroup = groupMap(bestGroups._1) ++ groupMap(bestGroups._2)
    // Now remove the two groups from the map
    groupMap = groupMap - bestGroups._1
    groupMap = groupMap - bestGroups._2
    // Update the map to store the new group.  Since the old id's are
    // removed, we can just re-use one of them without any change.
    groupMap = groupMap ++ Map((bestGroups._1 -&gt; newGroup))
}
</code></pre>

<p>```</p>

<p>That's it!  It's <em>super</em> simple and <em>super</em> slow.  Why is it so slow?  Well, in
general, you'll do the big while loop <code>O(N)</code> times, where N is your number of
birds.  Then the next block of code compares each possible pairing of bird
groups.  Since in general the number of groups is proportional to the number of
birds, this will be <code>O(N^2)</code> comparisons.  Throw these two bits together
and you get a runtime of <code>O(N3)</code>!  When you have 100,000 birds,
<em>that's</em> <strong>super</strong> slow.</p>

<p>So what can you do to hasten that up?  Well, the main goal of the big loop
through pairs of groups is to find the most <em>similar</em> pair of bird groups, so an
obvious choice would be to create a priority queue, so let's see how that looks:</p>

<p>``` scala Agglomerative Clustering, Priority Queue Style</p>

<pre><code>// Create a new priority queue that tracks the similarity of two groups and
// their id's
val groupQueue = new PriorityQueue[(double, int, int)]()
val groupSet = groupMap.toSet
for (Array((id1, birds1), (id2, birds2)) &lt;- groupMap.subsets(2).map(_.toArray)) {
    // Get the similarity between the two bird groups using the raw
    // values in m.
    val sim = groupSim(id1, birds1, id2, birds2, m)
    groupQueue.enque((sim, id1, id2))
}

var nextId = groupMap.size
while (groupMap.size &gt; numClusters) {
    var best = (0, -1, -1)
    do {
        best = p.dequeue
    } while (groupMap.contains(best._2) &amp;&amp; groupMap.contains(best._3))

    // Now merge the two groups together into a new group
    val newGroup = groupMap(best._1) ++ groupMap(best._2)
    // Now remove the two groups from the map
    groupMap = groupMap - best._1
    groupMap = groupMap - best._2

    // Create a new id for this group.
    val newId = nextId

    // Next, add in the similarity between the new group and all existing
    // groups to the queue.
    for ( (id, group) &lt;- groupMap )
        groupQueue.enqueue((groupSim(newId, newGroup, id, group, m),
                            newId, id))

    // Finally, update the map to store the new group.
    nextId += 1
    groupMap = groupMap ++ (newId-&gt; newGroup)
}
</code></pre>

<p><code>`
Now this approach is definitely _faster_ but it's also horribly memory
inefficient.  Even if you have an array based priority queue that doesn't
allocate any extra memory for each entry other than the tuple being stored, this
approach has one major problem: after every merge step</code>O(N)`` elements
immediately become invalidated.  Since the two merged groups no longer exist,
any comparison between them and other groups is moot.  However the priority
queue doesn't easily permit removing them so they just float around.  This is
precisely why lines 14-16 are in a do while loop that ensures the returned
groups exist.  It's highly likely that they wont.  I think we can do better,
both in terms of speed and in terms of memory.  So let's blaze through a faster
method in couple of steps.</p>

<p>First, we need to change the setup of the algorithm.  The previous two method
just depended on some simple data structures.  This approach requires two
additional structures for bookkeeping: a way to track chains of nearest
neighbors and a way to track the set of clusters not already a part of the
chain.</p>

<p>``` scala Agglomerative Clustering, Blazingly Fast Style Step 1</p>

<pre><code>// A mapping from cluster id's to their point sets.
val clusterMap = new HashMap[Int, HashSet[Int]]()

// A set of clusters to be considered for merging.
val remaining = new HashSet[Int]()

// Create a cluster for every data point and add it to the cluster map
// and to the examine set.
for (r &lt;- 0 until adj.size) {
    remaining.add(r)
    clusterMap(r) = HashSet(r)
}

// Create a stack to represent the nearest neighbor.  The real source of
// matic
val chain = new Stack[(Double, Int)]()

// Add in a random node from remaining to start the neighbor chain.
initializeChain(chain, remaining);
</code></pre>

<p>```</p>

<p>The <code>clusterMap</code> structure replaces our <code>birdMap</code> but does the same thing, but
the remaining set and the chain stack hold the crux of this approach.  Instead
of trying to find the best link between two clusters, we're going to depend on
chains of nearest neighbors, so the first node in the chain could be anything,
but the second node is simply the nearest neighbor to the first node.  To add
the third node, we find the nearest neighbor out of any nodes not in the chain.
We'll keep doing this until two nodes in the chain represent reciprocal nearest
neighbors, that is two nodes that are most similar to each other, and no other
nodes in or outside of the chain.  Upon finding these two nodes, we merge them,
immediately.  Then we just repeat the process until we have the desired number
of clusters.  In scala, this turns out to be pretty simple to do:</p>

<p>``` scala Agglomerative Clustering, Blazingly Fast Style Step 2</p>

<pre><code>// Find the nearest neighbors and merge as soon as recursive nearest
// neighbors are found.
while (clusterMap.size &gt; numClusters) {
    // Get the last link in the chain.
    val (parentSim, current) = chain.top

    // Find the nearest neighbor using the clusters not in the chain
    // already.
    val (linkSim, next) = findBest(remaining, adj, current)

    // Check the similarity for the best neighbor and compare it to that of
    // the current node in the chain.  If the neighbor sim is larger, then
    // the current node and it's parent aren't RNNs.  Otherwise, the current
    // node is RNNs with it's parent.
    if (linkSim &gt; parentSim) {
        // The current node to push is more similar to the last node in the
        // chain, so the last node and the next to last nodes can't be
        // reciprocal nearest neighbors.
        chain.push((linkSim, next))
        remaining.remove(next)
    } else {
        // The current node is less similar to the last node than the last
        // node is to it's predecesor in the chain, so the last two nodes in
        // the chain are just the kind of nodes we're looking to merge.

        // Pop the current node from the top. 
        chain.pop
        // Pop the parent of the best node.
        val (_, parent) = chain.pop

        // These are the two nodes we'll be merging.  The node we
        // found above is left in the remaining set and is essentially
        // forgotten.

        // Remove the current and parent clusters from the cluster map
        // and extract the sizes.
        val (c1Points, c1Size) = removeCluster(clusterMap, current)
        val (c2Points, c2Size) = removeCluster(clusterMap, parent)
        val total = c1Size + c2Size

        // Update the similarity between the new merged cluster and all
        // other existing clusters.  
        for (key &lt;- clusterMap.keys) 
            adj(current)(key) = updatedSimilarity(
                adj, current, c1Size, parent, c2Size, key)

        // Replace the mapping from current to now point to the merged
        // cluster and add current back into the set of remaining
        // clusters so that it's compared to nodes in the chain.
        clusterMap(current) = c1Points ++ c2Points
        remaining.add(current)

        // If the chain is now empty, re-initialize it.
        if (chain.size == 0)
            initializeChain(chain, remaining)
    }
}
</code></pre>

<p>```</p>

<p>And that's it!  By focusing on reciprocal nearest neighbors, the algorithm
merges together clusters that will always be merged, no matter how you find
them.  Furthermore, it's remarkably easy and fast to find these nodes.  By
building the chain, the number of things that can go on the chain gets smaller
and smaller.</p>

<p>Oh, but there's one other magic trick to making this super fast, and it depends
on how you compute <code>updateSimilarity</code>.  The silly way to do it would be to
traverse all the pairings between nodes in the new cluster and the each other
remaining cluster, but that in itself gets really slow as the clusters grow.
But rather than doing that, <a href="http://onlinelibrary.wiley.com/doi/10.1002/widm.53/full">these folks</a> suggest some recurrence relations
that can be computed in <em>constant time</em>, for any of the existing agglomerative
criteria methods.  Crazy right?  But just crazy enough to work correctly and be
too fast to believe.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Morphological Analysis made easy with Scala]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/02/29/morphological-analysis-made-easy-with-scala/"/>
    <updated>2012-02-29T19:49:00-08:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/02/29/morphological-analysis-made-easy-with-scala</id>
    <content type="html"><![CDATA[<p>Ever since I got involved with distributional semantics, i've been perplexed
about how to handle morphed words, which happens to be just about every noun and
verb in English.  What is a morphed word in English you ask?  It's pretty much
any word that's been changed to reflect things like past tense, plurality,
ownership, and all those things.  They're conjugated verbs and more!  But they
always pose a massive problem in distributional semantics.</p>

<p>Think about it for a second, what's the big difference between "cat" and "cats"?
Not too much, the second is simply saying there are multiple occurrences of a
"cat".  But what do people typically do for distributional semantics?  One of
two bad options: leave the two words as separate things and thus split the
information gained about "cat" across two words or stem every word and throw
away something important like the multitude of "cat".  Both seem totally wrong
and unsatisfactory.</p>

<p>So on and off I've searched for a good tool to do morphological analysis for
English.  Ideally, the analyzer could recognize the word "cats" and split into
two things: <code>cat</code> and <code>&lt;p&gt;</code> that way you can retain all your information about a
<code>cat</code> and still know that there were a multitude of them when you see "cats".
And for verbs?  You should see the same thing, <code>ran</code> could then become something
like <code>run</code> and <code>&lt;past&gt;</code>" so that again, you still know everything about running and that
it happened in the past.  But until today, I've never found a tool that does
this both quickly and easily in a usable language.</p>

<p>That all changes today.  Today I found the <a href="http://wiki.apertium.org/wiki/Lttoolbox-java">Lltool-box for Java</a>.  It creates
a large finite state machine for recognizing morphed words and figuring out the
root word, i.e. "run" in "ran" and "cat" in "cats", and how the word was
morphed, i.e. "run" for "ran" and "cat" for "cats".  All you have to do is get a
listing of how words are morphed, load it up until the <a href="http://wiki.apertium.org/wiki/Lttoolbox-java">Lttoolbox-java</a>
system and analyze away your sentences.</p>

<p>So let's take a spin on how to do this.  First, you read the wiki instructions
for <a href="http://wiki.apertium.org/wiki/Lttoolbox-java">Lttoolbox-java</a>, download it, and compile it.  Compiling is easy:</p>

<p><code>
mvn deploy -DskipTests
</code></p>

<p>The <code>-DskipTests</code> part seems needed since their unit tests don't pass.  But
after that, you can start using the code in your favorite jvm based language.
my personal fave is Scala, so let's run with that.  So what next?  Now you
create a <a href="http://en.wikipedia.org/wiki/Finite_state_transducer">Finite State Transducer</a> using a dictionary file and tell it to
analyze words:</p>

<p>``` scala Setting up the Finite State Transducer https://gist.github.com/1772752 Source Gist</p>

<pre><code>// Create the Finite State Transducer processor.
val fstp = new FSTProcessor()

// Load the finite state transducer with the compiled dictionary file.  
fstp.load(openInFileStream(args(0)))

// Setup the trandsducer to do morphological analysis and make sure it's valid.
fstp.initAnalysis
</code></pre>

<p>```</p>

<p>Now that you have a transducer ready to analyze words, you just feed it stuff like this:</p>

<p>``` scala Loading the transducer with input and output https://gist.github.com/1772752 Source Gist</p>

<pre><code>val in = new StringReader("cats, dogs and blubber all running quickly!")
val out = new StringWriter()
// Do the analysis.
fstp.analysis(in, out)
</code></pre>

<p>```</p>

<p>You gotta create a reader and writer for their transducer interface.  It's
funky, but it still leaves you with a pretty flexible interface.  So now what?
What does the output look like?  If you feed it "cats, dogs and blubber all
running quickly!", the output is pretty ugly at first:</p>

<p>``` html This will never look pretty</p>

<pre><code>^cats/cat&lt;n&gt;&lt;pl&gt;$^,/,&lt;cm&gt;$
^dogs/dog&lt;n&gt;&lt;pl&gt;$
^and/and&lt;cnjcoo&gt;$
^blubber/*blubber$
^all/all&lt;adj&gt;/all&lt;adv&gt;/all&lt;predet&gt;&lt;sp&gt;/all&lt;det&gt;&lt;qnt&gt;&lt;pl&gt;/all&lt;det&gt;&lt;qnt&gt;&lt;sp&gt;/all&lt;prn&gt;&lt;qnt&gt;&lt;mf&gt;&lt;sp&gt;$
^running/run&lt;vblex&gt;&lt;ger&gt;/run&lt;vblex&gt;&lt;pprs&gt;/run&lt;vblex&gt;&lt;subs&gt;/running&lt;adj&gt;/running&lt;n&gt;&lt;sg&gt;$
^quickly/quickly&lt;adv&gt;$
</code></pre>

<p>```</p>

<p>Beastly no? So i figure that it's best to write some regular expressions to
handle all of this.  You'll need a couple: one to split up words, one to match
analysed words, one for unrecognized words, and one to split up the tags.  In
scala you can do this pretty easily like this:</p>

<p>``` scala Regular Expressions for handling output from Lttoolbox-java https://gist.github.com/1772752 Source Gist</p>

<pre><code>// 1: Recognize a fully analyzed word so that they can be tokenized.  In the
// above test case, "cats," will not be separated by white space so we require
// this more complicated splitting method.
val parseRegex = """\^.*?\$""".r
// 2: Recognize a word with morphological tags.
val morphredRegex = """\^(.+?)/(.+?)(&lt;[0-9a-z&lt;&gt;]+&gt;).*\$"""
// 3: Recognize a word that could not be recognized.  The transducer prepends
// &amp;quot;*&amp;quot; to unrecognized tokens, so we match and eliminate it.
val unknownRegex = """\^(.+)/\*(.+?)\$"""
// 4: A regular expression for matching morphological tags.  This is simpler
// than writing a splitting rule.
val featureRegex = """&lt;.*?&gt;"""
</code></pre>

<p>```</p>

<p>Then all you need to do is run through the analyzed sentence and split it up
into separate tokens, some for root words and some for morphed features.  You
can do that like this:</p>

<p>``` scala Parsing that business Lttoolbox-java https://gist.github.com/1772752 Source Gist</p>

<pre><code>val tokens = parseRegex.findAllIn(out.toString).map(parseMatch =&amp;gt; 
// Match the current analyzed word as being morphed or unknown.  For morphed
// words, create a list of the lemma and the tags.  For unknown words just
// create a list of the lemma.
parseMatch.toString match {
    case morphredRegex(surface, lemma, tags) =&amp;gt;
        lemma :: featureRegex.findAllIn(tags).toList
    case unknownRegex(surface, lemma) =&amp;gt;
        List(lemma) 
}).reduceLeft(_++_).filter(!rejectFeatures.contains(_))
</code></pre>

<p>```</p>

<p>This bit of code's pretty sweet.  You first iterate over each analyzed word with
the first matcher.  Then you match each word with the two word level regular
expressions: one for fully analyzed words and one for unrecognized words.  After
that it's easy smeesy, you just split the tags up with the last regular
expression and turn it all into a list.  The last two bits at the end turn the
whole thing into a single list and lets you filter out tags or tokens you don't
want.</p>

<p>So with that, you can now do simple and fast morphological analysis in Java,
Scala, or even Clojure (but who'd do something silly like that?)!-</p>
]]></content>
  </entry>
  
</feed>
