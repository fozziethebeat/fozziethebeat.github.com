<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: scala | Wordsi by Fozzie The Beat]]></title>
  <link href="http://fozziethebeat.github.com/blog/categories/scala/atom.xml" rel="self"/>
  <link href="http://fozziethebeat.github.com/"/>
  <updated>2012-09-12T23:07:53-07:00</updated>
  <id>http://fozziethebeat.github.com/</id>
  <author>
    <name><![CDATA[Keith Stevens]]></name>
    <email><![CDATA[fozziethebeat@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Building A Phrase Graph]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/08/12/building-a-phrase-graph/"/>
    <updated>2012-08-12T12:13:00-07:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/08/12/building-a-phrase-graph</id>
    <content type="html"><![CDATA[<p>Research papers.  I hate them sometimes.  They present a great idea, talk about
how it can be used and applied, and then give only the barest description of how
to actually build and implement the idea, often with no pointers or links to
what they built.  My current frustration is with building a <strong>Phrase Graph</strong>.
The idea behind phrase graphs are pretty simple, they encode a large set of
sentences with a minimal automata.</p>

<p>Here's a simple example.  Say you have the following two sentences:</p>

<pre><code>#archery by the Republic of Korea and the guy is legally blind.
#archery by the Republic of Korea and the guy who is legally blind.
#archery by the Republic of Korea in archery by a guy who is legally blind.
</code></pre>

<p>There's quite a lot of overlap at the start of the sentence and at the end of
the sentence.  So a good phrase graph would look something like this:</p>

<p><img src="/images/phrase-graph-example.svg" alt="A simple Phrase Graph" /></p>

<p>So you can see that the nodes in the graph represent words found in the
sentences observed and if you weight the nodes based on how often they are
traversed, you can start to detect which phrases are used most frequently.  But
how does one do this?  And how does one do this efficiently?  This is where
research papers make me mad.  They fail to point out the simplest algorithms for
building these awesome ideas.  To complement these research ideas, this post'll
give a little more detail on what these phrase graphs are, an easy way to build
them using existing libraries, and code to write your own custom phrase graph!</p>

<h2>Badly described methods for building a phrase graph</h2>

<p>The first paper I read for building phrase graphs, titled <a href="http://www.jeffreynichols.com/papers/summary-iui2012.pdf">Summarizing Sporting
events using Twitter</a>, gives this highly detailed algorithm description:</p>

<p>{%blockquote%}
The phrase graph consists of a node for each word appearing in any status update, and an edge between each set of two words that are used adjacently in any status update
{%endblockquote%}</p>

<p>Seems easy to implement, no?  Here's a more detailed algorithm, found in
<a href="http://www.cs.uccs.edu/~jkalita/papers/2010/SharifiBeauxSocialcom2010.pdf">Experiments in Microblog Summarization</a>:</p>

<p>{%blockquote%}
To construct the left-hand side, the algorithm starts with the root node. It reduces the set of input sentences to the set of sentences that contain the current node’s phrase. The current node and the root node are initially the same. Since every input sentence is guaranteed to contain the root phrase, our list of sentences does not change initially. Subsequently, the algorithm isolates the set of words that occur immediately before the current node’s phrase. From this set, duplicate words are combined and assigned a count that represents how many instances of those words are detected. For each of these unique words, the algorithm adds them to the graph as nodes with their associated counts to the left of the current node.
{%endblockquote%}</p>

<p>This gives a lot more detail on what the phrase graph contains, and an easy
enough algorithm, but it's not exactly a <strong>fast</strong> algorithm, especially if you
want to do this using 10 million tweets about the Olympics.  Both descriptions
leave out a key detail: these phrase graphs are really just <a href="http://en.wikipedia.org/wiki/Trie#Compressing_tries">Compressed Tries</a>.</p>

<h2>Tries and their compressed cousins</h2>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/b/be/Trie_example.svg" alt="A simple Trie" /></p>

<p><a href="http://en.wikipedia.org/wiki/Trie#Compressing_tries">Tries</a> are one of the simplest data structures, and one of the most powerful
when processing natural languages.  Given a set of words or sentences, a
<a href="http://en.wikipedia.org/wiki/Trie#Compressing_tries">Trie</a> is essentially a standard tree where the leaves represent observed
words or sentences.  The power of it is that each internal node in the <a href="http://en.wikipedia.org/wiki/Trie#Compressing_tries">Trie</a>
represents overlapping sequences. So if you want to build a <a href="http://en.wikipedia.org/wiki/Trie#Compressing_tries">Trie</a> for an <a href="http://www.brics.dk/automaton/">English
Dictionary</a>, the root node would be a blank character, which then points to a
node for each letter of the alphabet.  From the "a" child, you would then have
access to all words starting with "a", and the further down the <a href="http://en.wikipedia.org/wiki/Trie#Compressing_tries">Trie</a> you
go, you get longer prefixes of words.</p>

<p>Now a <em>Phrase Graph</em> is essentially a <a href="http://en.wikipedia.org/wiki/Trie#Compressing_tries">Trie</a> which condenses not only shared
prefixes, but also any shared subsequence, be they in the middle, or the end.
Formally, they are directed acyclic graphs, and if they are treated as a lookup
structure, ala a dictionary, they are called <em>Minimal Acyclic Finite-State
Automata</em>.  And there's plenty of fast and simple ways to build these things.
The easiest places to start reading about these is <a href="http://acl.ldc.upenn.edu/J/J00/J00-1002.pdf">Incremental Construction of
Minimal Acyclic Finite State Automata</a>. The <a href="http://www.brics.dk/automaton/">Brics Automaton package</a> also
provides a really good implantation for these that works</p>

<p>``` scala A simple example using the Brics package
import dk.brics.automaton.BasicAutomata
val automata = BasicAutomata.makeStringUnion(</p>

<pre><code>"#archery by the Republic of Korea and the guy is legally blind",
"#archery by the Republic of Korea in archery by a guy who is legally blind")
</code></pre>

<p>println(automata.toDot)
```</p>

<p>The above code snippet will generate this simple minimal automata:</p>

<p><img src="/images/brics-phrase-graph-example.svg" alt="A GraphViz version of the Brics Automata" /></p>

<p>*Note, you may want to open this in a new tab to zoom in as every letter has
it's own state.</p>

<h2>Rolling your own automata builder</h2>

<p>Using <a href="http://www.brics.dk/automaton/">Brics</a> works really well if you just want to check whether or not a
sentence matches one seen in a corpus.  However, it doesn't easily let you check
how often particular sub-phrases are used within the corpus.  For that kinda
power, you'll have to craft your own implemenation.  And now it's time to share
the very code to do this!</p>

<h3>Nodes in the graph, that's where.</h3>

<p>Where does one start?  First, you need a node data structure, with some very
carefully crafted functions to determine equality (which indidentally, the
research papers <strong>don't</strong> point out).</p>

<p>``` scala The PhraseNode data structure
/<em>*
 * A simple node structure that records a label, a weight, and a mapping from this node to other nodes using labeled arcs.  This
 * implementation overrides {@link hashCode} and {@link equals} such that only nodes with the same label and which point to the same exact
 * children (i.e.  same objects, not equivalent objects), are considered equal.
 </em>/
class PhraseNode(val label: String) {</p>

<pre><code>/**
 * The internal weight for this {@link PhraseNode}.
 */
var inCount = 0d

/**
 * A mapping from this {@link PhraseNode} to children {@link PhraseNode}s using labeled arcs.
 */
var linkMap = Map[String,PhraseNode]()

/**
 * A record of the last {@link PhraseNode} added as a child to this {@link PhraseNode}.
 */
var lastAdded:PhraseNode = null

/**
 * Returns the {@link PhraseNode} connected to {@code this} {@link PhraseNode} via the arc {@code term}.  If no such node exists, a new
 * {@link PhraseNode} is created and returned.
 */
def neighbor(term: String) =
    linkMap.get(term) match {
        case Some(node) =&gt; node
        case None =&gt; { lastAdded = new PhraseNode(term)
                       linkMap += (term -&gt; lastAdded)
                       lastAdded
                     }
    }

/**
 * Adds {@code delta} to the {@code inCount} and returns a pointer to {@code this} {@link PhraseNode}.
 */
def addCount(delta: Double = 1) = {
    inCount += delta
    this
}

/**
 * Returns a hashcode based on java's internal hash code method for every object which uniquely identifies every object.
 */
def pointerHashCode = super.hashCode

/**
 * Override {@code hashCode} to use three factors:
 * &lt;ol&gt;
 *  &lt;li&gt;The hash code for {@code label}&lt;/li&gt;
 *  &lt;li&gt;The hash code for {@code label} of each child node&lt;/li&gt;
 *  &lt;li&gt;The hash code for {@code pointer} of each child node&lt;/li&gt;
 * &lt;/ol&gt;
 * This ensures that nodes only have the same hash code if they have the same label, same number of children, same links to those
 * children, and point to the very same children.  This is a cheap and fast way to ensure that we don't accidently consider two nodes
 * with the same link labels aren't equivalent.
 */
override def hashCode = 
    linkMap.map{ case(childLabel, child) =&gt;
        childLabel.hashCode ^ child.pointerHashCode
    }.foldLeft(label.hashCode)(_^_)

/**
 * Override {@code equals} to use the same three factors as {@cod hachCode}:K
 * &lt;ol&gt;
 *  &lt;li&gt;The {@code label}&lt;/li&gt;
 *  &lt;li&gt;The {@code label} of each child node&lt;/li&gt;
 *  &lt;li&gt;The {@code pointer} of each child node&lt;/li&gt;
 * &lt;/ol&gt;
 * 
 * This ensures that nodes only equal when they have the same distinguishing meta data and point to the same children.
 */
override def equals(that: Any) =
    that match {
        case other: PhraseNode =&gt; if (this.hashCode != other.hashCode) false
                                  else if (this.label != other.label) false
                                  else compareLinkMaps(this.linkMap, other.linkMap)
        case _ =&gt; false
    }

/**
 * Returns true if the two maps have the same size, same keys, and the key in each map points to the same object.  We use this instead
 * of simply calling equals between the two maps because we want to check node equality using just the pointer hash code, which prevents
 * walking down the entire graph structure from each node.
 */
def compareLinkMaps(lmap1: Map[String, PhraseNode], lmap2: Map[String, PhraseNode]) : Boolean = {
    if (lmap1.size != lmap2.size)
        return false
    for ( (key1, entry1) &lt;- lmap1 ) {
        val matched = lmap2.get(key1) match {
            case Some(entry2) =&gt; entry2.pointerHashCode == entry1.pointerHashCode
            case None =&gt; false
        }
        if (!matched)
            return false
    }
    true
}
</code></pre>

<p>}
```</p>

<p>This <code>PhraseNode</code> has three fairly simple data members, a label that records
which word the node represents, the weight of the node, and a map from this node
to it's children based on their labels.  The tricky part of this node is how you
determine equality.  Two nodes can be equal in two different senses: 1) they are
the exact same data structure in memory, and so their memory locations are the
same or 2) they have the same label and point to the same exact children in
memory.  Checking the first type of equality is easy, you can compare the hash
code of their addresses using the default <code>hashCode</code> method java provides for
every object.  Checking the second form of equality is more challenging to do
efficiently.  The naive way would be to recursively check that all children
eventually point to sub-graphs with the same structure.  However, checking the
hash code of the pointers of each children is <strong>much</strong> faster and accomplishes
the same goal.  Hence, this is why we override <code>hashCode</code> and <code>equals</code> with such
complicated code.</p>

<h3>Linking together those Phrase Nodes</h3>

<p>The algorithm for linking together <code>PhraseNodes</code> such that they form a minimal
transducer relies on a few interesting tricks and <strong>beautiful</strong> recursion.  The
first trick we need is lexicographically sorted input.  By sorting the input,
you're maximizing the size of matching prefixes between any neighboring words you want
to put in the transducer.  So let's look at how we do that adding in sorted
order.</p>

<p>Before we get there though, let's flesh sketch out a <code>CondensedTrie</code> data
structure.  It's pretty simple.  It starts off with just having a single
<code>PhraseNode</code> element, the root.</p>

<p>``` scala Not the data structure we deserve, but the data structure we need
/<em>*
 * The {@link CondensedTrie} represents a single phrase graph centered around a single key phrase.  Lists of tokens, representing sentences,
 * can be added to the {@link CondensedTrie} to create a minimal finite state automata which counts the number of times sequences of tokens
 * appear.  Lists must be added in fully sorted order, otherwise the behavior is undefined.  Once the {@link CondensedTrie} has been
 * completed, a sequence of tokens can be used to walk through the {@link CondensedTrie} and count the weight of that particular sequence.
 </em>/
class CondensedTrie() {</p>

<pre><code>/**
 * The root node in the {@link CondensedTrie}.  This always has an emtpy label.
 */
val root = new PhraseNode("")
</code></pre>

<p>}
```</p>

<p>Now comes adding entries.  Since we want a clean and easy to use interface,
we'll be defensive and assume the elements aren't sorted, but they are already
tokenized, so each element in the given list is a sequence of tokens.   How you
sort thoes beasts is a homework assignment.</p>

<p>``` scala Adding elements to the Trie</p>

<pre><code>/**
 * Trains the {@link CondensedTrie} on a list of token sequences.  This list does not have to be sorted and will instead be sorted
 * before any sentences are added.
 */
def train(tokenizedSentences: Seq[List[String]]) {
    for ( tokenizedSentence &lt;- tokenizedSentences.sortWith(Util.tokenListComparator) )
        add(tokenizedSentence)
    if (root.linkMap.size != 0)
        replaceOrRegister(root)
}

/**
 * Adds the list of tokens to this {@link CondensedTrie}.
 */
def add(tweet: List[String]) {
    val (lastSharedNode, remainingSuffix) = computeDeepestCommonNodeAndSuffix(root, tweet)
    if (lastSharedNode.linkMap.size != 0)
        replaceOrRegister(lastSharedNode)
    addSuffix(lastSharedNode, remainingSuffix)
}

/**
 * Returns the deepest {@link PhraseNode} in the {@link CondensedTrie} matching the tokens in {@code tweet}.  When a {@link PhraseNode}
 * no longer has an arc matching the first element in {@code tweet}, this returns that {@link PhraseNode} and the remaining tokens in
 * {@code tweet} that cold not be matched.
 */
def computeDeepestCommonNodeAndSuffix(node: PhraseNode, tweet: List[String]) : (PhraseNode, List[String]) =
    tweet match {
        case head::tail =&gt; node.linkMap.get(head) match {
            case Some(child) =&gt; {
                child.addCount()
                computeDeepestCommonNodeAndSuffix(child, tail)
            }
            case None =&gt; (node, tweet)
        }
        case _ =&gt; (node, tweet)
    }

/**
 * Adds all tokens in {@code tweet} as a branch stemming from {@code node}
 */
def addSuffix(node: PhraseNode, tweet: List[String]) {
    tweet.foldLeft(node)( (n, t) =&gt; n.neighbor(t).addCount() )
}
</code></pre>

<p>```</p>

<p>These methods are mostly simple.  <code>addSuffix</code> starts adding links to nodes
starting from some initial point, note that nodes will automatically create a
new node for a word if one doesn't already exist.
<code>computeDeepestCommonNodeAndSuffix</code> walks down the Trie starting at the root
consuming each token that has a node and returns the deepest node reachable,
i.e. finds the node with the longest common prefix with a given sequence of
tokens.  Finally adding a single tweet depends on getting the prefix, doing some
magic called <code>replaceOrRegister</code> and then adding the suffix to the last node in
the longest prefix.  So, only question left, what is this registry business?</p>

<p>The registry keeps track of all nodes in the graph after they've been validated.
And what does validation entail?  It involves checking wether or not an existing
node already exists in the registry.  If one does, you simply replace that
duplicate node with the one in the registry.  If no such node exists, in goes
the node.  And this is exactly what <code>replaceOrRegister</code> does.  To do this
efficiently <em>and</em> correctly, we call <code>replaceOrRegister</code> on the last node in our
comment prefix and walk all the way down along the most recently added path,
i.e. the added by the <strong>last</strong> element we added, and then zip up any matching
nodes which correspond to matching suffixes.  By starting at the bottom, we match together end points which have no
children and merge them.</p>

<p>Take our archery example above, all three sentences end with "is legally blind."
After we add the first sentence, there would be a node for each token in the
order of the sentence.  When we add the second sentence and walk down to the
end, we see that "blind." has a duplicate, which we can merge.  Taking one step
backwards, we'll see that "legally" also has an exact match, where two nodes
with the same label point to the <strong>same exact</strong> node, the node we just merged.
And then thanks to recursion, we keep zipping things along until we get to
"who", which has no exact match, and we can stop zipping.  Walking through an
example like this should make the code below a little clearer.</p>

<p>``` scala Should i stay or should i go</p>

<pre><code>/**
 * Recursively walks down the chain of last nodes added starting at {@code node} and then checks if the last child of that node are in the
 * registry.  If an equivalent {@link PhraseNode} matching the last child is in the registry, this replaces the last child with the
 * registry node.  If no matching {@link PhraseNode} exists in the registry, then the last child is added to the registry.
 */
def replaceOrRegister(node: PhraseNode) {
    // Recursively replace or register the last added child of the current node.
    val child = node.lastAdded
    if (child.linkMap.size != 0)
        replaceOrRegister(child)

    // Get the possible matches for the last child.
    val candidateChildren = register.get(child.label)
    // Select only the registry node which has an exact match to the last
    // child.  We can also replace this equivalence check for a subsumption
    // check later on to condence the trie even more while breaking the
    // automata contract.
    candidateChildren.filter(matchMethod(_, child)) match {
        // If such a child exists, merge the counts of the last child to the
        // existing child and link the parent to the existing child.  This
        // is just a convenient way to match a list, which is what gets
        // returned by filter
        case existingChild :: tail =&gt; 
            existingChild.addCount(child.inCount)
            // Make sure to update the most recently added node with the
            // registry version!
            node.lastAdded = existingChild
            node.linkMap += (child.label -&gt; existingChild)
        // If no chld exists, put the last child in the registery.
        case _ =&gt; register.put(child.label, child)
    }
}
</code></pre>

<p>```</p>

<p>And Voila, we now have all the code needed to make a phrase graph!</p>

<p><img src="/images/example.exact.phrase-graph.svg" alt="An exact phrase graph using our example sentences" /></p>

<h3>Tweaking the automata to condense more phrases</h3>

<p>BUT! Suppose you want something more minimal?  Suppose you think it's kinda
funny that interjection of "who" prevents "guy SOMETHING OR NOTHING is" from
being a phrase.  Or you try adding in the sentence</p>

<pre><code>#archery ZOIDBERG by the Republic of Korea and the guy who is legally blind.
</code></pre>

<p>and notice how it creates an entirely new branch for "the Republic of Korea"
starting at "ZOIDBERG", thus making the number of times you think you've seen
that phrase dependent on the previous tokens.  Can we fix this?  YES!  All we
have to do is relax our definition of finding a matching element in the registry
to finding a node whose outgoing links are a superset of the most recently added
children.</p>

<p>And since Scala is awesome, we can do this with minimal effort.</p>

<p>``` scala Enhancing our Trie to be even more compressed
class CondensedTrie(useSubsumingMatches: Boolean = false) {</p>

<pre><code>/**
 * The filtering method for determining which candidate node from the register will replace existing children nodes during the
 * compaction phase.
 */
val matchMethod = if (useSubsumingMatches) subsumeMatch _ 

/**
 * Returns true if {@code child} and {@code candidate} are exact matches.
 */
def exactMatch(candidate: PhraseNode, child: PhraseNode) = 
    candidate == child

/**
 * Returns true if {@code child} and {@code candidate} have the same label and the links from {@code child} are a subset of the links
 * from {@code candidate}.
 */
def subsumeMatch(candidate: PhraseNode, child: PhraseNode) =
    if (candidate.label != child.label)
        false
    else
        child.linkMap.map{ case(key, subchild) =&gt;
            candidate.linkMap.get(key) match {
                case Some(otherSubchild) if otherSubchild.pointerHashCode == subchild.pointerHashCode =&gt; true
                case _ =&gt; false
            }
        }.foldLeft(true)(_&amp;&amp;_)
</code></pre>

<p><code>``
All we had to do was update the contractor to take in a boolean, then create a
new data member that links to one of two comparison functions for pairs of
nodes: 1) an exact matching function, which we would use for a true compressed
trie and 2) a subset matching function, to get our even more compressed
sorta-trie.  If we swap in</code>subsumeMatch`, we now get this phrase graph:</p>

<p>Let's see how the two versions handle this as input:</p>

<pre><code>Republic of Korea in archery by a guy who is legally blind
#archery by the Republic of Korea and by the guy is legally blind
#archery by the Republic of Korea and the guy is legally blind
#archery by the Republic of Korea in archery by a guy who is legally blind
#archery zoidberg by the Republic of Korea and by the guy is legally blind
#archery zoidberg by the Republic of Korea in archery by a guy who is legally blin
</code></pre>

<p>Using exact Matching:
<img src="/images/test.exact.svg" alt="Using Exact Matching" /></p>

<p>Using link subset Matching:
<img src="/images/test.subsume.svg" alt="Using Subset matching" /></p>

<p>Finally! This second version is <strong>precisely</strong> the data structure those three
original papers were describing.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sonatype and Scala: A relationship of trickery]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/06/02/sonatype-and-scala-a-relationship-of-trickery/"/>
    <updated>2012-06-02T22:28:00-07:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/06/02/sonatype-and-scala-a-relationship-of-trickery</id>
    <content type="html"><![CDATA[<p>Open source projects are amazing.  They let you easily share the work you do
with others so that they can iterate and improve upon your ideas, an especially
invaluable quality when you later move on to other things.  <a href="http://maven.apache.org/">Maven</a> in a very
similar fashion makes sharing open source (or even closed source) java projects
super easy.  If you structure your codebase in a <a href="http://maven.apache.org/">Maven</a> way and register an
account with <a href="https://oss.sonatype.org/">Sonatype</a>, you can get your project's jars distributed
worldwide so that anyone and everyone can used your stuff without having to
download and compile your code.</p>

<p>For pure java projects, this process is utterly <a href="https://docs.sonatype.org/display/Repository/Sonatype+OSS+Maven+Repository+Usage+Guide">trivial</a>, just follow some
instructions and it all works out beautifully.  But java is verbose and often
prefer to use <a href="http://www.scala-lang.org/">Scala</a>, especially for small prototype systems.  But how do
you deploy a mavenized scala project with <a href="https://oss.sonatype.org/">Sonatype</a>?  Well, it all pretty
much works out except that you need to create a jar of <strong>javadocs</strong> before
<a href="https://docs.sonatype.org/display/Repository/Sonatype+OSS+Maven+Repository+Usage+Guide#SonatypeOSSMavenRepositoryUsageGuide-6.CentralSyncRequirement">Sonatype</a> will let you publish anything.  Not being java and all, scala does
not do this out of the box, and maven won't either.  But, there's a cute hack
you can do if you're using maven 3.0.  Just add this snippet to your pom file
and watch the deployment rock:</p>

<p>``` xml
  <build></p>

<pre><code>&lt;plugins&gt;
  ...
  &lt;plugin&gt;
    &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;
    &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;
    &lt;version&gt;3.0.2&lt;/version&gt;
    &lt;executions&gt;
      &lt;execution&gt;
        &lt;id&gt;javadoc-jar&lt;/id&gt;
        &lt;phase&gt;package&lt;/phase&gt;
        &lt;goals&gt;
          &lt;goal&gt;doc-jar&lt;/goal&gt;
        &lt;/goals&gt;
      &lt;/execution&gt;
    &lt;/executions&gt;
  &lt;/plugin&gt;
&lt;/plugins&gt;
</code></pre>

<p>  </build>
```</p>

<p>This uses the versitile <a href="http://davidb.github.com/scala-maven-plugin/">scala maven plugin</a> and runs the <code>doc-jar</code> goal to
build scala docs every time you type <code>mvn package</code> or some command that depends
on <code>mvn package</code>, this includes <code>mvn deploy</code>.  The id bit creates a
<code>javadoc.jar</code> containing the scaladoc.  So with just that tidbit, you can deploy
your maven projects to <a href="https://oss.sonatype.org/">Sonatype</a> with no issues at all.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Stepping into the Mixtures with Gibbs Sampling]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/05/17/stepping-into-the-mixtures-with-gibbs-sampling/"/>
    <updated>2012-05-17T16:07:00-07:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/05/17/stepping-into-the-mixtures-with-gibbs-sampling</id>
    <content type="html"><![CDATA[<script src="http://fozziethebeat.github.com/javascripts/raphael-min.js"></script>


<script src="http://fozziethebeat.github.com/javascripts/raphael-extra.js"></script>


<script src="http://fozziethebeat.github.com/javascripts/graphing-resize.js"></script>


<h3>Refresher</h3>

<p>{% img right /images/empty_pool.jpg 300 600 Empty Pools = Not fun for diving %}
As stated before <a href="blog/2012/05/04/getting-to-the-mixtures-in-dirichlet-process-mixture-models/">here</a> and <a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">elsewhere</a>, mixture models are pretty cool.
But rather than diving head first into a complicated mixture model like the
<a href="http://en.wikipedia.org/wiki/Dirichlet_process">Dirichlet Process Mixture Model</a> or the <a href="http://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process">Hierarchical Dirichlet Process</a>,
we're going to ease our way into these models, just like you'd learn to accept
the horrors of a pool of water before you try deep sea diving.  First, we'll
figure out how to apply the <a href="http://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs Sampling Method</a> with the <a href="http://en.wikipedia.org/wiki/Linked_list">Linked List</a>
of machine learning methods, <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a>.  Then, after <a href="http://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs Sampling</a>
feels comfy, we'll tackle a <a href="http://en.wikipedia.org/wiki/Mixture_model">Finite Mixture Model</a> with <a href="http://en.wikipedia.org/wiki/Multivariate_normal_distribution">Gaussian</a>
components.  And with the magic of <a href="https://github.com/scalala/Scalala">Scalala</a> and <a href="http://www.scalanlp.org/">Scalanlp</a> the code'll be
short, sweet, and easy to understand.</p>

<h3>The Toy of Machine Learning Methods</h3>

<p>{% img left /images/Single_linked_list.png 400 200 Linked Lists = Tool for Learning %}
Simple but instructive data structures like a <a href="http://en.wikipedia.org/wiki/Linked_list">Linked List</a> work well as an
instructive tool.  Good libraries provide solid and efficient implementations,
but it's straightforward enough for beginners to implement in a variety of
manners.  For our goals, <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a> will serve the same purpose.  As simple
as the model is, it's reasonably powerful.</p>

<p>So what is the <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a> model?  Let's say we want to group a bundle of
emails into two groups: spam and not spam.  And all you have to go on are the
contents of the emails.  In this situation, we'd like to just use the words in a
single email to decide whether or not it's spamtastic or spamfree.  <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a>
assumes that you can make this decision based on each word in the
email individually and then combine these decisions.  It's <a href="http://en.wikipedia.org/wiki/Bayes'_rule">Bayesian</a>
because it's a simple <a href="http://en.wikipedia.org/wiki/Bayesian_network">Bayesian Model</a> and it's Naive because this approach
assumes the words are totally independent and have no statistical relation
whatsoever.</p>

<h3>Model Representations</h3>

<p>Due to it's simplicity, and that naive assumption, Naive Bayes makes for an easy
model to understand and describe using a variety of models.  here's some of the
most frequent descriptions:</p>

<h4>A Bayesian Network</h4>

<table>
<td>
<div id='naivebayes'></div>
<script type='text/javascript'>
(function() {

  function draw() {
        var paper = Raphael('naivebayes', 200, 300);
        paper.circle(60, 20, 20);
        paper.text(60, 20, "α")
             .attr({"font-size": 25});

        paper.arrow(60, 40, 60, 80, 10);

        paper.rect(10,60, 100, 150);

        paper.circle(60, 100, 20);
        paper.text(60, 100, "K")
             .attr({"font-size": 25});


        paper.circle(150, 100, 20);
        paper.text(150, 100, "γ")
             .attr({"font-size": 25});

        paper.arrow(150, 120, 150, 150, 10);

        paper.rect(120, 140, 60, 60);
        paper.circle(150, 170, 20);
        paper.text(150, 170, "θ")
             .attr({"font-size": 25});

        paper.arrow(130, 170, 80, 170, 10);

        paper.rect(29, 140, 60, 60);

        paper.arrow(60, 120, 60, 150, 10);

        paper.circle(60, 170, 20);
        paper.text(60, 170, "X")
             .attr({"font-size": 25});
    }

    draw();

    $(window).resize(function() {
        draw();
    });

})();
</script>
</td>
<td>

{% math %}
\begin{array}{lll}
\phi & \sim & Dirichlet(\alpha) \\
\theta_{s} & \sim & Dirichlet(\gamma_s) \\
\theta_{n} & \sim & Dirichlet(\gamma_n) \\
K_{j} & \sim & Multinomial(\phi) \\
X_j & \sim & Multinomial(\theta_{K_{j}}) 
\end{array}
{% endmath %}

</td>
</table>


<p>These two views of Naive Bayes describe the same model, but different ways
of computing it.  For the model on the right, if we work backwards, it says that
a data point {% m %} X_j {% em %} comes from some multinomial distribution {% m %} \theta_{K_j} {% em %}.  {% m %} K_j {% em %} is a latent variable that encode which
group, i.e. spam or not spam, the data point came from, and that itself comes
from a multinomial {% m %} \phi {% em %}.  The two {% m %} \theta_s, \theta_n {% em %} distributions describe our knowledge about spam and non-spam emails,
respectively.  And finally, all three multinomial distributions come from
<a href="http://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet</a> distributions.  The added magic of the <a href="http://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet</a>
distrubtion is that it <strong>generates</strong> multinomial distributions, this
relationship is more formally known as a <a href="http://en.wikipedia.org/wiki/Conjugate_prior">Conjugate Prior</a>.  The plate
diagram on the left says <strong>nearly</strong> the same story except it <em>collapses</em> out the {% m %} \phi {% em %} distribution and links {% m %} \alpha {% em %} directly to
the compnent labels.  We could also in theory collapse the {% m %} \theta {% em%} distrubtions, but we'll not do that
for the moment.</p>

<h4>A Graphical Model</h4>

<table>
<td>
<div id="nb-graph"></div>
</td>
<td>
<div id="nb-factor"></div>
</td>
</table>




<script type='text/javascript'>
(function() {

  function draw() {
        var paper = Raphael('nb-graph', 200, 200);

        paper.circle(100, 30, 20);
        paper.text(100, 30, "Y")
             .attr({"font-size": 20});

        paper.circle(30, 160, 20);
        paper.text(30, 160, "w_1")
             .attr({"font-size": 20});

        paper.circle(80, 160, 20);
        paper.text(80, 160, "w_2")
             .attr({"font-size": 20});

        paper.text(120, 160, "...")
             .attr({"font-size": 25});

        paper.circle(170, 160, 20);
        paper.text(170, 160, "w_v")
             .attr({"font-size": 20});

        paper.arrow(100, 50, 30, 140, 10);
        paper.arrow(100, 50, 80, 140, 10);
        paper.arrow(100, 50, 170, 140, 10);
    }

    draw();

    $(window).resize(function() {
        draw();
    });

})();
</script>




<script type='text/javascript'>
(function() {

  function draw() {
        var paper = Raphael('nb-fail', 200, 200);

        paper.circle(100, 30, 20);
        paper.text(100, 30, "Y")
             .attr({"font-size": 20});

        paper.circle(30, 160, 20);
        paper.text(30, 160, "w_1")
             .attr({"font-size": 20});

        paper.circle(80, 160, 20);
        paper.text(80, 160, "w_2")
             .attr({"font-size": 20});

        paper.text(120, 160, "...")
             .attr({"font-size": 25});

        paper.circle(170, 160, 20);
        paper.text(170, 160, "w_v")
             .attr({"font-size": 20});

        paper.factor(100, 50, 30, 140, 10);
        paper.factor(100, 50, 80, 140, 10);
        paper.factor(100, 50, 170, 140, 10);
    }

    draw();

    $(window).resize(function() {
        draw();
    });

})();
</script>


<p>This graphical view tells an alternative story.  Instead of probability
distrubutions making other distributions and then finally making data points.
this says that the class of a data point, {% m %} Y {% em %} generates the
features of the data point, {% m %}, w_1, w_2, \cdots, w_v {% em %}.  This is
where the naive part comes from, each "feature", i.e. word in the email, is
completely disconnected from the other words and only depends on the class of
the email.</p>

<p>{% img right /images/thomas_bayes.gif Thomas Bayes, not so Naive %}</p>

<h4>A Probability Distribution</h4>

<p>Finally, there's the written way to understand Naive Bayes, as a conditional
probability distrubtion.  Let's write out the distribution for just the spam
class:</p>

<p>{% math %}
p(spam | email) \propto p(spam) \prod_{word \in email} p(word | spam)
{% endmath %}</p>

<p>This looks pretty much the same for non-spam.  Simple, no? If we combine
together the different views above with this written form, then we can think of
the class {%m%}spam{%em%} as a <a href="http://en.wikipedia.org/wiki/Multinomial_distribution">Multinomial</a> distribution over words and the
probability of a word given the class spam is really just the probability of
picking the word based on how many times it's shown up in spammy emails.
Similarly, the probability of seeing spam is just the number of times you've
seen a delcious piece of spam, be it on your phone, at home, or in the frying
pan.  Using that intuition and some Bayesian math, the above definition gets
boild down to:</p>

<p>{% math %}
p(s|e) \propto \frac{C_{s} + \alpha_s - 1}{N + \alpha_s + \alpha_n- 1}</p>

<pre><code>            \prod_{w \in e} \theta_{s,w}^{C_{w,e}}
</code></pre>

<p>{% endmath %}</p>

<p>Let's get some definitions out of the way.</p>

<p>{% math %}
\begin{array}{lll}
C_s &amp;=&amp; \text{number of times spam's been seen} \
C<em>{w,e} &amp;=&amp; \text{number of times seeing word w in email e} \
N &amp;=&amp; \text{number of data points seen} \
\alpha</em>{s,n} &amp;=&amp; \text{priors for each class} \
\theta_{s,w} &amp;=&amp; \text{the wight given to word w in class spam} \
\end{array}
{% endmath %}</p>

<p>Let's tease this appart into two pieces, {% m %} p(spam) {% em %} and {%m%} p(email|spam) {%em%}.  The first part is:</p>

<p>{% math %}
p(s) \propto \frac{C<em>{s} + \alpha_s - 1}{N + \alpha_s + \alpha_n- 1}
{% endmath %}
This basically says that the probability of spam is the number of time's we've
seen it, plus some smoothing factors so that both span and non-spam have some
positive possibility of existing.  Amazingly! this is also equivalent to the
</em>collapsed_ {%m%}\phi{%em%} distribution we mentioned in the bayesian
description of Naive Bayes, by doing just a little bit of math and realizing
that the Dirichlet is a conjugate prior of a multinomial, we get the above
distribution for each class.</p>

<p>Next,
{% math %}
p(email|s) = \prod<em>{w \in e} \theta</em>{s,w}<sup>{C_{w,e}}</sup>
{% endmath %}</p>

<p>If {%m%}\theta_s{%em%} describes the probability of each word occuring in a
spammy email, then {%m%}\theta_{w,s}<sup>{C_{w,e}}{%em%}</sup> becomes the likelihood of
seeing that word as many times as observed in the email.  By computing the
product of all these words, we <em>naively</em> get the probability of the full email.
Slam the two distributions together and we get the full likelihood of the email
given the class spam.</p>

<h3>Don't forget Gibbs!</h3>

<p>Now that Naive Bayes makes some more sense in a variety of descriptive flavours
{% img right /images/Josiah_Willard_Gibbs_-from_MMS-.jpg 300 600 The man who made sampling cool before HipHop%}
and we've defined our distrubutions and hyper parameters, it's time to throw in
gibbs sampling.  To quickly refresh our memories, the recipe for gibbs at this
stage is:</p>

<ol>
<li>for each data point {%m%} X_j {%em%}

<ol>
<li>Forget about {%m%} X_j {%em%}</li>
<li>Compute the likelihood that each component, {%m%}\theta_*{%em%}, generated {%m%}X_j{%em%}</li>
<li>Sample and jot down a new component, {%m%}K_j{%em%}, for {%m%}X_J{%em%}</li>
<li>Remember {%m%} X_J {%em%}</li>
</ol>
</li>
<li>Restimate our component parameters, {%m%}\theta_{*} {%em%}</li>
<li>Repeat</li>
</ol>


<p>You might be wondering "why forget about the current data point?"  The reason is
that we're trying to decide what to do with a <strong>new data point</strong>.  Gibbs
sampling only really works because the distrubtions we're working with are
<a href="http://en.wikipedia.org/wiki/Exchangeable_random_variables">exchangeable</a>, i.e. the ordering of our data doesn't matter.  So we can
just pretend any data point is new by just forgetting it ever existed, Note,
this is why {%m%}p(spam){%em%} has a {%m%}-1{%em%} in the denominator, that count
has been "forgotten".  Other than that tidbit, we just pretend everything else
is the same.</p>

<h3>Writing out the code</h3>

<p>{% img left /images/spam_sampling.jpg 150 150 Tasty Samples of Spam %}
With Gibbs sampling fresh in our minds, and our probability distributions clear,
let's see what some real code looks like.  To do this, I'm going to use
<a href="https://github.com/scalala/Scalala">Scalala</a> and <a href="http://www.scalanlp.org/">Scalanlp</a>, which take care of the linear algebra and
probability distrubtions for us really concisely.  And while it may not be the
fastest library around, it's super easy to read and understand.  For faster
implementations, you can just translate this code into whatever beast you desire
and optimize away.</p>

<h4>Setting up before sampling</h4>

<p>Before we go on a spam sampling binge, we gotta setup our initial groupings.
Gibbs sampling works pretty well with a random starting point, so let's go with
that:</p>

<p>``` scala</p>

<pre><code>def sampleThetas(labelStats: Array[DenseVectorRow[Double]]) =
    labelStats.map(n_c =&gt; new Dirichlet(n_c+gamma).sample)

val k = numComponents
val v = numFeatures
val n = numDataPoints
var labelStats = Array.fill(k)(new DenseVectorRow[Double](
    Array.fill(v)(0d)))
var thetas = sampleThetas(labelStats)

val labels = new Multinomial(new Dirichlet(alpha).sample.data).sample(n).toArray
var labelCounts = new DenseVectorRow[Double](Array.fill(k)(0d))
data.zip(labels).foreach{
    case (x_j, j) =&gt; {labelStats(j) += x_j; labelCounts(j) += 1} }
</code></pre>

<p>```</p>

<p>So what's going on here?  Well, first, we just get some basic stats about our
data such as the size, the number of features and the number of components we
want, in this case, 2.  Then we make a data structure that will hold the number
of times we see each feature within each component, <code>labelStats</code>; the
number of times we've seen each component overall <code>labelCounts</code>; and the
parameters for each compnent, <code>thetas</code>.  Finally, randomly assign each point to
a random component and update the <code>labelStats</code> and <code>labelCounts</code>.   Note that we
get the <code>thetas</code> from a Dirichlet distribution that uses the <code>labelStats</code> and a
smoothing parameter <code>gamma</code>, just like we noted in the Bayesian Plate diagram
above.</p>

<p>Now for the real sampling meat:</p>

<p>``` scala</p>

<pre><code>for (i &lt;- 0 until numIterations) {
    for ( (x_j, j) &lt;- data.zipWithIndex ) {
        val label = labels(j)
        labelStats(label) -= x_j
        labelCounts(label) -= 1

        val prior = labelCounts + alpha - 1 / (n-1+alpha.sum)
        val likelihood = new DenseVectorRow[Double](thetas.map( theta =&gt;
            (theta :^ x_j).iterator.product).toArray)

        val probs = prior :* likelihood 
        val new_label= new Multinomial(probs / probs.sum).sample

        labelCounts(new_label) += 1
        labelStats(new_label) += x_j
        labels(j) = new_label
    }
    thetas = sampleThetas(labelStats)
}
</code></pre>

<p>```</p>

<p>The first three lines in the loop get the current label for <code>x_j</code> and then
forget about it's data.  <code>prior</code> represents the likelihood of selecting each
component just based on it's frequency, i.e. {%m%}p(spam){%em%}.  <code>likelihood</code>
represents the likelihood of each component generating <code>x_j</code>, i.e. {%m%} p(e|s){%em%}.
To finish off the sampling procedure, we sample a new label by turning these
likelihoods into a multinomial distrubtion and sample from it.  With the new
label we then add in the data for <code>x_j</code> to the that component's stats.  Once
we've made a full sweep through the data set, we update our parmeters for each
component by resampling from a Dirichlet distribution by using the our
<code>labelStats</code>.</p>

<p>And Voila! We have a simple gibbs sampler for Naive Bayes.</p>

<h3>Running it on some data</h3>

<p>Let's see how this unsupervised version of Naive Bayes handles a really generic
looking dataset: globs of data points from three different Gaussian
distributions, each with a different mean and variance.  And let's start off
with just trying to find 2 groups, as we've been working along with so far.</p>

<p>{% img right /images/test.nb.groups.png Splitting apart some simple groups %}</p>

<p>Looks pretty neat huh?  Even with a terrible and completely random starting
point, our learner manages to split up the data in a sensible way by finding a
dividing plane between two groups.  What's even cooler, is that notice how after
iteration 1 completes, nearly all the data is assigned to one class.  By somehow
by iteration 10, we've managed to finagle our way out of that terrible solution
and into a <strong>significantly</strong> better solution.  And by the 100th iteration, we've
stayed there in a stable state.</p>

<p>Now what happens if we try finding 3 components in the data set?</p>

<p>{% img right /images/test.nb.3.groups.png Splitting apart some simple groups %}</p>

<p>Here we get a similar split, but not what we're looking for.  Our mixture model
gets all befuddled with the group on the bottom left and tries to flailingly
split into two smaller groups.  If you run this a couple times, you'll see that
it does the same thing with the two green groups, it can't split them evenly as
you'd expect.</p>

<h3>The Secret no one mentions</h3>

<p>This simple model i've walked through is the unsupervised equivalent to the
supervised Naive Bayes model.  It does a pretty reasonable job of splitting
apart some groups of data, but there are clearly datasets it has some issues
dealing with.  But what's not been said so far is that this model is actually
not just Naive Bayes, it's a <a href="http://en.wikipedia.org/wiki/Mixture_model">Finite Mixture Model</a> with Multionomial
components.  The code, probabilities, and graphs i've layed out all work for
more than two components.  So next, i'll show two cool ways to extend this
model:</p>

<ol>
<li>With Gaussian components, which can better handle this toy data set</li>
<li>With an infinite number of components, which figure out that paramter <strong>for
you</strong> like a <em>magician</em>.</li>
</ol>


<p>{% img center /images/magician.jpg Magicians figure out their own parameters %}</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting to the mixtures in Dirichlet Process Mixture Models]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/05/04/getting-to-the-mixtures-in-dirichlet-process-mixture-models/"/>
    <updated>2012-05-04T13:01:00-07:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/05/04/getting-to-the-mixtures-in-dirichlet-process-mixture-models</id>
    <content type="html"><![CDATA[<p>The <a href="http://en.wikipedia.org/wiki/Dirichlet_process">Dirichlet Process</a> is pretty sweet as Edwin Chen and numerous others
have <a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">pointed out</a>.  When combined with a mixture of Bayesian models, it's
really effective for finding an accurate number of models needed to represent
your bundle of data, especially useful when you have no idea how many bundles of
related words you may have.  Edwin Chen does a really great job of
<a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">explaining</a> how the Dirichlet Process works, how to implement it, and how it
can break down the McDonald's menu, but he leaves out details on how to combine
it with mixture models, which is by far the coolest part.</p>

<p>The short description for how to merge the Dirichlet Process with <a href="http://en.wikipedia.org/wiki/Mixture_model">Nonparametric
Mixture Modeling</a> (and infer the model) looks like this:</p>

<ol>
<li>Define probability distributions for your mixture model</li>
<li>Rewrite your distributions with Bayes rule so that parameters depend on data
and hyper parameters</li>
<li>Repeat step 2 for hyper parameters as desired (it's turtles all the way down, as Kevin
Knight <a href="http://www.isi.edu/natural-language/people/bayes-with-tears.pdf">said</a>)</li>
<li>For each data point

<ol>
<li>Forget about the current data point</li>
<li>Compute the likelihood that each mixture made this data point</li>
<li>Sampling a new label for this data point using these likelihoods</li>
<li>Jot down this new label</li>
<li>Remember this data point</li>
</ol>
</li>
<li>After seeing all the data, re-estimate all your parameters based on the new
assignments</li>
<li>Repeat steps 4 and 5 ad nausea.</li>
</ol>


<p>In short, this is the <a href="http://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs Sampling</a> approach.  If you already know what
your distributions look like, and this paltry description made perfect sense,
Hurray! Please go bake a cake for everyone that feels a little underwhelmed with
this description.</p>

<p>{% img center /images/cake.jpg Delicious cake for people still reading %}</p>

<p>Now that someone is off baking us cake, let's dive down into what the
distributions look like.  Taking a look at a number of papers describing
variations of the Dirichlet Process Mixture Model, like <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.9111">The Infinite Gaussian Mixture Model</a>
and <a href="http://www.cs.princeton.edu/~blei/papers/BleiJordan2004.pdf">Variational Inference for Dirichlet Process Mixture Models</a>, they first
make things seem pretty straightforward.  To start, they generally throw down
this simple equation:</p>

<p>{% math %}
p(y|\theta_1, \cdots, \theta_k, \pi_1, \cdots, \pi_k) = \sum_j<sup>k</sup> \pi_j p(y|\theta_j)
{% endmath %}</p>

<p>Simple, no?  If you want mixtures of <a href="http://www.umiacs.umd.edu/~resnik/pubs/gibbs.pdf">Gaussian</a> models, then {%m%} \theta_j{%em%} has a mean and a variance, we'll call these {%m%} \mu_j{%em%} and {%m%}\sigma_j<sup>2{%em%}.</sup>   The next step is then to rewrite this equation in a couple of ways so that
you define the parameters, i.e. {% m %} \theta_j {% em %} in terms of the data
and other parameters.  One example: {% m %} \mu_j {% em %} looks like this:</p>

<p>{% math %}
p(\mu_j|c,y,s_j, \lambda, r) \sim \mathcal{N}\left( \frac{\bar{y}n_js_j + \lambda
r}{n_js_j + r},\frac{1}{n_js_j+r} \right)
{% endmath %}</p>

<p>Not <em>too</em> bad after reading through the definition for all the parameters; the
first bundle describes the mean of the mixture and the second bundle describes
the variance of the mixture.  You may have noticed that our means for each
mixture come from a Gaussian distribution themselves (I did say it was turtles
all the way down).  If you're using a nice math coding environment, like
<a href="https://github.com/scalala/Scalala">Scalala</a> and <a href="http://www.scalanlp.org/">Scalanlp</a>, you can easily sample from this distribution like
this</p>

<p><code>scala
val theta_j = new Gamma((y_mean*n_j*s_j + lambda*r)/(n_j*s_j+r), 1d/(n_j*s_j +r)).sample
</code></p>

<p>This goes on for most of the parameters for the model until you get to one of
the core tear inducing rewrites.  This lovely gem describes the probably
distribution function for one of the hyper parameters in the <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.9111">Infinite Gaussian Mixture Model</a>:</p>

<p>{% math %}
p(\alpha|k,n) \propto \frac{\alpha<sup>{k-3/2}</sup> exp(-1/(2\alpha))\Gamma(\alpha)}</p>

<pre><code>                       {\Gamma(\alpha / k)}
</code></pre>

<p>{% endmath %}</p>

<p><em>This is not easy to sample from</em> especially if you're new to sophisticated
sampling methods.  And sadly, just about every academic publication describing
these kinds of models gets to a point where they assume you know what they're
talking about and can throw down <a href="http://en.wikipedia.org/wiki/Mixture_model">Monte Carlo Markov Chain</a> sampling
procedures like they're rice at a wedding.</p>

<p>{% img center /images/wedding-rice.jpg MCMC sampling methods for the newlyweds %}</p>

<p>So what's a lowly non-statistician data modeller to do? Start with a well
written description of a similar, but significantly simpler model.  In this
case, <a href="http://www.umiacs.umd.edu/~resnik/pubs/gibbs.pdf">Gibbs Sampling for the Uninitiated</a>, which describes how to apply
Gibbs to the Naive Bayes model.  I'll summarize this awesome paper in the next
part of this series and then tweak it a little to make a <a href="http://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model">Finite Gaussian Mixture Model</a>.<br/>
After that's said and done, I'll show how to extend the finite version into the
Dirichlet Process Mixture Model (with some simplifications).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Puzzle Time: Transforming Names]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/04/23/puzzle-time-transforming-names/"/>
    <updated>2012-04-23T22:18:00-07:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/04/23/puzzle-time-transforming-names</id>
    <content type="html"><![CDATA[<p>This post starts what will hopefully be a new trend: computational solutions to
[NPR's Sunday Puzzle][1].  In this weeks puzzle, we have to</p>

<p>{% blockquote %}
Think of a common man's name in four letters, one syllable. Move each letter exactly halfway around the alphabet. For example, A would become N, N would become A, and B would become O. The result will be a common woman's name in two syllables. What names are these?
{% endblockquote %}</p>

<p>Word puzzles can often be pretty hard for automated computer programs.  This is
in part what made [IBM's Watson][2] so darn cool.  But thankfully this week's
puzzle is amazingly simple for a program, in fact, i'd even say it's ideal for a
program.  To solve it, just need two simple steps:</p>

<ol>
<li>Gather a bunch of male and female names</li>
<li>Process the male names as noted to see if we have any matching female names</li>
</ol>


<p>Step 1 is perhaps the hardest since it requires gathering not only names, but
common names in english.  However, [this][3] handy site has done the work for
us.  Here they have listings of common English names based on a variety of
categories: region, ranking, gender, alphabetical, etc.  For our purposes, we'll
just grab the a long list of the most frequent male and female names from
[here][4] and [here][5].  All the names are nicely laid out in a table with a
pretty regular format.  In fact the format is so regular we can write a [<em>regular expression</em>][6] for it.  Here's an example of the table:</p>

<p>``` html</p>

<table align="left" CELLPADDING="3" CELLSPACING="3" style="table1">
<tr bgcolor="F5FDD9"><td><b>Name&nbsp;&nbsp;</td><td><b>% Frequency&nbsp;&nbsp;</td><td><b>Approx Number&nbsp;&nbsp;</td><td><b>Rank&nbsp;&nbsp;</td></tr>
<tr bgcolor="white"><td>AARON</td><td>0.24</td><td> 350,151 </td><td>77</td></tr>
<tr bgcolor="white"><td>ABDUL</td><td>0.007</td><td> 10,213 </td><td>831</td></tr>
<tr bgcolor="F5FDD9"><td>ABE</td><td>0.006</td><td> 8,754 </td><td>854</td></tr>
<tr bgcolor="white"><td>ABEL</td><td>0.019</td><td> 27,720 </td><td>485</td></tr>
<tr bgcolor="white"><td>ABRAHAM</td><td>0.035</td><td> 51,064 </td><td>347</td></tr>
<tr bgcolor="white"><td>ABRAM</td><td>0.005</td><td> 7,295 </td><td>1053</td></tr>
<tr bgcolor="F5FDD9"><td>ADALBERTO</td><td>0.005</td><td> 7,295 </td><td>1040</td></tr>
<tr bgcolor="white"><td>ADAM</td><td>0.259</td><td> 377,871 </td><td>69</td></tr>
```

Regular right?  It's so darn simple we can use some simple command line tools to
clean it up:

```
cat female_names_alpha.htm | grep '<tr bgcolor="' | sed "s/.*<td>\([A-Z]\+\).*/\1/" | tail -n +2 > female_names.txt
```

If we downloaded the female names into `female_names_alpha.htm`, the first part
will simply print out the contents of the html page to standard out.  The second
part, the grep command, will catch all rows in the table, which thankfully have
the same start prefix.  Part three extracts the names nestled in a row element.
And finally the tail part eliminates the first row of the table, e.g. the head
row, which grep accidentally catches.  This'll grab us a ton of names.

So what's next?  Well, we have a bunch of male and female names.  But we've got
too many, namely we have names that clearly won't work as they have too many or
too few letters.  So let's so some more cleaning, but this time in scala:

``` scala
val maleNames = Source.fromFile(args(0)).getLines.filter(_.size == 4)
```

This dandy line reads the text from the file and filters out any lines that have
more than four characters.  Since our initial processing placed each name on a
line, this in effect removes any names with more than four letters.  If we do
this with the female names, we'll have lists of common four letter names for the
two genders.  For later use, we'll also turn the female names into a set by
calling `.toSet` on the list.

Last, we can do the transformation on each male name to see if it's a valid
female name and then print out any combinations that match.  The next three
lines do this slickly:

``` scala
maleNames.map(w => (w, w.map(l => (((l-'A'+13) % 26) + 'A').toChar).toString))
         .filter( bg => femaleNames.contains(bg._2))
         .foreach(println)
```

This works in three steps.  First, the `map` call transforms each male name into
a `tuple` which consists of the original male name, and then a _potential_
female name by sliding up each letter in the name by 13 letters.  With ascii
characters, this is done just by first grounding the letter to 0, by subtracting
the char value for `A`, adding 13, and then rolling over the value to be back
in the range of the 26 letter characters by taking the modulo (`%`) and finally
bumping the character value into the range of real ascii characters by adding
`A`.   That's all done in the first line.  Step two is to simply throw out any
generated female names that don't show up in our list of female names, done by
line 3.  And the final line just prints out any matches we get.  With our name
lists, this turns out to be:

```
(ERIN,REVA)
(EVAN,RINA)
(GLEN,TYRA)
(IVAN,VINA)
(RYAN,ELNA)
```

The rules of the game stipulated that the male name had to have one syllable and
the female name has to have two.  We didn't code for this at all since that
parts slightly more complicated, and since our pre-processing reduced the set of
options down to five, we can easily pick out a valid answer.  In this case,
"Glen" and "Tyra" or "Ryan" and "Elna" look like valid responses.  I'd vote for
the first pair since I haven't heard "Elna" in forever.


  [1]: http://www.npr.org/2012/04/22/151120151/a-puzzle-worthy-of-don-draper
  [2]: http://www-03.ibm.com/innovation/us/watson/index.html
  [3]: http://names.mongabay.com
  [4]: http://names.mongabay.com/male_names.htm
  [5]: http://names.mongabay.com/female_names.htm
  [6]: http://en.wikipedia.org/wiki/Regular_expression

]]></content>
  </entry>
  
</feed>
