<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Wordsi by Fozzie The Beat]]></title>
  <link href="http://fozziethebeat.github.com/atom.xml" rel="self"/>
  <link href="http://fozziethebeat.github.com/"/>
  <updated>2012-08-15T12:02:47+09:00</updated>
  <id>http://fozziethebeat.github.com/</id>
  <author>
    <name><![CDATA[Keith Stevens]]></name>
    <email><![CDATA[fozziethebeat@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Building A Phrase Graph]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/08/12/building-a-phrase-graph/"/>
    <updated>2012-08-12T12:13:00+09:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/08/12/building-a-phrase-graph</id>
    <content type="html"><![CDATA[<p>Research papers.  I hate them sometimes.  They present a great idea, talk about
how it can be used and applied, and then give only the barest description of how
to actually build and implement the idea, often with no pointers or links to
what they built.  My current frustration is with building a <strong>Phrase Graph</strong>.
The idea behind phrase graphs are pretty simple, they encode a large set of
sentences with a minimal automata.</p>

<p>Here&#8217;s a simple example.  Say you have the following two sentences:</p>

<pre><code>#archery by the Republic of Korea and the guy is legally blind.
#archery by the Republic of Korea and the guy who is legally blind.
#archery by the Republic of Korea in archery by a guy who is legally blind.
</code></pre>

<p>There&#8217;s quite a lot of overlap at the start of the sentence and at the end of
the sentence.  So a good phrase graph would look something like this:</p>

<p><img src="http://fozziethebeat.github.com/images/phrase-graph-example.svg" alt="A simple Phrase Graph" /></p>

<p>So you can see that the nodes in the graph represent words found in the
sentences observed and if you weight the nodes based on how often they are
traversed, you can start to detect which phrases are used most frequently.  But
how does one do this?  And how does one do this efficiently?  This is where
research papers make me mad.  They fail to point out the simplest algorithms for
building these awesome ideas.  To complement these research ideas, this post&#8217;ll
give a little more detail on what these phrase graphs are, an easy way to build
them using existing libraries, and code to write your own custom phrase graph!</p>

<h2>Badly described methods for building a phrase graph</h2>

<p>The first paper I read for building phrase graphs, titled <a href="http://www.jeffreynichols.com/papers/summary-iui2012.pdf">Summarizing Sporting
events using Twitter</a>, gives this highly detailed algorithm description:</p>

<blockquote><p>The phrase graph consists of a node for each word appearing in any status update, and an edge between each set of two words that are used adjacently in any status update</p></blockquote>


<p>Seems easy to implement, no?  Here&#8217;s a more detailed algorithm, found in
<a href="http://www.cs.uccs.edu/~jkalita/papers/2010/SharifiBeauxSocialcom2010.pdf">Experiments in Microblog Summarization</a>:</p>

<blockquote><p>To construct the left-hand side, the algorithm starts with the root node. It reduces the set of input sentences to the set of sentences that contain the current node’s phrase. The current node and the root node are initially the same. Since every input sentence is guaranteed to contain the root phrase, our list of sentences does not change initially. Subsequently, the algorithm isolates the set of words that occur immediately before the current node’s phrase. From this set, duplicate words are combined and assigned a count that represents how many instances of those words are detected. For each of these unique words, the algorithm adds them to the graph as nodes with their associated counts to the left of the current node.</p></blockquote>


<p>This gives a lot more detail on what the phrase graph contains, and an easy
enough algorithm, but it&#8217;s not exactly a <strong>fast</strong> algorithm, especially if you
want to do this using 10 million tweets about the Olympics.  Both descriptions
leave out a key detail: these phrase graphs are really just <a href="http://en.wikipedia.org/wiki/Trie#Compressing_tries">Compressed Tries</a>.</p>

<h2>Tries and their compressed cousins</h2>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/b/be/Trie_example.svg" alt="A simple Trie" /></p>

<p><a href="http://en.wikipedia.org/wiki/Trie#Compressing_tries">Tries</a> are one of the simplest data structures, and one of the most powerful
when processing natural languages.  Given a set of words or sentences, a
<a href="http://en.wikipedia.org/wiki/Trie#Compressing_tries">Trie</a> is essentially a standard tree where the leaves represent observed
words or sentences.  The power of it is that each internal node in the <a href="http://en.wikipedia.org/wiki/Trie#Compressing_tries">Trie</a>
represents overlapping sequences. So if you want to build a <a href="http://en.wikipedia.org/wiki/Trie#Compressing_tries">Trie</a> for an <a href="http://www.brics.dk/automaton/">English
Dictionary</a>, the root node would be a blank character, which then points to a
node for each letter of the alphabet.  From the &#8220;a&#8221; child, you would then have
access to all words starting with &#8220;a&#8221;, and the further down the <a href="http://en.wikipedia.org/wiki/Trie#Compressing_tries">Trie</a> you
go, you get longer prefixes of words.</p>

<p>Now a <em>Phrase Graph</em> is essentially a <a href="http://en.wikipedia.org/wiki/Trie#Compressing_tries">Trie</a> which condenses not only shared
prefixes, but also any shared subsequence, be they in the middle, or the end.
Formally, they are directed acyclic graphs, and if they are treated as a lookup
structure, ala a dictionary, they are called <em>Minimal Acyclic Finite-State
Automata</em>.  And there&#8217;s plenty of fast and simple ways to build these things.
The easiest places to start reading about these is <a href="http://acl.ldc.upenn.edu/J/J00/J00-1002.pdf">Incremental Construction of
Minimal Acyclic Finite State Automata</a>. The <a href="http://www.brics.dk/automaton/">Brics Automaton package</a> also
provides a really good implantation for these that works</p>

<figure class='code'><figcaption><span>A simple example using the Brics package</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">import</span> <span class="nn">dk.brics.automaton.BasicAutomata</span>
</span><span class='line'><span class="k">val</span> <span class="n">automata</span> <span class="k">=</span> <span class="nc">BasicAutomata</span><span class="o">.</span><span class="n">makeStringUnion</span><span class="o">(</span>
</span><span class='line'>    <span class="s">&quot;#archery by the Republic of Korea and the guy is legally blind&quot;</span><span class="o">,</span>
</span><span class='line'>    <span class="s">&quot;#archery by the Republic of Korea in archery by a guy who is legally blind&quot;</span><span class="o">)</span>
</span><span class='line'><span class="n">println</span><span class="o">(</span><span class="n">automata</span><span class="o">.</span><span class="n">toDot</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>The above code snippet will generate this simple minimal automata:</p>

<p><img src="http://fozziethebeat.github.com/images/brics-phrase-graph-example.svg" alt="A GraphViz version of the Brics Automata" /></p>

<p>*Note, you may want to open this in a new tab to zoom in as every letter has
it&#8217;s own state.</p>

<h2>Rolling your own automata builder</h2>

<p>Using <a href="http://www.brics.dk/automaton/">Brics</a> works really well if you just want to check whether or not a
sentence matches one seen in a corpus.  However, it doesn&#8217;t easily let you check
how often particular sub-phrases are used within the corpus.  For that kinda
power, you&#8217;ll have to craft your own implemenation.  And now it&#8217;s time to share
the very code to do this!</p>

<h3>Nodes in the graph, that&#8217;s where.</h3>

<p>Where does one start?  First, you need a node data structure, with some very
carefully crafted functions to determine equality (which indidentally, the
research papers <strong>don&#8217;t</strong> point out).</p>

<figure class='code'><figcaption><span>The PhraseNode data structure</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="cm">/**</span>
</span><span class='line'><span class="cm"> * A simple node structure that records a label, a weight, and a mapping from this node to other nodes using labeled arcs.  This</span>
</span><span class='line'><span class="cm"> * implementation overrides {@link hashCode} and {@link equals} such that only nodes with the same label and which point to the same exact</span>
</span><span class='line'><span class="cm"> * children (i.e.  same objects, not equivalent objects), are considered equal.</span>
</span><span class='line'><span class="cm"> */</span>
</span><span class='line'><span class="k">class</span> <span class="nc">PhraseNode</span><span class="o">(</span><span class="k">val</span> <span class="n">label</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="cm">/**</span>
</span><span class='line'><span class="cm">     * The internal weight for this {@link PhraseNode}.</span>
</span><span class='line'><span class="cm">     */</span>
</span><span class='line'>    <span class="k">var</span> <span class="n">inCount</span> <span class="k">=</span> <span class="mi">0</span><span class="n">d</span>
</span><span class='line'>
</span><span class='line'>    <span class="cm">/**</span>
</span><span class='line'><span class="cm">     * A mapping from this {@link PhraseNode} to children {@link PhraseNode}s using labeled arcs.</span>
</span><span class='line'><span class="cm">     */</span>
</span><span class='line'>    <span class="k">var</span> <span class="n">linkMap</span> <span class="k">=</span> <span class="nc">Map</span><span class="o">[</span><span class="kt">String</span>,<span class="kt">PhraseNode</span><span class="o">]()</span>
</span><span class='line'>
</span><span class='line'>    <span class="cm">/**</span>
</span><span class='line'><span class="cm">     * A record of the last {@link PhraseNode} added as a child to this {@link PhraseNode}.</span>
</span><span class='line'><span class="cm">     */</span>
</span><span class='line'>    <span class="k">var</span> <span class="n">lastAdded</span><span class="k">:</span><span class="kt">PhraseNode</span> <span class="o">=</span> <span class="kc">null</span>
</span><span class='line'>
</span><span class='line'>    <span class="cm">/**</span>
</span><span class='line'><span class="cm">     * Returns the {@link PhraseNode} connected to {@code this} {@link PhraseNode} via the arc {@code term}.  If no such node exists, a new</span>
</span><span class='line'><span class="cm">     * {@link PhraseNode} is created and returned.</span>
</span><span class='line'><span class="cm">     */</span>
</span><span class='line'>    <span class="k">def</span> <span class="n">neighbor</span><span class="o">(</span><span class="n">term</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=</span>
</span><span class='line'>        <span class="n">linkMap</span><span class="o">.</span><span class="n">get</span><span class="o">(</span><span class="n">term</span><span class="o">)</span> <span class="k">match</span> <span class="o">{</span>
</span><span class='line'>            <span class="k">case</span> <span class="nc">Some</span><span class="o">(</span><span class="n">node</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">node</span>
</span><span class='line'>            <span class="k">case</span> <span class="nc">None</span> <span class="k">=&gt;</span> <span class="o">{</span> <span class="n">lastAdded</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">PhraseNode</span><span class="o">(</span><span class="n">term</span><span class="o">)</span>
</span><span class='line'>                           <span class="n">linkMap</span> <span class="o">+=</span> <span class="o">(</span><span class="n">term</span> <span class="o">-&gt;</span> <span class="n">lastAdded</span><span class="o">)</span>
</span><span class='line'>                           <span class="n">lastAdded</span>
</span><span class='line'>                         <span class="o">}</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="cm">/**</span>
</span><span class='line'><span class="cm">     * Adds {@code delta} to the {@code inCount} and returns a pointer to {@code this} {@link PhraseNode}.</span>
</span><span class='line'><span class="cm">     */</span>
</span><span class='line'>    <span class="k">def</span> <span class="n">addCount</span><span class="o">(</span><span class="n">delta</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="mi">1</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">inCount</span> <span class="o">+=</span> <span class="n">delta</span>
</span><span class='line'>        <span class="k">this</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="cm">/**</span>
</span><span class='line'><span class="cm">     * Returns a hashcode based on java&#39;s internal hash code method for every object which uniquely identifies every object.</span>
</span><span class='line'><span class="cm">     */</span>
</span><span class='line'>    <span class="k">def</span> <span class="n">pointerHashCode</span> <span class="k">=</span> <span class="k">super</span><span class="o">.</span><span class="n">hashCode</span>
</span><span class='line'>
</span><span class='line'>    <span class="cm">/**</span>
</span><span class='line'><span class="cm">     * Override {@code hashCode} to use three factors:</span>
</span><span class='line'><span class="cm">     * &lt;ol&gt;</span>
</span><span class='line'><span class="cm">     *  &lt;li&gt;The hash code for {@code label}&lt;/li&gt;</span>
</span><span class='line'><span class="cm">     *  &lt;li&gt;The hash code for {@code label} of each child node&lt;/li&gt;</span>
</span><span class='line'><span class="cm">     *  &lt;li&gt;The hash code for {@code pointer} of each child node&lt;/li&gt;</span>
</span><span class='line'><span class="cm">     * &lt;/ol&gt;</span>
</span><span class='line'><span class="cm">     * This ensures that nodes only have the same hash code if they have the same label, same number of children, same links to those</span>
</span><span class='line'><span class="cm">     * children, and point to the very same children.  This is a cheap and fast way to ensure that we don&#39;t accidently consider two nodes</span>
</span><span class='line'><span class="cm">     * with the same link labels aren&#39;t equivalent.</span>
</span><span class='line'><span class="cm">     */</span>
</span><span class='line'>    <span class="k">override</span> <span class="k">def</span> <span class="n">hashCode</span> <span class="k">=</span>
</span><span class='line'>        <span class="n">linkMap</span><span class="o">.</span><span class="n">map</span><span class="o">{</span> <span class="k">case</span><span class="o">(</span><span class="n">childLabel</span><span class="o">,</span> <span class="n">child</span><span class="o">)</span> <span class="k">=&gt;</span>
</span><span class='line'>            <span class="n">childLabel</span><span class="o">.</span><span class="n">hashCode</span> <span class="o">^</span> <span class="n">child</span><span class="o">.</span><span class="n">pointerHashCode</span>
</span><span class='line'>        <span class="o">}.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">label</span><span class="o">.</span><span class="n">hashCode</span><span class="o">)(</span><span class="n">_^_</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="cm">/**</span>
</span><span class='line'><span class="cm">     * Override {@code equals} to use the same three factors as {@cod hachCode}:K</span>
</span><span class='line'><span class="cm">     * &lt;ol&gt;</span>
</span><span class='line'><span class="cm">     *  &lt;li&gt;The {@code label}&lt;/li&gt;</span>
</span><span class='line'><span class="cm">     *  &lt;li&gt;The {@code label} of each child node&lt;/li&gt;</span>
</span><span class='line'><span class="cm">     *  &lt;li&gt;The {@code pointer} of each child node&lt;/li&gt;</span>
</span><span class='line'><span class="cm">     * &lt;/ol&gt;</span>
</span><span class='line'><span class="cm">     * </span>
</span><span class='line'><span class="cm">     * This ensures that nodes only equal when they have the same distinguishing meta data and point to the same children.</span>
</span><span class='line'><span class="cm">     */</span>
</span><span class='line'>    <span class="k">override</span> <span class="k">def</span> <span class="n">equals</span><span class="o">(</span><span class="n">that</span><span class="k">:</span> <span class="kt">Any</span><span class="o">)</span> <span class="k">=</span>
</span><span class='line'>        <span class="n">that</span> <span class="k">match</span> <span class="o">{</span>
</span><span class='line'>            <span class="k">case</span> <span class="n">other</span><span class="k">:</span> <span class="kt">PhraseNode</span> <span class="o">=&gt;</span> <span class="k">if</span> <span class="o">(</span><span class="k">this</span><span class="o">.</span><span class="n">hashCode</span> <span class="o">!=</span> <span class="n">other</span><span class="o">.</span><span class="n">hashCode</span><span class="o">)</span> <span class="kc">false</span>
</span><span class='line'>                                      <span class="k">else</span> <span class="k">if</span> <span class="o">(</span><span class="k">this</span><span class="o">.</span><span class="n">label</span> <span class="o">!=</span> <span class="n">other</span><span class="o">.</span><span class="n">label</span><span class="o">)</span> <span class="kc">false</span>
</span><span class='line'>                                      <span class="k">else</span> <span class="n">compareLinkMaps</span><span class="o">(</span><span class="k">this</span><span class="o">.</span><span class="n">linkMap</span><span class="o">,</span> <span class="n">other</span><span class="o">.</span><span class="n">linkMap</span><span class="o">)</span>
</span><span class='line'>            <span class="k">case</span> <span class="k">_</span> <span class="k">=&gt;</span> <span class="kc">false</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="cm">/**</span>
</span><span class='line'><span class="cm">     * Returns true if the two maps have the same size, same keys, and the key in each map points to the same object.  We use this instead</span>
</span><span class='line'><span class="cm">     * of simply calling equals between the two maps because we want to check node equality using just the pointer hash code, which prevents</span>
</span><span class='line'><span class="cm">     * walking down the entire graph structure from each node.</span>
</span><span class='line'><span class="cm">     */</span>
</span><span class='line'>    <span class="k">def</span> <span class="n">compareLinkMaps</span><span class="o">(</span><span class="n">lmap1</span><span class="k">:</span> <span class="kt">Map</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">PhraseNode</span><span class="o">],</span> <span class="n">lmap2</span><span class="k">:</span> <span class="kt">Map</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">PhraseNode</span><span class="o">])</span> <span class="k">:</span> <span class="kt">Boolean</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">if</span> <span class="o">(</span><span class="n">lmap1</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="n">lmap2</span><span class="o">.</span><span class="n">size</span><span class="o">)</span>
</span><span class='line'>            <span class="k">return</span> <span class="kc">false</span>
</span><span class='line'>        <span class="k">for</span> <span class="o">(</span> <span class="o">(</span><span class="n">key1</span><span class="o">,</span> <span class="n">entry1</span><span class="o">)</span> <span class="k">&lt;-</span> <span class="n">lmap1</span> <span class="o">)</span> <span class="o">{</span>
</span><span class='line'>            <span class="k">val</span> <span class="n">matched</span> <span class="k">=</span> <span class="n">lmap2</span><span class="o">.</span><span class="n">get</span><span class="o">(</span><span class="n">key1</span><span class="o">)</span> <span class="k">match</span> <span class="o">{</span>
</span><span class='line'>                <span class="k">case</span> <span class="nc">Some</span><span class="o">(</span><span class="n">entry2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">entry2</span><span class="o">.</span><span class="n">pointerHashCode</span> <span class="o">==</span> <span class="n">entry1</span><span class="o">.</span><span class="n">pointerHashCode</span>
</span><span class='line'>                <span class="k">case</span> <span class="nc">None</span> <span class="k">=&gt;</span> <span class="kc">false</span>
</span><span class='line'>            <span class="o">}</span>
</span><span class='line'>            <span class="k">if</span> <span class="o">(!</span><span class="n">matched</span><span class="o">)</span>
</span><span class='line'>                <span class="k">return</span> <span class="kc">false</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>        <span class="kc">true</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>This <code>PhraseNode</code> has three fairly simple data members, a label that records
which word the node represents, the weight of the node, and a map from this node
to it&#8217;s children based on their labels.  The tricky part of this node is how you
determine equality.  Two nodes can be equal in two different senses: 1) they are
the exact same data structure in memory, and so their memory locations are the
same or 2) they have the same label and point to the same exact children in
memory.  Checking the first type of equality is easy, you can compare the hash
code of their addresses using the default <code>hashCode</code> method java provides for
every object.  Checking the second form of equality is more challenging to do
efficiently.  The naive way would be to recursively check that all children
eventually point to sub-graphs with the same structure.  However, checking the
hash code of the pointers of each children is <strong>much</strong> faster and accomplishes
the same goal.  Hence, this is why we override <code>hashCode</code> and <code>equals</code> with such
complicated code.</p>

<h3>Linking together those Phrase Nodes</h3>

<p>The algorithm for linking together <code>PhraseNodes</code> such that they form a minimal
transducer relies on a few interesting tricks and <strong>beautiful</strong> recursion.  The
first trick we need is lexicographically sorted input.  By sorting the input,
you&#8217;re maximizing the size of matching prefixes between any neighboring words you want
to put in the transducer.  So let&#8217;s look at how we do that adding in sorted
order.</p>

<p>Before we get there though, let&#8217;s flesh sketch out a <code>CondensedTrie</code> data
structure.  It&#8217;s pretty simple.  It starts off with just having a single
<code>PhraseNode</code> element, the root.</p>

<figure class='code'><figcaption><span>Not the data structure we deserve, but the data structure we need</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="cm">/**</span>
</span><span class='line'><span class="cm"> * The {@link CondensedTrie} represents a single phrase graph centered around a single key phrase.  Lists of tokens, representing sentences,</span>
</span><span class='line'><span class="cm"> * can be added to the {@link CondensedTrie} to create a minimal finite state automata which counts the number of times sequences of tokens</span>
</span><span class='line'><span class="cm"> * appear.  Lists must be added in fully sorted order, otherwise the behavior is undefined.  Once the {@link CondensedTrie} has been</span>
</span><span class='line'><span class="cm"> * completed, a sequence of tokens can be used to walk through the {@link CondensedTrie} and count the weight of that particular sequence.</span>
</span><span class='line'><span class="cm"> */</span>
</span><span class='line'><span class="k">class</span> <span class="nc">CondensedTrie</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="cm">/**</span>
</span><span class='line'><span class="cm">     * The root node in the {@link CondensedTrie}.  This always has an emtpy label.</span>
</span><span class='line'><span class="cm">     */</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">root</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">PhraseNode</span><span class="o">(</span><span class="s">&quot;&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now comes adding entries.  Since we want a clean and easy to use interface,
we&#8217;ll be defensive and assume the elements aren&#8217;t sorted, but they are already
tokenized, so each element in the given list is a sequence of tokens.   How you
sort thoes beasts is a homework assignment.</p>

<figure class='code'><figcaption><span>Adding elements to the Trie</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="cm">/**</span>
</span><span class='line'><span class="cm"> * Trains the {@link CondensedTrie} on a list of token sequences.  This list does not have to be sorted and will instead be sorted</span>
</span><span class='line'><span class="cm"> * before any sentences are added.</span>
</span><span class='line'><span class="cm"> */</span>
</span><span class='line'><span class="k">def</span> <span class="n">train</span><span class="o">(</span><span class="n">tokenizedSentences</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">List</span><span class="o">[</span><span class="kt">String</span><span class="o">]])</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">for</span> <span class="o">(</span> <span class="n">tokenizedSentence</span> <span class="k">&lt;-</span> <span class="n">tokenizedSentences</span><span class="o">.</span><span class="n">sortWith</span><span class="o">(</span><span class="nc">Util</span><span class="o">.</span><span class="n">tokenListComparator</span><span class="o">)</span> <span class="o">)</span>
</span><span class='line'>        <span class="n">add</span><span class="o">(</span><span class="n">tokenizedSentence</span><span class="o">)</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(</span><span class="n">root</span><span class="o">.</span><span class="n">linkMap</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="mi">0</span><span class="o">)</span>
</span><span class='line'>        <span class="n">replaceOrRegister</span><span class="o">(</span><span class="n">root</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="cm">/**</span>
</span><span class='line'><span class="cm"> * Adds the list of tokens to this {@link CondensedTrie}.</span>
</span><span class='line'><span class="cm"> */</span>
</span><span class='line'><span class="k">def</span> <span class="n">add</span><span class="o">(</span><span class="n">tweet</span><span class="k">:</span> <span class="kt">List</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="o">(</span><span class="n">lastSharedNode</span><span class="o">,</span> <span class="n">remainingSuffix</span><span class="o">)</span> <span class="k">=</span> <span class="n">computeDeepestCommonNodeAndSuffix</span><span class="o">(</span><span class="n">root</span><span class="o">,</span> <span class="n">tweet</span><span class="o">)</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(</span><span class="n">lastSharedNode</span><span class="o">.</span><span class="n">linkMap</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="mi">0</span><span class="o">)</span>
</span><span class='line'>        <span class="n">replaceOrRegister</span><span class="o">(</span><span class="n">lastSharedNode</span><span class="o">)</span>
</span><span class='line'>    <span class="n">addSuffix</span><span class="o">(</span><span class="n">lastSharedNode</span><span class="o">,</span> <span class="n">remainingSuffix</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="cm">/**</span>
</span><span class='line'><span class="cm"> * Returns the deepest {@link PhraseNode} in the {@link CondensedTrie} matching the tokens in {@code tweet}.  When a {@link PhraseNode}</span>
</span><span class='line'><span class="cm"> * no longer has an arc matching the first element in {@code tweet}, this returns that {@link PhraseNode} and the remaining tokens in</span>
</span><span class='line'><span class="cm"> * {@code tweet} that cold not be matched.</span>
</span><span class='line'><span class="cm"> */</span>
</span><span class='line'><span class="k">def</span> <span class="n">computeDeepestCommonNodeAndSuffix</span><span class="o">(</span><span class="n">node</span><span class="k">:</span> <span class="kt">PhraseNode</span><span class="o">,</span> <span class="n">tweet</span><span class="k">:</span> <span class="kt">List</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="k">:</span> <span class="o">(</span><span class="kt">PhraseNode</span><span class="o">,</span> <span class="kt">List</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="k">=</span>
</span><span class='line'>    <span class="n">tweet</span> <span class="k">match</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">case</span> <span class="n">head</span><span class="o">::</span><span class="n">tail</span> <span class="k">=&gt;</span> <span class="n">node</span><span class="o">.</span><span class="n">linkMap</span><span class="o">.</span><span class="n">get</span><span class="o">(</span><span class="n">head</span><span class="o">)</span> <span class="k">match</span> <span class="o">{</span>
</span><span class='line'>            <span class="k">case</span> <span class="nc">Some</span><span class="o">(</span><span class="n">child</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">{</span>
</span><span class='line'>                <span class="n">child</span><span class="o">.</span><span class="n">addCount</span><span class="o">()</span>
</span><span class='line'>                <span class="n">computeDeepestCommonNodeAndSuffix</span><span class="o">(</span><span class="n">child</span><span class="o">,</span> <span class="n">tail</span><span class="o">)</span>
</span><span class='line'>            <span class="o">}</span>
</span><span class='line'>            <span class="k">case</span> <span class="nc">None</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">node</span><span class="o">,</span> <span class="n">tweet</span><span class="o">)</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>        <span class="k">case</span> <span class="k">_</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">node</span><span class="o">,</span> <span class="n">tweet</span><span class="o">)</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="cm">/**</span>
</span><span class='line'><span class="cm"> * Adds all tokens in {@code tweet} as a branch stemming from {@code node}</span>
</span><span class='line'><span class="cm"> */</span>
</span><span class='line'><span class="k">def</span> <span class="n">addSuffix</span><span class="o">(</span><span class="n">node</span><span class="k">:</span> <span class="kt">PhraseNode</span><span class="o">,</span> <span class="n">tweet</span><span class="k">:</span> <span class="kt">List</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">tweet</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="n">node</span><span class="o">)(</span> <span class="o">(</span><span class="n">n</span><span class="o">,</span> <span class="n">t</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">n</span><span class="o">.</span><span class="n">neighbor</span><span class="o">(</span><span class="n">t</span><span class="o">).</span><span class="n">addCount</span><span class="o">()</span> <span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>These methods are mostly simple.  <code>addSuffix</code> starts adding links to nodes
starting from some initial point, note that nodes will automatically create a
new node for a word if one doesn&#8217;t already exist.
<code>computeDeepestCommonNodeAndSuffix</code> walks down the Trie starting at the root
consuming each token that has a node and returns the deepest node reachable,
i.e. finds the node with the longest common prefix with a given sequence of
tokens.  Finally adding a single tweet depends on getting the prefix, doing some
magic called <code>replaceOrRegister</code> and then adding the suffix to the last node in
the longest prefix.  So, only question left, what is this registry business?</p>

<p>The registry keeps track of all nodes in the graph after they&#8217;ve been validated.
And what does validation entail?  It involves checking wether or not an existing
node already exists in the registry.  If one does, you simply replace that
duplicate node with the one in the registry.  If no such node exists, in goes
the node.  And this is exactly what <code>replaceOrRegister</code> does.  To do this
efficiently <em>and</em> correctly, we call <code>replaceOrRegister</code> on the last node in our
comment prefix and walk all the way down along the most recently added path,
i.e. the added by the <strong>last</strong> element we added, and then zip up any matching
nodes which correspond to matching suffixes.  By starting at the bottom, we match together end points which have no
children and merge them.</p>

<p>Take our archery example above, all three sentences end with &#8220;is legally blind.&#8221;
After we add the first sentence, there would be a node for each token in the
order of the sentence.  When we add the second sentence and walk down to the
end, we see that &#8220;blind.&#8221; has a duplicate, which we can merge.  Taking one step
backwards, we&#8217;ll see that &#8220;legally&#8221; also has an exact match, where two nodes
with the same label point to the <strong>same exact</strong> node, the node we just merged.
And then thanks to recursion, we keep zipping things along until we get to
&#8220;who&#8221;, which has no exact match, and we can stop zipping.  Walking through an
example like this should make the code below a little clearer.</p>

<figure class='code'><figcaption><span>Should i stay or should i go</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="cm">/**</span>
</span><span class='line'><span class="cm"> * Recursively walks down the chain of last nodes added starting at {@code node} and then checks if the last child of that node are in the</span>
</span><span class='line'><span class="cm"> * registry.  If an equivalent {@link PhraseNode} matching the last child is in the registry, this replaces the last child with the</span>
</span><span class='line'><span class="cm"> * registry node.  If no matching {@link PhraseNode} exists in the registry, then the last child is added to the registry.</span>
</span><span class='line'><span class="cm"> */</span>
</span><span class='line'><span class="k">def</span> <span class="n">replaceOrRegister</span><span class="o">(</span><span class="n">node</span><span class="k">:</span> <span class="kt">PhraseNode</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="c1">// Recursively replace or register the last added child of the current node.</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">child</span> <span class="k">=</span> <span class="n">node</span><span class="o">.</span><span class="n">lastAdded</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(</span><span class="n">child</span><span class="o">.</span><span class="n">linkMap</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="mi">0</span><span class="o">)</span>
</span><span class='line'>        <span class="n">replaceOrRegister</span><span class="o">(</span><span class="n">child</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// Get the possible matches for the last child.</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">candidateChildren</span> <span class="k">=</span> <span class="n">register</span><span class="o">.</span><span class="n">get</span><span class="o">(</span><span class="n">child</span><span class="o">.</span><span class="n">label</span><span class="o">)</span>
</span><span class='line'>    <span class="c1">// Select only the registry node which has an exact match to the last</span>
</span><span class='line'>    <span class="c1">// child.  We can also replace this equivalence check for a subsumption</span>
</span><span class='line'>    <span class="c1">// check later on to condence the trie even more while breaking the</span>
</span><span class='line'>    <span class="c1">// automata contract.</span>
</span><span class='line'>    <span class="n">candidateChildren</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">matchMethod</span><span class="o">(</span><span class="n">_</span><span class="o">,</span> <span class="n">child</span><span class="o">))</span> <span class="k">match</span> <span class="o">{</span>
</span><span class='line'>        <span class="c1">// If such a child exists, merge the counts of the last child to the</span>
</span><span class='line'>        <span class="c1">// existing child and link the parent to the existing child.  This</span>
</span><span class='line'>        <span class="c1">// is just a convenient way to match a list, which is what gets</span>
</span><span class='line'>        <span class="c1">// returned by filter</span>
</span><span class='line'>        <span class="k">case</span> <span class="n">existingChild</span> <span class="o">::</span> <span class="n">tail</span> <span class="k">=&gt;</span>
</span><span class='line'>            <span class="n">existingChild</span><span class="o">.</span><span class="n">addCount</span><span class="o">(</span><span class="n">child</span><span class="o">.</span><span class="n">inCount</span><span class="o">)</span>
</span><span class='line'>            <span class="c1">// Make sure to update the most recently added node with the</span>
</span><span class='line'>            <span class="c1">// registry version!</span>
</span><span class='line'>            <span class="n">node</span><span class="o">.</span><span class="n">lastAdded</span> <span class="k">=</span> <span class="n">existingChild</span>
</span><span class='line'>            <span class="n">node</span><span class="o">.</span><span class="n">linkMap</span> <span class="o">+=</span> <span class="o">(</span><span class="n">child</span><span class="o">.</span><span class="n">label</span> <span class="o">-&gt;</span> <span class="n">existingChild</span><span class="o">)</span>
</span><span class='line'>        <span class="c1">// If no chld exists, put the last child in the registery.</span>
</span><span class='line'>        <span class="k">case</span> <span class="k">_</span> <span class="k">=&gt;</span> <span class="n">register</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="n">child</span><span class="o">.</span><span class="n">label</span><span class="o">,</span> <span class="n">child</span><span class="o">)</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>And Voila, we now have all the code needed to make a phrase graph!</p>

<p><img src="http://fozziethebeat.github.com/images/example.exact.phrase-graph.svg" alt="An exact phrase graph using our example sentences" /></p>

<h3>Tweaking the automata to condense more phrases</h3>

<p>BUT! Suppose you want something more minimal?  Suppose you think it&#8217;s kinda
funny that interjection of &#8220;who&#8221; prevents &#8220;guy SOMETHING OR NOTHING is&#8221; from
being a phrase.  Or you try adding in the sentence</p>

<pre><code>#archery ZOIDBERG by the Republic of Korea and the guy who is legally blind.
</code></pre>

<p>and notice how it creates an entirely new branch for &#8220;the Republic of Korea&#8221;
starting at &#8220;ZOIDBERG&#8221;, thus making the number of times you think you&#8217;ve seen
that phrase dependent on the previous tokens.  Can we fix this?  YES!  All we
have to do is relax our definition of finding a matching element in the registry
to finding a node whose outgoing links are a superset of the most recently added
children.</p>

<p>And since Scala is awesome, we can do this with minimal effort.</p>

<figure class='code'><figcaption><span>Enhancing our Trie to be even more compressed</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">class</span> <span class="nc">CondensedTrie</span><span class="o">(</span><span class="n">useSubsumingMatches</span><span class="k">:</span> <span class="kt">Boolean</span> <span class="o">=</span> <span class="kc">false</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="cm">/**</span>
</span><span class='line'><span class="cm">     * The filtering method for determining which candidate node from the register will replace existing children nodes during the</span>
</span><span class='line'><span class="cm">     * compaction phase.</span>
</span><span class='line'><span class="cm">     */</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">matchMethod</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="n">useSubsumingMatches</span><span class="o">)</span> <span class="n">subsumeMatch</span> <span class="k">_</span>
</span><span class='line'>
</span><span class='line'>    <span class="cm">/**</span>
</span><span class='line'><span class="cm">     * Returns true if {@code child} and {@code candidate} are exact matches.</span>
</span><span class='line'><span class="cm">     */</span>
</span><span class='line'>    <span class="k">def</span> <span class="n">exactMatch</span><span class="o">(</span><span class="n">candidate</span><span class="k">:</span> <span class="kt">PhraseNode</span><span class="o">,</span> <span class="n">child</span><span class="k">:</span> <span class="kt">PhraseNode</span><span class="o">)</span> <span class="k">=</span>
</span><span class='line'>        <span class="n">candidate</span> <span class="o">==</span> <span class="n">child</span>
</span><span class='line'>
</span><span class='line'>    <span class="cm">/**</span>
</span><span class='line'><span class="cm">     * Returns true if {@code child} and {@code candidate} have the same label and the links from {@code child} are a subset of the links</span>
</span><span class='line'><span class="cm">     * from {@code candidate}.</span>
</span><span class='line'><span class="cm">     */</span>
</span><span class='line'>    <span class="k">def</span> <span class="n">subsumeMatch</span><span class="o">(</span><span class="n">candidate</span><span class="k">:</span> <span class="kt">PhraseNode</span><span class="o">,</span> <span class="n">child</span><span class="k">:</span> <span class="kt">PhraseNode</span><span class="o">)</span> <span class="k">=</span>
</span><span class='line'>        <span class="k">if</span> <span class="o">(</span><span class="n">candidate</span><span class="o">.</span><span class="n">label</span> <span class="o">!=</span> <span class="n">child</span><span class="o">.</span><span class="n">label</span><span class="o">)</span>
</span><span class='line'>            <span class="kc">false</span>
</span><span class='line'>        <span class="k">else</span>
</span><span class='line'>            <span class="n">child</span><span class="o">.</span><span class="n">linkMap</span><span class="o">.</span><span class="n">map</span><span class="o">{</span> <span class="k">case</span><span class="o">(</span><span class="n">key</span><span class="o">,</span> <span class="n">subchild</span><span class="o">)</span> <span class="k">=&gt;</span>
</span><span class='line'>                <span class="n">candidate</span><span class="o">.</span><span class="n">linkMap</span><span class="o">.</span><span class="n">get</span><span class="o">(</span><span class="n">key</span><span class="o">)</span> <span class="k">match</span> <span class="o">{</span>
</span><span class='line'>                    <span class="k">case</span> <span class="nc">Some</span><span class="o">(</span><span class="n">otherSubchild</span><span class="o">)</span> <span class="k">if</span> <span class="n">otherSubchild</span><span class="o">.</span><span class="n">pointerHashCode</span> <span class="o">==</span> <span class="n">subchild</span><span class="o">.</span><span class="n">pointerHashCode</span> <span class="k">=&gt;</span> <span class="kc">true</span>
</span><span class='line'>                    <span class="k">case</span> <span class="k">_</span> <span class="k">=&gt;</span> <span class="kc">false</span>
</span><span class='line'>                <span class="o">}</span>
</span><span class='line'>            <span class="o">}.</span><span class="n">foldLeft</span><span class="o">(</span><span class="kc">true</span><span class="o">)(</span><span class="n">_&amp;&amp;_</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>All we had to do was update the contractor to take in a boolean, then create a
new data member that links to one of two comparison functions for pairs of
nodes: 1) an exact matching function, which we would use for a true compressed
trie and 2) a subset matching function, to get our even more compressed
sorta-trie.  If we swap in <code>subsumeMatch</code>, we now get this phrase graph:</p>

<p>Let&#8217;s see how the two versions handle this as input:</p>

<pre><code>Republic of Korea in archery by a guy who is legally blind
#archery by the Republic of Korea and by the guy is legally blind
#archery by the Republic of Korea and the guy is legally blind
#archery by the Republic of Korea in archery by a guy who is legally blind
#archery zoidberg by the Republic of Korea and by the guy is legally blind
#archery zoidberg by the Republic of Korea in archery by a guy who is legally blin
</code></pre>

<p>Using exact Matching:
<img src="http://fozziethebeat.github.com/images/test.exact.svg" alt="Using Exact Matching" /></p>

<p>Using link subset Matching:
<img src="http://fozziethebeat.github.com/images/test.subsume.svg" alt="Using Subset matching" /></p>

<p>Finally! This second version is <strong>precisely</strong> the data structure those three
original papers were describing.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building Visualizations to Test Summarizations]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/08/03/building-visualizations-to-test-summarizations/"/>
    <updated>2012-08-03T12:08:00+09:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/08/03/building-visualizations-to-test-summarizations</id>
    <content type="html"><![CDATA[<p>I&#8217;m currently working on interesting on-line methods for summarizing streams of
documents.  The basic idea is that documents come hurtling into your inbox at a
startling rate and you&#8217;d like a quick, easy, online method to summarize that
they&#8217;re about.  A lot of approaches to text summarization use an offline
approach, meaning that those methods inspect <em>all</em> the documents.  That&#8217;s not
practical, especially if you want to, oh say, do this thing on all the tweets
about the ongoing <a href="http://www.london2012.com/">2012 Olympics in London</a>.  So my goal is to work up a good
enough algorithm for doing this process completely online.  Even though it sadly
won&#8217;t be working well enough to actually run online while the Olympics is going
on (I&#8217;m still working on said algorithm), it could be pretty cool.</p>

<p>However, figuring out if you&#8217;re doing something right or wrong on many million
tweets about 50 different sports is kind of challenging.  So while i&#8217;m gathering
ton of data to process, and then processing it all, I figured I should design a
night UI for exploring the results.  Being a terrible UI guy I thought I could
never pull it off, but thanks to the magicians behind <a href="http://square.github.com/crossfilter/">Crossfilter</a> and
<a href="http://d3js.org/">D3.js</a>, it turned out to be pretty easy.  The result of my UI wizardry is
currently <a href="http://fozziethebeat.github.com/tweetolympics/">here</a>.  And while there&#8217;s quite a lot more to add, such a way to
select other sports or other summarization methods, it does the bulk of what I
want:</p>

<ol>
<li>It builds histograms of tweet&#8217;s based on three dimensions: the date, the
hour, and the &#8220;cluster&#8221; of the tweet.</li>
<li>It lets you select sub-regions of these dimensions and automatically updates
the histograms for other selection files.  So if you put a range on the day,
you can see the histograms according to hour and cluster for that date range.</li>
<li>Given a range, you can also see the most representative, or summary, tweets
for the most frequent clusters in that range.  There&#8217;s still a little bit
missing, I should really be ordering the summaries by their time, but
that&#8217;ll come later.</li>
</ol>


<p>As complicated as all that initially sounds, I barely had to write any
JavaScript on my own, which is truly fortunate since I barely know JavaScript.</p>

<h2>The joy of making that UI.</h2>

<p>Since I know next to nothing about JavaScript, <a href="http://d3js.org/">D3.js</a>, and <a href="http://square.github.com/crossfilter/">Crossfire</a>, I
did a lot of hacking, console debugging, and total guessing to make this beast
work.  So here&#8217;s a quick rundown on what these three things are doing together
and how they synergize into my current app.  There&#8217;s still quite a bit I don&#8217;t
know, so i&#8217;ll mostly focus on what I figured out in my hackings.</p>

<h3>Loading that dataset</h3>

<p>Cross filter arrays of key-valye javascript objects, which can be easily pulled
out of <a href="http://en.wikipedia.org/wiki/Comma-separated_values">Comma Separated Files</a>.  However, those initial object arrays are
totally untyped, so you need to do some processing to shuffle values out of raw
strings into something more usable.  I&#8217;m currently using two styles of data: 1) one
format that simple records the time of a tweet and it&#8217;s cluster identifier and
2) one that records the cluster identifier, the time of the first tweet in that
cluster, the time of the average tweet in that cluster, and the summary tweet.
They&#8217;re pretty simple and look like this:</p>

<figure class='code'><figcaption><span>Example data for the tweet groups</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='html'><span class='line'>Date,Time,Group
</span><span class='line'>07272344,1343400289,1
</span><span class='line'>07272351,1343400698,2
</span><span class='line'>07272351,1343400706,3
</span></code></pre></td></tr></table></div></figure>


<p>and</p>

<figure class='code'><figcaption><span>Example data for the tweet summaries</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='html'><span class='line'>StartTime,MeanTime,Group,Summary
</span><span class='line'>1343458794,1343400289,1,I cannot wait for the swimming diving and gymnastics #London2012
</span><span class='line'>1343458793,1343400698,2,Hey @mdoolittle #olympics day today. I hope we&#39;ll have comments from you esp. 4 #gymnastics parts! ;) I&#39;m an EX-gymnast too(it shows!lol)
</span><span class='line'>1343458791,1343400706,3,looking forward to the opening ceremony tonight just disappointed that SABC wont be showing much gymnastics #Olympics2012
</span></code></pre></td></tr></table></div></figure>


<p><a href="http://d3js.org/">D3</a> makes this super easy to handle.  All you do is call <code>d3.csv(fileName, callback)</code>.  In my example, this turns out to be:</p>

<figure class='code'><figcaption><span>How to process the summaries</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="nx">d3</span><span class="p">.</span><span class="nx">csv</span><span class="p">(</span><span class="s2">&quot;/crossfilter/tweet.gymnastics.particle.mean.all.splits.json&quot;</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">summaries</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>    <span class="c1">// Add in types to the summaries.</span>
</span><span class='line'>    <span class="nx">summaries</span><span class="p">.</span><span class="nx">forEach</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">d</span><span class="p">,</span> <span class="nx">i</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>       <span class="nx">d</span><span class="p">.</span><span class="nx">index</span> <span class="o">=</span> <span class="nx">i</span><span class="p">;</span>
</span><span class='line'>       <span class="nx">d</span><span class="p">.</span><span class="nx">group</span> <span class="o">=</span> <span class="nb">parseInt</span><span class="p">(</span><span class="nx">d</span><span class="p">.</span><span class="nx">Group</span><span class="p">);</span>
</span><span class='line'>       <span class="nx">d</span><span class="p">.</span><span class="nx">startTime</span> <span class="o">=</span> <span class="nx">parseTime</span><span class="p">(</span><span class="nx">d</span><span class="p">.</span><span class="nx">StartTime</span><span class="p">);</span>
</span><span class='line'>       <span class="nx">d</span><span class="p">.</span><span class="nx">meanTime</span> <span class="o">=</span> <span class="nx">parseTime</span><span class="p">(</span><span class="nx">d</span><span class="p">.</span><span class="nx">MeanTime</span><span class="p">);</span>
</span><span class='line'>    <span class="p">});</span>
</span></code></pre></td></tr></table></div></figure>


<p>and</p>

<figure class='code'><figcaption><span>How to process the groups</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="nx">d3</span><span class="p">.</span><span class="nx">csv</span><span class="p">(</span><span class="s2">&quot;/crossfilter/tweet.gymnastics.particle.mean.all.groups.json&quot;</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">tweets</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>    <span class="c1">// Add in types to the tweets.</span>
</span><span class='line'>    <span class="nx">tweets</span><span class="p">.</span><span class="nx">forEach</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">d</span><span class="p">,</span> <span class="nx">i</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>        <span class="nx">d</span><span class="p">.</span><span class="nx">index</span> <span class="o">=</span> <span class="nx">i</span><span class="p">;</span>
</span><span class='line'>        <span class="nx">d</span><span class="p">.</span><span class="nx">group</span> <span class="o">=</span> <span class="nb">parseInt</span><span class="p">(</span><span class="nx">d</span><span class="p">.</span><span class="nx">Group</span><span class="p">);</span>
</span><span class='line'>        <span class="nx">d</span><span class="p">.</span><span class="nx">time</span> <span class="o">=</span> <span class="nx">parseDate</span><span class="p">(</span><span class="nx">d</span><span class="p">.</span><span class="nb">Date</span><span class="p">);</span>
</span><span class='line'>        <span class="nx">d</span><span class="p">.</span><span class="nx">date</span> <span class="o">=</span> <span class="nx">parseTime</span><span class="p">(</span><span class="nx">d</span><span class="p">.</span><span class="nx">Time</span><span class="p">);</span>
</span><span class='line'>    <span class="p">});</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Crossing the filters on that data</h3>

<p>Once you&#8217;ve got data loaded, you gotta do something with it, no?
<a href="http://square.github.com/crossfilter/">Crossfilter</a> lets you do some super powerful things with very little work.
The primary job of cross filter is to take your array of objects and let you
select different dimensions to act as keys in that array.  Initially your key is
just the index of the array.  But after calling <code>dimension</code> on a crossfiltered
object, you can select any variable in your object to be a key.  Since I wanted
three charts, that means I need three keys: 1) a key on the day, 2) a key on the
hour, and 3) a key on the cluster id.  I also want counts for the number of
tweets in the bins corresponding to each dimension.  That sounds like a lot of
work, but it&#8217;s as easy as this:</p>

<figure class='code'><figcaption><span>Create dimensions for the charts.</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="c1">// Create the crossfilter over the tweets.</span>
</span><span class='line'><span class="kd">var</span> <span class="nx">tweet</span> <span class="o">=</span> <span class="nx">crossfilter</span><span class="p">(</span><span class="nx">tweets</span><span class="p">),</span>
</span><span class='line'>    <span class="c1">// This groups all tweets together.</span>
</span><span class='line'>    <span class="nx">all</span> <span class="o">=</span> <span class="nx">tweet</span><span class="p">.</span><span class="nx">groupAll</span><span class="p">(),</span>
</span><span class='line'>    <span class="c1">// Select the day of the tweet as a dimension and compute the counts.</span>
</span><span class='line'>    <span class="nx">date</span> <span class="o">=</span> <span class="nx">tweet</span><span class="p">.</span><span class="nx">dimension</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">d</span><span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="nx">d3</span><span class="p">.</span><span class="nx">time</span><span class="p">.</span><span class="nx">day</span><span class="p">(</span><span class="nx">d</span><span class="p">.</span><span class="nx">date</span><span class="p">);</span> <span class="p">}),</span>
</span><span class='line'>    <span class="nx">dates</span> <span class="o">=</span> <span class="nx">date</span><span class="p">.</span><span class="nx">group</span><span class="p">(),</span>
</span><span class='line'>    <span class="c1">// Select the hour of the tweet as a dimension and compute the counts.</span>
</span><span class='line'>    <span class="nx">hour</span> <span class="o">=</span> <span class="nx">tweet</span><span class="p">.</span><span class="nx">dimension</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">d</span><span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="nx">d</span><span class="p">.</span><span class="nx">date</span><span class="p">.</span><span class="nx">getHours</span><span class="p">()</span> <span class="o">+</span> <span class="nx">d</span><span class="p">.</span><span class="nx">date</span><span class="p">.</span><span class="nx">getMinutes</span><span class="p">()</span> <span class="o">/</span> <span class="mi">60</span><span class="p">;</span> <span class="p">}),</span>
</span><span class='line'>    <span class="nx">hours</span> <span class="o">=</span> <span class="nx">hour</span><span class="p">.</span><span class="nx">group</span><span class="p">(</span><span class="nb">Math</span><span class="p">.</span><span class="nx">floor</span><span class="p">),</span>
</span><span class='line'>    <span class="c1">// Select the cluster id of the tweet as a dimension and compute the counts.</span>
</span><span class='line'>    <span class="nx">cluster</span> <span class="o">=</span> <span class="nx">tweet</span><span class="p">.</span><span class="nx">dimension</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">d</span><span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="nx">d</span><span class="p">.</span><span class="nx">group</span><span class="p">;</span> <span class="p">}),</span>
</span><span class='line'>    <span class="nx">clusters</span> <span class="o">=</span> <span class="nx">cluster</span><span class="p">.</span><span class="nx">group</span><span class="p">();</span>
</span></code></pre></td></tr></table></div></figure>


<p>That&#8217;s it! All you need is two lines to select a dimension for your chart and
compute the data for the histogram.  Easy Breezy.</p>

<h3>Charting those groups</h3>

<p>Now that you&#8217;ve got some dimensions set up and some counts to go along with
them, it&#8217;s time to plot those fine numbers.  For each chart you want, all you
have to do is note what dimension you want to use, provide the summary counts,
put some limits on the plots, then apply all that to some plotting object like a
bar graph.  I&#8217;m just using bar charts, but <a href="http://nickqizhu.github.com/dc.js/">this other crossfilter example</a>
gives some sweet alternatives you can re-use.</p>

<figure class='code'><figcaption><span>Making the hourly chart</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="c1">// The first chart tracks the hours of each tweet.  It has the</span>
</span><span class='line'><span class="c1">// standard 24 hour time range and uses a 24 hour clock.</span>
</span><span class='line'><span class="nx">barChart</span><span class="p">().</span><span class="nx">dimension</span><span class="p">(</span><span class="nx">hour</span><span class="p">)</span>
</span><span class='line'>          <span class="p">.</span><span class="nx">group</span><span class="p">(</span><span class="nx">hours</span><span class="p">)</span>
</span><span class='line'>          <span class="c1">// Setup the type of dimension you want and the range of values that</span>
</span><span class='line'>          <span class="c1">// are valid.  Note that I&#39;m still a little fuzzy on this part.</span>
</span><span class='line'>          <span class="p">.</span><span class="nx">x</span><span class="p">(</span><span class="nx">d3</span><span class="p">.</span><span class="nx">scale</span><span class="p">.</span><span class="nx">linear</span><span class="p">()</span>
</span><span class='line'>                     <span class="p">.</span><span class="nx">domain</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">24</span><span class="p">])</span>
</span><span class='line'>                     <span class="p">.</span><span class="nx">rangeRound</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span> <span class="o">*</span> <span class="mi">24</span><span class="p">])),</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Printing the tweet summaries</h3>

<p>The fun part is printing all the summaries for the tweets that have been
selected.  The original <a href="http://square.github.com/crossfilter/">Crossfilter</a> example was pretty simple, it just
printout out the actual rows being selected in the histograms.  But I wanted to
do something more complicated.  I wanted to figure out which clusters existed in
the selection, get the summaries attached to each cluster (and only one copy
of the summary per cluster), and then organize the summaries by date.  Not
knowing javascript, that sounded kinda hard.  In my candy land language,
[Scala][7], it&#8217;s pretty easy to do with some groupBys and maps, but does
javascript have all this?  YES!  Turns out the <code>clusters</code> object` computed to
print the histogram has nearly everything I want, the collection of cluster
identifiers found in the current filter selection.  And since all arrays in
JavaScript have a map operator, I can get the array of summaries I so desperately
desired.</p>

<figure class='code'><figcaption><span>Getting those summaries for the selected tweets</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="c1">// Map each of the top clusters to their corresponding summaries.</span>
</span><span class='line'><span class="c1">// Note that the entries in clusters use group identifiers starting at 1, so we</span>
</span><span class='line'><span class="c1">// have to subtract by 1 to make them valid indices.</span>
</span><span class='line'><span class="kd">var</span> <span class="nx">clusterSummaries</span> <span class="o">=</span> <span class="nx">clusters</span><span class="p">.</span><span class="nx">top</span><span class="p">(</span><span class="mi">40</span><span class="p">).</span><span class="nx">map</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">d</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>    <span class="k">return</span> <span class="nx">summaries</span><span class="p">[</span><span class="nx">d</span><span class="p">.</span><span class="nx">key</span><span class="o">-</span><span class="mi">1</span><span class="p">];</span>
</span><span class='line'><span class="p">});</span>
</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>Next comes the cool part, creating a hierarchy on the cluster summaries based on
the date.  These two lines together do that magic:</p>

<figure class='code'><figcaption><span>Building a nice heirarchy on the tweets according to date</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="kd">var</span> <span class="nx">nestByDate</span> <span class="o">=</span> <span class="nx">d3</span><span class="p">.</span><span class="nx">nest</span><span class="p">().</span><span class="nx">key</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">d</span><span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="nx">d3</span><span class="p">.</span><span class="nx">time</span><span class="p">.</span><span class="nx">day</span><span class="p">(</span><span class="nx">d</span><span class="p">.</span><span class="nx">startTime</span><span class="p">);</span> <span class="p">});</span>
</span><span class='line'><span class="c1">// Group the summaries by their date.</span>
</span><span class='line'><span class="kd">var</span> <span class="nx">tweetsByGroup</span> <span class="o">=</span> <span class="nx">nestByDate</span><span class="p">.</span><span class="nx">entries</span><span class="p">(</span><span class="nx">clusterSummaries</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>The first line creates an object that will nest any array of items with a
<code>startTime</code> attribute according to their day and the second line runs that
nester over the cluster summaries to get a mapping from days to arrays of
summaries occuring on each day.  Using that nested object, you can build a table
of tweet summaries for each day by attaching the data object to the list div
holding those tables:</p>

<figure class='code'><figcaption><span>Attaching the nested data to the html</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="c1">// For each day&#39;s summaries, add them as a table with the date as</span>
</span><span class='line'><span class="c1">// the header.</span>
</span><span class='line'><span class="nx">div</span><span class="p">.</span><span class="nx">each</span><span class="p">(</span><span class="kd">function</span><span class="p">()</span> <span class="p">{</span>
</span><span class='line'>  <span class="c1">// This binds the date nested group of tweet summaries to the `.date` element</span>
</span><span class='line'>  <span class="c1">// of the `summary-list div.</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">date</span> <span class="o">=</span> <span class="nx">d3</span><span class="p">.</span><span class="nx">select</span><span class="p">(</span><span class="k">this</span><span class="p">)</span>
</span><span class='line'>               <span class="p">.</span><span class="nx">selectAll</span><span class="p">(</span><span class="s2">&quot;.date&quot;</span><span class="p">)</span>
</span><span class='line'>               <span class="p">.</span><span class="nx">data</span><span class="p">(</span><span class="nx">tweetsByGroup</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">d</span><span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="nx">d</span><span class="p">.</span><span class="nx">key</span><span class="p">;</span> <span class="p">});</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// This appends a new `.date` and `.day` div for each item in the nested list</span>
</span><span class='line'>  <span class="c1">// the summary&#39;s start time as the title for the list.</span>
</span><span class='line'>  <span class="nx">date</span><span class="p">.</span><span class="nx">enter</span><span class="p">()</span>
</span><span class='line'>      <span class="p">.</span><span class="nx">append</span><span class="p">(</span><span class="s2">&quot;div&quot;</span><span class="p">)</span>
</span><span class='line'>      <span class="p">.</span><span class="nx">attr</span><span class="p">(</span><span class="s2">&quot;class&quot;</span><span class="p">,</span> <span class="s2">&quot;date&quot;</span><span class="p">)</span>
</span><span class='line'>      <span class="p">.</span><span class="nx">append</span><span class="p">(</span><span class="s2">&quot;div&quot;</span><span class="p">)</span>
</span><span class='line'>      <span class="p">.</span><span class="nx">attr</span><span class="p">(</span><span class="s2">&quot;class&quot;</span><span class="p">,</span> <span class="s2">&quot;day&quot;</span><span class="p">)</span>
</span><span class='line'>      <span class="p">.</span><span class="nx">text</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">d</span><span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="nx">formatDate</span><span class="p">(</span><span class="nx">d</span><span class="p">.</span><span class="nx">values</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">startTime</span><span class="p">);</span> <span class="p">});</span>
</span><span class='line'>
</span><span class='line'>  <span class="nx">date</span><span class="p">.</span><span class="nx">exit</span><span class="p">().</span><span class="nx">remove</span><span class="p">();</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// This binds each entry in the nested list, i.e. the list of summaries for</span>
</span><span class='line'>  <span class="c1">// each day, to the divs created above.  This way each div, titled with a</span>
</span><span class='line'>  <span class="c1">// particular day, has the array of summaries found on that day.</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">summary</span> <span class="o">=</span> <span class="nx">date</span><span class="p">.</span><span class="nx">order</span><span class="p">()</span>
</span><span class='line'>                    <span class="p">.</span><span class="nx">selectAll</span><span class="p">(</span><span class="s2">&quot;.summarylist&quot;</span><span class="p">)</span>
</span><span class='line'>                    <span class="p">.</span><span class="nx">data</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">d</span><span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="nx">d</span><span class="p">.</span><span class="nx">values</span><span class="p">;</span> <span class="p">},</span>
</span><span class='line'>                         <span class="kd">function</span><span class="p">(</span><span class="nx">d</span><span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="nx">d</span><span class="p">.</span><span class="nx">index</span><span class="p">;</span> <span class="p">});</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// This creates a new `.summarylist` div to surround the summaries.</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">summaryEnter</span> <span class="o">=</span> <span class="nx">summary</span><span class="p">.</span><span class="nx">enter</span><span class="p">()</span>
</span><span class='line'>                            <span class="p">.</span><span class="nx">append</span><span class="p">(</span><span class="s2">&quot;div&quot;</span><span class="p">)</span>
</span><span class='line'>                            <span class="p">.</span><span class="nx">attr</span><span class="p">(</span><span class="s2">&quot;class&quot;</span><span class="p">,</span> <span class="s2">&quot;summarylist&quot;</span><span class="p">);</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// This creates a new &#39;.summary&#39; div for each summary to place in the list.</span>
</span><span class='line'>  <span class="nx">summaryEnter</span><span class="p">.</span><span class="nx">append</span><span class="p">(</span><span class="s2">&quot;div&quot;</span><span class="p">)</span>
</span><span class='line'>              <span class="p">.</span><span class="nx">attr</span><span class="p">(</span><span class="s2">&quot;class&quot;</span><span class="p">,</span> <span class="s2">&quot;summary&quot;</span><span class="p">)</span>
</span><span class='line'>              <span class="p">.</span><span class="nx">text</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">d</span><span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="nx">d</span><span class="p">.</span><span class="nx">Summary</span><span class="p">;</span> <span class="p">});</span>
</span><span class='line'>
</span><span class='line'>  <span class="nx">summary</span><span class="p">.</span><span class="nx">exit</span><span class="p">().</span><span class="nx">remove</span><span class="p">();</span>
</span><span class='line'>
</span><span class='line'>  <span class="nx">summary</span><span class="p">.</span><span class="nx">order</span><span class="p">();</span>
</span><span class='line'><span class="p">});</span>
</span></code></pre></td></tr></table></div></figure>


<p>And that&#8217;s it!  Again, easy breazy.</p>

<h3>Setting up the divs for the stuff you want</h3>

<p>The last thing you need whenever you&#8217;re going to be mashing data into a
website via <a href="http://d3js.org/">D3</a> is some divs to host that data.  For my application, I need
just two types of divs: charts to hold the histograms and tables to hold the
summaries.  These look like:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='html'><span class='line'><span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">&quot;charts&quot;</span><span class="nt">&gt;</span>
</span><span class='line'>  <span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">&quot;hour-chart&quot;</span> <span class="na">class=</span><span class="s">&quot;chart&quot;</span><span class="nt">&gt;</span>
</span><span class='line'>    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;title&quot;</span><span class="nt">&gt;</span>Time of Day<span class="nt">&lt;/div&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/div&gt;</span>
</span><span class='line'>  <span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">&quot;cluster-chart&quot;</span> <span class="na">class=</span><span class="s">&quot;chart&quot;</span><span class="nt">&gt;</span>
</span><span class='line'>    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;title&quot;</span><span class="nt">&gt;</span>Cluster<span class="nt">&lt;/div&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/div&gt;</span>
</span><span class='line'>  <span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">&quot;date-chart&quot;</span> <span class="na">class=</span><span class="s">&quot;chart&quot;</span><span class="nt">&gt;</span>
</span><span class='line'>    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;title&quot;</span><span class="nt">&gt;</span>Date<span class="nt">&lt;/div&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/div&gt;</span>
</span><span class='line'><span class="nt">&lt;/div&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">&quot;lists&quot;</span><span class="nt">&gt;</span>
</span><span class='line'>  <span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">&quot;summary-list&quot;</span> <span class="na">class=</span><span class="s">&quot;list&quot;</span><span class="nt">&gt;&lt;/div&gt;</span>
</span><span class='line'><span class="nt">&lt;/div&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>They&#8217;re pretty dead simple.  One chart for each histogram I&#8217;m plotting and a
general div for the lists.  The lists will get populated with more divs
dynamically based on how many dates fall into a selected range.</p>

<h2>Creating the data to put in this app</h2>

<p>So how did I get all these tweets? And how did I split them up into different
clusters?  That&#8217;s <em>secret</em> for now, but if my current research project is
looking good, that&#8217;ll be come the topic of a new research paper, and if not,
i&#8217;ll be the topic of a blog post describing what failed!  So stay tuned!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ACL and EMNLP 2012 Debrief]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/07/13/acl-and-emnlp-2012-debrief/"/>
    <updated>2012-07-13T18:08:00+09:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/07/13/acl-and-emnlp-2012-debrief</id>
    <content type="html"><![CDATA[<h1>Attending two conference in Jeju, South Korea</h1>

<p>Today is the last day of two co-located top tier conferences for Natural
Language Processing and Computational Linguistics, ACL and EMNLP.  There&#8217;s been
six full days of presentations, posters, and many conversations over coffee
breaks, lunch, and dinner.  Being a lone researcher from two labs attending the conferences, i&#8217;ve done a lot of floating and mingling with different groups.  Here&#8217;s a debrief of some of the most interesting talks and posters I&#8217;ve seen, some good conversations i&#8217;ve had, and some thoughts on other parts of the conferences.</p>

<h2>Talks and Posters</h2>

<p>By a wide margin, the most intersting talks, in my opinion, were those that used
clean and well structured (non-parametric) bayesian models to analyze text or
speech.  Here&#8217;s a short run down of the ones I liked most.</p>

<h3><a href="http://aclweb.org/anthology-new/P/P12/P12-1005.pdf"> A nonparametric Bayesian Approach to Accoustic Model Discovery</a></h3>

<p>This paper tied together <a href="http://en.wikipedia.org/wiki/Hidden_Markov_model">Hidden Markov Models</a> and <a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">Dirichlet Process Mixture
Models</a> to automatically learn word phonemes in spoken data.  Everytime an
utterance is observed, guess at some word boundaries and then for each segment
in the word boundary, they select a HMM from a Dirichlet Process to model the
phoneme.  Their performance was pretty good considering it was fully
unsupervised and their model even learned finer distinctions between phonemes
based on common speaker patterns that weren&#8217;t recorded in the test labels.</p>

<h3><a href="http://aclweb.org/anthology-new/P/P12/P12-1009.pdf">SITS: A Hierarchical Non-parametric Model using Speaker Identity for Topic Segmentation in Multiparty Conversation</a></h3>

<p>This model extends LDA to utilize speaker identities to automatically determine
the topics each speaker prefers to use and the likelihood of a speaker changing
a topic during a conversation.  For example, in a Vice-Presidential debate, they
captured Sarah Palin&#8217;s tendancy to completely change the topic of the
conversation when responding to a question.</p>

<h3><a href="http://aclweb.org/anthology-new/P/P12/P12-1029.pdf">Word Sense Disambiguation Improves Information Retrieval</a></h3>

<p>WSD has often been linked to IR with mixed results.  This late test attempt
first performs WSD by learning word senses from aligned texts in Chinese and
English, where word senses are determined by the presence of multiple
translations of a word between the two languages and ways in which they can be
translated.  Using this information they were able to improve a standard IR
baseline on the TREC dataset.</p>

<h3><a href="http://aclweb.org/anthology-new/P/P12/P12-1037.pdf">Learning to &#8220;Read Between the Lines&#8217; using Bayesian Logic Programming</a></h3>

<p>This was my first exposure to Bayesian Logic Programming, which provide a nice
and more scalable alternative to Markov Logic Networks.  Using a pipeline
system, they do some information extraction and then use a BLP to infer logical
connections that can then be used to infer unsaid facts from observed
information.  For instance, If we know Barack Obama is President of the U.S.,
and we know that presidents, or leaders in general, are members of their group,
then the BLP should probabilisticly infer that Barack Obama is a member of the
U.S. even if it&#8217;s never said.</p>

<h3><a href="http://aclweb.org/anthology-new/P/P12/P12-1063.pdf">Computational Approaches to Sentence Completion</a></h3>

<p>This evaluated Language Model and Distributional Models using a SAT styled
sentence completion test.  They covered a lot of different approaches to this
problem, includeing two ways of using LSA, a Recurrent Neural network, and
standard N-Gram based Language Models.  The LSA models clearly did best which
suggests something should be done to enhance the other models.  I&#8217;d like to see
if the different models made different types of errors, for instance, did one
handle world knowledge based questions better, like &#8220;the bike law was _ by
congeress&#8221; where the blank should be &#8220;ratified&#8221;, and do others handle other
types of questions better.  If this is the case, then perhaps we can learn
these situations and build a model to decide when to use each approach on a
test question.</p>

<h3><a href="http://www.aclweb.org/anthology/P/P12/P12-1046.pdf">Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing</a></h3>

<p>This model used a really elegant and simple three level Pitman-Yor process to
combine two sophisticated parsing models together, Symbol-Refined trees and Tree
Substitution Grammars.  The first approach refines each node in syntax trees
with finer categories and then second considers subtrees within a full tree.  By
using the three level Pitman-Yor model, given a parsed example, the first level
process models the full tree, the second level models symbol refinements, and
the third models possible tree substituions.  Given the huge state space, they
showed a simple but effect blocked sampling algorithm.</p>

<h3><a href="http://aclweb.org/anthology-new/P/P12/P12-2017.pdf">Using Rejuvenation to Improve Particle Filtering for Bayesian Word Segmentation</a></h3>

<p>This model uses an unsupervised Sequential Markov Model to represent word
segments. They improved upon a prior online approach by including Rejuvenation
to the sequential model, which resamples a subset of prior examples after
modeling a new sample.  By doing this, they model can fix past mistakes made
when there was less knowledge about the dataset.  This is mostly cool because I
fancy Sequential Monte Carlo methods.</p>

<h3><a href="http://aclweb.org/anthology-new/P/P12/P12-1075.pdf">Unsupervised Relation Discovery with Sense Disambiguation</a></h3>

<p>This paper presented a really interested use of fully unsupervised Word Sense
Induction methods.  They used a very cute variant of LDA where documents are a
series of extracted dependency relation tuples using the same connecting
relation.  The &#8220;words&#8221; were then feature vectors modeling each tuples words and
the topics observed from the tuples source sentence and document.  The WSD model
showed good improvement over relation clustering, but has some limitations.  The
topics learned by LDA provide an initial clustering over the relations, then a
second stage of HAC groups related topics.  Would a fully non-parametric model
create a more accurate number of clusters automatically by only generated topics
when they are truely separate?  Could this also be used to test different WSI
approaches and give an indiciation of where they are lacking or what features
are missing?</p>

<h3><a href="http://aclweb.org/anthology-new/D/D12/D12-1128.pdf">Joining Forces Payes Off: Multilingual Joint Word Sense Disambiguation</a></h3>

<p>This Model utilizes BabelNet with a simple ensemble model to really boost
knowledge based WSD.  For a given word in a context, they generate the possibile
translations in multiple languages and then for every sense they compute a graph
centrality score using BableNet using the candidate translations.  Each
translation serves as an individual WSD model which are combined with a simple
probablistic mixture model.  Given BabelNet, this looks really promising.  It
might be good to try using this over NYT with my Topic Model evaluations and use
it as a comparison between Knowledge Based WSD and fully unsupervised WSI.</p>

<h3>The Appification of the Web and the Renaissance of Conversational User Interfaces</h3>

<p><a href="http://www.patrickpantel.com/">Patrick Pantel</a> gave this amazing plenary talk and covered the coming issues
and opportunities presented by the growing trend of users accessing the internet
via siloed apps on mobile devices and the desire of users to perform tasks using
a conversational approach.  He presented a lot of really good issues that need
consideration and solving.  He also went into dept about a graphical model
connecting user queries to user intentions and possible actions that can address
those desires.  I&#8217;m really fascinated by what can be done with a given model
once it&#8217;s been built.  For instance, if you have this kind of model and the
user, which has some model of their general intentions or desired actions, then
asks &#8220;what can i do that is interesting&#8221;, can you then project the dataset
backwards into a view of possible actions and intentions that the user can then
navigate through to explore possible things to do.  Concretely, if the model
knows I like certain types of food at certain prices, can it take that knowledge
and then organize surrounding restaurants in terms of what i like so i can pick
and choose these things easily, especially when I don&#8217;t really know what I want
initially?</p>

<h3><a href="http://aclweb.org/anthology-new/D/D12/D12-1080.pdf">Learning Constraints for Consistent Timeline Extraction</a></h3>

<p>The motivating example behind this paper was the challenge of extracting
semantically consistent facts regarding a single entity.  For example, given
many mentions about James T. Kirk and many events with him as a participant, can
you extract the logically consistent facts, such as that he was born after his
father and was a space cadet before being a captain or instructor?  They do this
with a joint supervised system that extracts facts and validates their
consistency.</p>

<h3><a href="http://aclweb.org/anthology-new/D/D12/D12-1101.pdf">Monte Carlo MCMC: Efficient Inference by Approximate Sampling</a></h3>

<p>This paper was really exciting and elegant.  When doing a Metropolis Hastings
update in a graphical model, it requires computing the acceptance rate in terms
of factors that change based on the update.  However some of these factors can
be expensive to compute, and there may be many many factors.  To improve
efficiency, they simply use a subset of the factors in a probability
factorization to compute the acceptance rate, using two possible methods for
selecting that subset.  First, they consider a random subset of a uniform size,
and alternatively, they sample more and more samples until the samples have an
estimated mean with 95% confidence, allowing different proposals to use
different sized subsets of factors and giving the inference more flexibility.
In short, really simple, really smart, and really effective.</p>

<h3><a href="http://aclweb.org/anthology-new/D/D12/D12-1065.pdf">Discovering Diverse and Salient Threads in Document Collections</a></h3>

<p>News corpora cover many related artciles about a string of events.  This
approach uses a simple graph analysis algorithm to extract chains of articles
that cover a single topic spread accross many documents.  The main issues seem
to be with scale and how to select how many and which threads to extract.  For
instance, can you first impose a hierarchy of topics over the dataset and then
select threads within that decomposition?  Is there some number of diverse
threads that sufficiently describe the dataset?  And how would you learn a lot
of different threads?</p>

<h3><a href="http://aclweb.org/anthology-new/D/D12/D12-1117.pdf">Constructing Task-Specific Taxonomies for Document Collection Browsing</a></h3>

<p>This sounds really cool, but i didn&#8217;t have a chance to see the presentation and haven&#8217;t read it yet.</p>

<h3><a href="http://aclweb.org/anthology-new/D/D12/D12-1110.pdf">Semantic Compositionality through Recursive Matrix-Vector Spaces</a></h3>

<p><a href="http://ai.stanford.edu/~ang/">Andrew Ng</a> currently seems to have a man crush on neural networks.  He and a
student show a neat improvement upon Recursive Neural Networks, which can do compositionality between phrases that have non-linear properties.  Take the phrase &#8220;absolutely sad&#8221;, it does some funny things composition wise, and existing RNN models can&#8217;t handle it.  They go a step beyond by projecting each component phrase&#8217;s vector representation by a matrix modeling the other word&#8217;s uses.  So in the example, sad modifies absolutely and visa versa, then the joint vector for both modified words gets pushed through a standard RNN.  It worked out pretty well, but requires training.  I&#8217;d love to see something here that doesn&#8217;t need training.</p>

<h2>Conversations with other people</h2>

<p>My talk about <a href="http://fozziethebeat.github.com/blog/2012/06/04/topic-model-comparisons-how-to-replicate-an-experiment/">Exploring Topic Coherence over many models</a> got some
interesting responses that were really interesting.</p>

<ol>
<li>First, we didn&#8217;t really vary the feature weighting mechanism before doing LSA
and PLSA, so this may be affecting the performance of those models with
information that LDA doesn&#8217;t have, if we try other schemes, or no scheme,
what happens?  does that mean we could improve LDA if we have a way of
encoding that information?</li>
<li>How doe the models compare when using corpora of different sizes?  We could
use TASA as an example of a smaller corpus, or UKWac and Wikipedia as larger
corpora.  Does LDA get better representations with less information or does
SVD?  Can we then improve either method with those lessons.</li>
<li>What about word senses?  Can we pos tag and word sense tag the corpus and
then see improvement along multiple lines?  When doing the coherence
evaluation, we would probably just have to use the raw tokens initially, but
for the word similarity and document evaluations, we can use the sense tagged
words.  Issues with this: How do we do sense tagging? SemCore is
reasonably sized but WSD models kinda suck.  Are they sufficient to
improve results?  What about WSI models?  If WSI models do something
positive, this would be a good evaluation of those models without
actually knowing what kind of senses or how many we need to learn.</li>
</ol>


<h2>The location</h2>

<p>I wasn&#8217;t such a huge fan of the conferences being located on Jeju island.  There
were things to do on the island, but they were all expensive, as was the food,
in my opinion.  Furthermore, the banquet was mildly terrible.  We had it in this
tiny restaurant next to the ocean, which had a nice view granted, but was too
small for the attendees and it was a buffet, so a hundred computational
linguists, which all heard about <a href="http://en.wikipedia.org/wiki/Chinese_restaurant_process">Chinese Restaurant Processes</a> got to wait in
line to sample tasty Korean food.  As awesomely ironic as it was, it made
getting food a terrible experience.  I&#8217;d have prefered the be in some major city
or college town where things are cheap (note how the 2010 meeting of ACL was
<strong>amazing</strong>).  Jeju is nice and all, especially if i had the time and money to
explore the island more, but I had neither really, and I think nearly all
students felt the same way.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Gamma Ramma]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/06/25/gamma-ramma/"/>
    <updated>2012-06-25T10:23:00+09:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/06/25/gamma-ramma</id>
    <content type="html"><![CDATA[<h3>Before getting to Cooler Mixture Models</h3>

<p>Before going into the depths that is ways to improve the simple Multinomial
Mixture Model that I discussed in the <a href="blog/2012/05/17/stepping-into-the-mixtures-with-gibbs-sampling">last post</a>, I want to give a little
adventure story regarding the <a href="http://en.wikipedia.org/wiki/Gamma_distribution">Gamma</a> distribution.  Why the <a href="http://en.wikipedia.org/wiki/Gamma_distribution">Gamma</a>
distribution?  Mainly because it is deeply tied to <a href="http://en.wikipedia.org/wiki/Normal_distribution">Gaussian</a> distributions
and those will factor heavily into cool things you can do in mixture models.</p>

<h3>This thing called my Conjugate Prior</h3>

<p>So how is the <a href="http://en.wikipedia.org/wiki/Gamma_distribution">Gamma</a> distribution related to a <a href="http://en.wikipedia.org/wiki/Normal_distribution">Gaussian</a> distribution?
The short story is that a properly parameterized <a href="http://en.wikipedia.org/wiki/Gamma_distribution">Gamma</a> distribution can
approximate the precision (or inverse variance) for a <a href="http://en.wikipedia.org/wiki/Normal_distribution">Gaussian</a> distribution
(the proper wording for this relationship is that a <a href="http://en.wikipedia.org/wiki/Gamma_distribution">Gamma</a> distribution is a
<a href="http://en.wikipedia.org/wiki/Conjugate_prior">conjugate prior</a> for a <a href="http://en.wikipedia.org/wiki/Normal_distribution">Gaussian</a> distribution when you know the mean but
don&#8217;t know the precision).  This turns out to be super powerful when you have
some idea of what the mean of your <a href="http://en.wikipedia.org/wiki/Normal_distribution">Gaussian</a> is but need to estimate the
variance.  <strong>However</strong>, this distributions has some flavors that can really
wreck havoc on your ability to learn these values if you use them in the wrong
place, and similarly people are <em>really</em> terrible at pointing out which flavour
you can and should use.  So here&#8217;s an adventure on taste testing the <a href="http://en.wikipedia.org/wiki/Gamma_distribution">Gamma</a>
distribution.</p>

<h3>Sampling from Gamma Distributions</h3>

<p>Let&#8217;s start with an example.  Let&#8217;s say we have two one dimensional Gaussian
distributions.  Each distribution is defined by two parameters: <script type="math/tex"> \mu
</script> for the mean, or center point, and <script type="math/tex"> \sigma<sup>2</sup> </script> for the variance,
or inverse precision.  So let&#8217;s say our two distributions have these two
parametrizations: <script type="math/tex"> \mu_1 = 5.0, \sigma<sup>2_1</sup> = 1.0</script> and <script type="math/tex"> \mu_2 = 10.0,\sigma<sup>2_2</sup> = 6.0</script>.  Here&#8217;s a nice density
plot of these two distributions together:</p>

<p><img src="http://fozziethebeat.github.com/images/two_univariate_gaussians.png" width="500" height="500"></p>

<p>Now let&#8217;s look at what the <a href="http://en.wikipedia.org/wiki/Gamma_distribution">Gamma</a> distribution can do if we sample from it
with the &#8220;proper&#8221; parameters (which I&#8217;ll point out later on).</p>

<p><img src="http://fozziethebeat.github.com/images/gamma_breeze_samples.png" width="500" height="500"></p>

<p>It looks like a good approximation of what we know the two variance values to be
and the samples can be generated really easily with Scala&#8217;s <a href="https://github.com/dlwh/breeze">breeze</a> library
which you&#8217;ll see in the next section.  But let&#8217;s for funsies sake say that we
don&#8217;t like using <a href="https://github.com/dlwh/breeze">breeze</a> (say because we hate Scala and prefer to trust
libraries written in <a href="http://java.sun.com/">Java</a>), we may then want to use some other
implementation of this distribution, like say in <a href="http://commons.apache.org/math/">Apache Commons Math</a> or
Java&#8217;s <a href="http://acs.lbl.gov/software/colt/">Colt</a> library.  What happens if we take the parameters we passed into
<a href="https://github.com/dlwh/breeze">breeze</a> and passed them to these libraries?  What comes out?  Let&#8217;s See!</p>

<p><img src="http://fozziethebeat.github.com/images/gamma_comparison_samples.png" width="600" height="600"></p>

<p>That looks kinda funky.  What&#8217;s going on?  This exposes the difference between
the two main flavors of this distribution: two related but poorly explained ways
to parameterize the model!  Both flavours depend on a shape parameter
that (aptly named) guides the shape of the distribution.  The difference in the
flavours is the use of either a scale parameter, sometimes referred to as <script type="math/tex">
\theta </script>, or a rate parameter, sometimes denoted as <script type="math/tex">\beta</script>.  Is
there a relation between these two?  Totally! They are inverses of each other,
i.e. <script type="math/tex"> \theta = 1/\beta </script>.</p>

<h3>Finding the funky culprit</h3>

<p>Now how do we know which of the above three implementations is doing the right
thing?  One way is to use the <a href="http://en.wikipedia.org/wiki/Inverse-gamma_distribution">Inverse Gamma</a> distribution.  Samples from an
<a href="http://en.wikipedia.org/wiki/Inverse-gamma_distribution">Inverse Gamma</a> distribution should (roughly) be the reciprocal of samples
from a <a href="http://en.wikipedia.org/wiki/Gamma_distribution">Gamma</a> distribution with the same parameters.  But these packages
don&#8217;t really have an implementation of an <a href="http://en.wikipedia.org/wiki/Inverse-gamma_distribution">Inverse Gamma</a> distribution, so
what else can we do to check?  Well, Colt looks to be doing two really <strong>wierd</strong>
things that violate our intuitions about variances: (a) one set of variance
estimates are completely missing and (b) the one that does exist gives negative
variance values, and we know this to be impossible since the variance is defined
to be an average of real values raised to the power of two.  And if we read the
<a href="http://acs.lbl.gov/software/colt/">Colt</a> <a href="http://acs.lbl.gov/software/colt/api/index.html">javadoc</a> more carefully, we can figure out that they use the rate
and not the scale, but it&#8217;s never totally <strong>obvious</strong> in their documentation.
So if we fix that in our parameterization, we get this agreeable set of plots:</p>

<p><img src="http://fozziethebeat.github.com/images/gamma_comparison_fixed_samples.png" width="600" height="600"></p>

<p>That&#8217;s <strong>much</strong> better looking.</p>

<h3>Finding the magic parameters</h3>

<p>Now let&#8217;s dive deeper and see how I made all these (now fixed) samples:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">import</span> <span class="nn">breeze.stats.distributions.Gamma</span>
</span><span class='line'><span class="k">import</span> <span class="nn">org.apache.commons.math3.distribution.GammaDistribution</span>
</span><span class='line'><span class="k">import</span> <span class="nn">cern.jet.random.</span><span class="o">{</span><span class="nc">Gamma</span><span class="o">=&gt;</span><span class="nc">CGamma</span><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="k">val</span> <span class="n">data</span> <span class="k">=</span> <span class="o">...</span>
</span><span class='line'><span class="k">val</span> <span class="n">alpha</span> <span class="k">=</span> <span class="mi">1</span><span class="n">d</span> <span class="o">+</span> <span class="mi">5000</span>
</span><span class='line'><span class="k">val</span> <span class="n">beta_1</span> <span class="k">=</span> <span class="mi">1</span><span class="n">d</span> <span class="o">+</span> <span class="n">data</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">_-</span><span class="mf">5.0</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">pow</span><span class="o">(</span><span class="n">_</span><span class="o">,</span><span class="mi">2</span><span class="o">)).</span><span class="n">sum</span><span class="o">/</span><span class="mi">2</span><span class="n">d</span>
</span><span class='line'><span class="k">val</span> <span class="n">beta_2</span> <span class="k">=</span> <span class="mi">1</span><span class="n">d</span> <span class="o">+</span> <span class="n">data</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">_-</span><span class="mf">10.0</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">pow</span><span class="o">(</span><span class="n">_</span><span class="o">,</span><span class="mi">2</span><span class="o">)).</span><span class="n">sum</span><span class="o">/</span><span class="mi">2</span><span class="n">d</span>
</span><span class='line'><span class="k">val</span> <span class="n">pw</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">PrintWriter</span><span class="o">(</span><span class="n">args</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
</span><span class='line'><span class="k">val</span> <span class="n">numsamples</span> <span class="k">=</span> <span class="n">args</span><span class="o">(</span><span class="mi">2</span><span class="o">).</span><span class="n">toInt</span>
</span><span class='line'><span class="k">for</span> <span class="o">(</span> <span class="o">((</span><span class="n">a</span><span class="o">,</span><span class="n">b</span><span class="o">),</span> <span class="n">i</span><span class="o">)</span> <span class="k">&lt;-</span> <span class="nc">List</span><span class="o">((</span><span class="n">α</span><span class="o">,</span> <span class="mi">1</span><span class="o">/</span><span class="n">β_1</span><span class="o">),</span> <span class="o">(</span><span class="n">α</span><span class="o">,</span> <span class="mi">1</span><span class="o">/</span><span class="n">β_2</span><span class="o">)).</span><span class="n">zipWithIndex</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="c1">// Create a Gamma distribution using breeze </span>
</span><span class='line'>    <span class="k">val</span> <span class="n">gdist</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Gamma</span><span class="o">(</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span>
</span><span class='line'>    <span class="c1">// Create a Gamma distribution using Apache Commons Math</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">acgDist</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">GammaDistribution</span><span class="o">(</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">for</span> <span class="o">(</span><span class="n">x</span> <span class="k">&lt;-</span> <span class="mi">0</span> <span class="n">until</span> <span class="n">numsamples</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">pw</span><span class="o">.</span><span class="n">println</span><span class="o">(</span><span class="s">&quot;breeze %d %f&quot;</span><span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="o">(</span><span class="mi">1</span><span class="o">/</span><span class="n">gdist</span><span class="o">.</span><span class="n">sample</span><span class="o">)/</span><span class="mi">10</span><span class="n">d</span><span class="o">))</span>
</span><span class='line'>        <span class="n">pw</span><span class="o">.</span><span class="n">println</span><span class="o">(</span><span class="s">&quot;commons-math %d %f&quot;</span><span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="o">(</span><span class="mi">1</span><span class="o">/</span><span class="n">acgDist</span><span class="o">.</span><span class="n">sample</span><span class="o">)/</span><span class="mi">10</span><span class="n">d</span><span class="o">))</span>
</span><span class='line'>        <span class="c1">// This is the easiet way to sample from Colt&#39;s gamma distribution.  Note that </span>
</span><span class='line'>        <span class="c1">// i&#39;m now turning the scale parameter back into the rate parameter.</span>
</span><span class='line'>        <span class="n">pw</span><span class="o">.</span><span class="n">println</span><span class="o">(</span><span class="s">&quot;colt %d %f&quot;</span><span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="o">(</span><span class="mi">1</span><span class="o">/</span><span class="nc">CGamma</span><span class="o">.</span><span class="n">staticNextDouble</span><span class="o">(</span><span class="n">a</span><span class="o">,</span> <span class="mi">1</span><span class="o">/</span><span class="n">b</span><span class="o">))/</span><span class="mi">10</span><span class="n">d</span><span class="o">))</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'><span class="n">pw</span><span class="o">.</span><span class="n">close</span>
</span></code></pre></td></tr></table></div></figure>


<p>In this snippet, I&#8217;ve left out <code>data</code>, but it&#8217;s simply a map for accessing the list of
samples I drew earlier to make the Gaussian plot.  The key equations here are
for computing what I&#8217;m calling <code>alpha</code> and <code>beta_*</code>.  Looking at a handy table
of <a href="http://en.wikipedia.org/wiki/Conjugate_prior#Continuous_likelihood_distributions">conjugate priors for continuous likelihood distributions</a>, we find these
two core equations used in the code:</p>

<script type="math/tex; mode=display">
\begin{array}{lll}
\alpha & = & \alpha_{0} + \frac{n}{2} \\
\beta & = & \beta_{0} + \frac{\sum_i (x_i - mu)^2 }{2} \\
\end{array}
</script>


<p>Where <script type="math/tex">n</script> is the number of points in the group and <script type="math/tex">x_i</script>
denotes points within coming from the same distribution.  Looking at that table
of conjugate priors and the list created in the first for loop, you might wonder
why i use the reciprocal of the <script type="math/tex">\beta</script> values, i.e. the scales.  This
is because of the cute footnote in the conjugate prior table noting that</p>

<script type="math/tex">\beta</script>


<p> as computed above is in fact the rate.</p>

<h3>An even more terrifying parametrization</h3>

<p>Now if you thought what I&#8217;ve touched on is cool, a little funky, and possible
frustrating in the minor differences, it&#8217;s time to really complicate things.  A
really smart man by the name of Carl von Rasmussen designed an <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.9111">Infinite Gaussian Mixture Model</a>
that depends heavily on the <a href="http://en.wikipedia.org/wiki/Gamma_distribution">Gamma</a> distribution for just the purpose I&#8217;ve
described.  <em><strong>BUT</strong></em> he gives <strong>this</strong> parameterization:</p>

<script type="math/tex; mode=display">
\begin{array}{lll}
\alpha & = & \alpha_{0} + n \\
\beta & = & \frac{\alpha{0}}{\beta_0*\alpha_0 + \sum (x_i - \mu)^2}
\end{array}
</script>


<p>Throw this into our sampling code and we get this <strong>rediculous</strong> plot:</p>

<p><img src="http://fozziethebeat.github.com/images/gamma_comparison_samples_ras.png" width="600" height="600"></p>

<p>Which is <em>totally</em> wrong.  So what gives?  <em>Well</em>, at a much later date, Mr.
Rasmussen points out that as he defines them, the two parameters are slightly
mutated versions of the shape and scale as we&#8217;ve described them so far.  The
real way to use his parameters is to mutate them by doing this:</p>

<script type="math/tex; mode=display">
\begin{array}{lll}
\alpha_{legit} & = & \frac{\alpha_{rasmussen}}{2} \\
\beta_{legit} & = & \frac{2*\beta_{rasmussen}}{\alpha_{rasmussen}} \\
\end{array}
</script>


<h3>Summary!</h3>

<p>So to summarize this adventure time blast, I have one key lesson: figure out
which parameterization your software is using and which parameterization a
model designer is using and make sure they match.  Otherwise you&#8217;ll spend a
terrible amount of time wondering why you <strong>just</strong> <em><strong>can&#8217;t</strong></em> estimate those
variances accurately.  I failed to do this a few weeks ago and was quite
befuddled for a while, so don&#8217;t be like me.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Topic Model Comparisons: how to replicate an experiment]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/06/04/topic-model-comparisons-how-to-replicate-an-experiment/"/>
    <updated>2012-06-04T14:35:00+09:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/06/04/topic-model-comparisons-how-to-replicate-an-experiment</id>
    <content type="html"><![CDATA[<p>We (Keith Stevens, Philip Kegelmeyer, David Andrzejewski, and David Buttler)
published the paper <a href="https://github.com/fozziethebeat/TopicModelComparison">Exploring Topic Coherence over many models and many topics</a>
(link to appear soon) which compares several topic models using a variety of
measures in an attempt to determine which model should be used in which
application.  This evaluation secondly compares automatic coherence measures as
a quick, task free method for comparing a variety of models.  Below is a
detailed series of steps on how to replicate the results from the paper.  Note
that these instructions are cross posted as part of <a href="https://github.com/fozziethebeat/TopicModelComparison">this</a> <a href="https://github.com/">GitHub</a> project.</p>

<p>The evaluation setup breaks down into the following steps:</p>

<ol>
<li>Select a corpus and pre-process.</li>
<li>Remove stop words, infrequent words, and format the corpus.</li>
<li>Perform topic modelling on all documents</li>
<li>Compute topic coherence measures for induced topics</li>
<li>Compute word similarities using semantic pairing tests</li>
<li>Compute Classifier accuracy using induced topics</li>
</ol>


<p>Each of these steps are automated in the bash scripts provided in this
repository.  To run those scripts read the last section for downloading the
needed components, setting parameters, and then watching the scripts blaze
through the setup.</p>

<p>The rest of this writeup explains each step in more detail than was permitted in
the published paper.</p>

<h2>Selecting the corpus</h2>

<p>The evaluation requires the use of a semantically labeled corpus that has a
relatively cohesive focus.  The original paper used all articles from 2003 of
the <a href="http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2008T19">New York Times Annotated Corpus</a> provided by the <a href="http://www.ldc.upenn.edu/">Linguistics Data Consortium</a>.<br/>
Any similarly structured corpus should work.</p>

<p>The New York Times corpus requires some pre-processing before it can be easily
used in the evaluation.  The original corpus comes in a series of tarballed xml
files where each file looks something like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;nitf</span> <span class="na">change.date=</span><span class="s">&quot;month day, year&quot;</span> <span class="na">change.time=</span><span class="s">&quot;HH:MM&quot;</span> <span class="na">version=</span><span class="s">&quot;-//IPTC//DTD NITF 3.3//EN&quot;</span><span class="nt">&gt;</span>
</span><span class='line'><span class="nt">&lt;head&gt;</span>
</span><span class='line'>  <span class="nt">&lt;title&gt;</span>Article Title<span class="nt">&lt;/title&gt;</span>
</span><span class='line'>  <span class="nt">&lt;meta</span> <span class="na">content=</span><span class="s">&quot;Section Name&quot;</span> <span class="na">name=</span><span class="s">&quot;online_sections&quot;</span><span class="nt">/&gt;</span>
</span><span class='line'><span class="nt">&lt;/head&gt;</span>
</span><span class='line'><span class="nt">&lt;body&gt;</span>
</span><span class='line'>  <span class="nt">&lt;body.contents&gt;</span>
</span><span class='line'>    <span class="nt">&lt;block</span> <span class="na">class=</span><span class="s">&quot;full_text&quot;</span><span class="nt">&gt;</span>
</span><span class='line'>      <span class="nt">&lt;p&gt;</span>Article text<span class="nt">&lt;/p&gt;</span>
</span><span class='line'>    <span class="nt">&lt;/block&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/body.contents&gt;</span>
</span><span class='line'><span class="nt">&lt;/body&gt;</span>
</span><span class='line'><span class="nt">&lt;/nitf&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>This leaves out a lot of details, but it covers the key items we will need: (1)
the full text of the article and (2) all online_sections for the article.
Extracting this can be kinda hairy.  The following snippet gives a gist of how
to extract and format the necessary data:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">import</span> <span class="nn">scala.xml.XML</span>
</span><span class='line'>
</span><span class='line'><span class="k">val</span> <span class="n">doc</span> <span class="k">=</span> <span class="nc">XML</span><span class="o">.</span><span class="n">loadFile</span><span class="o">(</span><span class="n">file</span><span class="o">)</span>
</span><span class='line'><span class="k">val</span> <span class="n">sections</span> <span class="k">=</span> <span class="o">(</span><span class="n">doc</span> <span class="o">\\</span> <span class="s">&quot;meta&quot;</span><span class="o">).</span><span class="n">filter</span><span class="o">(</span><span class="n">node</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">node</span> <span class="o">\</span> <span class="s">&quot;@name&quot;</span><span class="o">).</span><span class="n">text</span> <span class="o">==</span> <span class="s">&quot;online_sections&quot;</span><span class="o">)</span>
</span><span class='line'>                              <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">node</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">node</span> <span class="o">\</span> <span class="s">&quot;@content&quot;</span><span class="o">).</span><span class="n">text</span><span class="o">)</span>
</span><span class='line'>                              <span class="o">.</span><span class="n">mkString</span><span class="o">(</span><span class="s">&quot;;&quot;</span><span class="o">)</span>
</span><span class='line'><span class="k">val</span> <span class="n">text</span> <span class="k">=</span> <span class="o">(</span><span class="n">doc</span> <span class="o">\\</span> <span class="s">&quot;block&quot;</span><span class="o">).</span><span class="n">filter</span><span class="o">(</span><span class="n">node</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">node</span> <span class="o">\</span> <span class="s">&quot;@class&quot;</span><span class="o">).</span><span class="n">text</span> <span class="o">==</span> <span class="s">&quot;full_text&quot;</span><span class="o">)</span>
</span><span class='line'>                           <span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="n">node</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">node</span> <span class="o">\</span> <span class="s">&quot;p&quot;</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">_</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="o">(</span><span class="s">&quot;\n&quot;</span><span class="o">,</span> <span class="s">&quot; &quot;</span><span class="o">).</span><span class="n">trim</span><span class="o">))</span>
</span><span class='line'>                           <span class="o">.</span><span class="n">mkString</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>Before printing the data, we also need to tokenize everything.  We used the Open
NLP MaxEnt tokenizers.  First download the english MaxEnt tokenizer model
<a href="http://opennlp.sourceforge.net/models-1.5/en-token.bin">here</a> then do the following before processing each document</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">tokenizerModel</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">TokenizerModel</span><span class="o">(</span><span class="k">new</span> <span class="nc">FileInputStream</span><span class="o">(</span><span class="n">modelFileName</span><span class="o">))</span>
</span><span class='line'><span class="k">val</span> <span class="n">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">TokenizerME</span><span class="o">(</span><span class="n">tokenizerModel</span><span class="o">)</span>
</span><span class='line'><span class="k">val</span> <span class="n">stopWords</span> <span class="k">=</span> <span class="nc">Source</span><span class="o">.</span><span class="n">fromFile</span><span class="o">(</span><span class="n">args</span><span class="o">(</span><span class="mi">1</span><span class="o">)).</span><span class="n">getLines</span><span class="o">.</span><span class="n">toSet</span>
</span><span class='line'><span class="k">def</span> <span class="n">acceptToken</span><span class="o">(</span><span class="n">token</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=</span> <span class="o">!</span><span class="n">stopWords</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="n">token</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>And then do the following to each piece of <code>text</code> extracted:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">tokenizedText</span> <span class="k">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">toLowerCase</span><span class="o">.</span><span class="n">tokenize</span><span class="o">(</span><span class="n">text</span><span class="o">).</span><span class="n">filter</span><span class="o">(</span><span class="n">acceptToken</span><span class="o">).</span><span class="n">mkString</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">)</span>
</span><span class='line'><span class="n">printf</span><span class="o">(</span><span class="s">&quot;%s\t%s\n&quot;</span><span class="o">,</span> <span class="n">sections</span><span class="o">,</span> <span class="n">tokenizedText</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>This should generate one line per document in the format</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">section_1</span><span class="o">(;</span><span class="n">section_n</span><span class="o">)+&lt;</span><span class="nc">TAB</span><span class="o">&gt;</span><span class="n">doc_text</span>
</span></code></pre></td></tr></table></div></figure>


<p>With properly tokenized text and a series of stop words removed..</p>

<h2>Filtering tokens</h2>

<p>In order to limit the memory requirements of our processing steps, we discard
any word that is not in the list of word similarity pairs or the top 100k most
frequent tokens in the corpus.  The following bash lines will accomplish this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">cat</span> <span class="nc">$oneDocFile</span> <span class="o">|</span> <span class="n">cut</span> <span class="o">-</span><span class="n">f</span> <span class="mi">2</span> <span class="o">|</span> <span class="n">tr</span> <span class="s">&quot; &quot;</span> <span class="s">&quot;\n&quot;</span> <span class="o">|</span> <span class="n">sort</span> <span class="o">|</span> <span class="n">uniq</span> <span class="o">-</span><span class="n">c</span> <span class="o">|</span> <span class="o">\</span>
</span><span class='line'>                  <span class="n">sort</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">k</span> <span class="mi">1</span> <span class="o">-</span><span class="n">r</span> <span class="o">|</span> <span class="n">head</span> <span class="o">-</span><span class="n">n</span> <span class="mi">100000</span> <span class="o">|</span> <span class="o">\</span>
</span><span class='line'>                  <span class="n">awk</span> <span class="err">&#39;</span><span class="o">{</span> <span class="n">print</span> <span class="n">$2</span> <span class="o">}</span><span class="err">&#39;</span> <span class="o">&gt;</span> <span class="nc">$topTokens</span>
</span><span class='line'><span class="n">cat</span> <span class="n">wordsim</span><span class="o">*.</span><span class="n">terms</span><span class="o">.</span><span class="n">txt</span> <span class="nc">$topTokens</span> <span class="o">|</span> <span class="n">uniq</span> <span class="o">&gt;</span> <span class="o">.</span><span class="n">temp</span><span class="o">.</span><span class="n">txt</span>
</span><span class='line'><span class="n">mv</span> <span class="o">.</span><span class="n">temp</span><span class="o">.</span><span class="n">txt</span> <span class="nc">$topTokens</span>
</span></code></pre></td></tr></table></div></figure>


<p>Once we&#8217;ve gotten the top tokens that&#8217;ll be used during processing, we do one
more filtering of the corpus to reduce each document down to only the accepted
words and discard any documents that contain no useful content words.  Running
<a href="http://opennlp.sourceforge.net/models-1.5/en-token.bin">FilterCorpus</a> with the top tokens file and the corpus file will return a
properly filtered corpus.</p>

<h2>Topic Modeling</h2>

<p>With all the pre-processing completed, we can now generate topics for the
corpus.  We do this using two different methods (1) Latent Dirichlet Allocation
and (2) Latent Semantic Analysis.  Unless otherwise stated, we we performed
topic modeling using each method for 1 to 100 topics, and for 110 to 500 topics
with steps of 10.</p>

<h3>Processing for Latent Dirichlet Allocation</h3>

<p>We use <a href="http://mallet.cs.umass.edu/">Mallet&#8217;s</a> fast parallel implementaiton of Latent Dirichlet Allocation
to do the topic modelling.  Since <a href="http://mallet.cs.umass.edu/">Mallet&#8217;s</a> interface does not let us easily
limit the set of tokens or set the indices we want each token to have, we
provide a class to do this: <a href="http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2008T19">TopicModelNewYorkTimes</a>.  This takes five
arguments</p>

<ol>
<li>The set of content words to represent</li>
<li>Number of top words to report for each topic</li>
<li>The documents to represent</li>
<li>The number of topics</li>
<li>A name for the output data.</li>
</ol>


<p>And we run this with the following command.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">scala</span> <span class="n">edu</span><span class="o">.</span><span class="n">ucla</span><span class="o">.</span><span class="n">sspace</span><span class="o">.</span><span class="nc">TopicModelNewYorkTimes</span> <span class="nc">$topTokens</span> <span class="mi">10</span> <span class="nc">$oneDocFile</span> <span class="nc">$nTopics</span> <span class="n">nyt03_LDA_$nTopics</span>
</span></code></pre></td></tr></table></div></figure>


<p>for the specified range of topics.  The command will then perform LDA and store
the term by topic matrix in <code>nyt03_LDA_$nTopics-ws.dat</code>, the document by
topic matrix in <code>nyt03_LDA_$nTopics-ds.dat</code>, and the top 10 words for each
topic in <code>nyt03_LDA_$nTopics.top10</code>.</p>

<h3>Processing for Latent Semantic Analysis</h3>

<p>Latent Semantic Analysis at it&#8217;s core decomposes a term by document matrix into
two smaller latent matrices using one of two methods: (1) <a href="http://en.wikipedia.org/wiki/Singular_value_decomposition">Singular Value Decomposition</a>
and (2) <a href="http://en.wikipedia.org/wiki/Non-negative_matrix_factorization">Non-negative Matrix Factorization</a>.  We do this in two steps:</p>

<ol>
<li>Build a weighted term document matrix.</li>
<li>Factorize the matrix using either SVD or NMF.</li>
</ol>


<p>We use the <a href="https://github.com/fozziethebeat/TopicModelComparison/blob/master/src/main/scala/edu/ucla/sspace/BuildTermDocMatrix.scala">BuildTermDocMatrix</a> class to perform the first step.  It takes
four arguments:</p>

<ol>
<li>A list of words to represent</li>
<li>A feature transformation method, valid options are tfidf, logentropy, and
none</li>
<li>The corpus to represent</li>
<li>An output filename</li>
</ol>


<p>We run this once on our properly formatted corpus using the top set of tokens
using this command</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">scala</span> <span class="n">edu</span><span class="o">.</span><span class="n">ucla</span><span class="o">.</span><span class="n">sspace</span><span class="o">.</span><span class="nc">BuildTermDocMatrix</span> <span class="nc">$topTokens</span> <span class="n">logentropy</span> <span class="nc">$oneDocFile</span> <span class="nc">$oneDocFile</span><span class="o">.</span><span class="n">mat</span>
</span></code></pre></td></tr></table></div></figure>


<p>With the term document matrix, we then decompose it using the
<a href="https://github.com/fozziethebeat/TopicModelComparison/blob/master/src/main/scala/edu/ucla/sspace/MatrixFactorNewYorkTimes.scala">MatrixFactorNewYorkTimes</a> method, which uses either SVD or NMF to decompose
the matrix and stores a term by latent factor matrix and a document by latent
factor matrix to disk.  A sample run of this looks like:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">scala</span> <span class="n">edu</span><span class="o">.</span><span class="n">ucla</span><span class="o">.</span><span class="n">sspace</span><span class="o">.</span><span class="nc">MatrixFactorNewYorkTimes</span> <span class="nc">$oneDocFile</span><span class="o">.</span><span class="n">mat</span> <span class="n">nmf</span> <span class="mi">10</span> <span class="n">nyt03_NMF_10</span><span class="o">-</span><span class="n">ws</span><span class="o">.</span><span class="n">dat</span> <span class="n">nyt03_NMF_10</span><span class="o">-</span><span class="n">ds</span><span class="o">.</span><span class="n">dat</span>
</span></code></pre></td></tr></table></div></figure>


<p>Which will decompose the term doc matrix using 10 latent features, or topics,
and store the term by topic matrix in <code>nyt03_NMF_10-ws.dat</code> and the document by
topic matrix in <code>nyt03_NMF_10-ds.dat</code>.  Because the SVD is deterministic and the
result for 500 topics includes the results for all smaller topics, we do this
just once for the SVD with 500 topics and use the appropriate number of
SVD-based topics later on.</p>

<p>After producing all the decompositions, we extract the top terms for each model
using <a href="https://github.com/fozziethebeat/TopicModelComparison/blob/master/src/main/scala/edu/ucla/sspace/ExtractTopTerms.scala">ExtractTopTerms</a>.  A run of this looks like</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">scala</span> <span class="n">edu</span><span class="o">.</span><span class="n">ucla</span><span class="o">.</span><span class="n">sspace</span><span class="o">.</span><span class="nc">ExtractTopTerms</span> <span class="nc">$topTokens</span> <span class="nc">$nTopics</span> <span class="n">nyt03_NMF_$nTopics</span><span class="o">-</span><span class="n">ws</span><span class="o">.</span><span class="n">dat</span> <span class="o">&gt;</span> <span class="n">nyt03_NMF_10</span><span class="o">.</span><span class="n">top10</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Computing Topic Coherence for all topics</h2>

<p>Computing the topic coherence depends critically on computing some similarity
value between two words that appear in the same topic.  We do this in a
multi-step process:</p>

<ol>
<li>Compute the list of all words appearing in any topic</li>
<li>Compute the Pointwise Mutual Information scores between all listed words
within an external corpus (for the UCI metric)</li>
<li>Compute document Co-Occurence scores for all listed words in the New York
Times corpus (for the UMass metric)</li>
<li>Start a server for each set of scores and query the server for the coherence
of each topic</li>
</ol>


<p>To compute the set of all words appearing in any topic, we just use this bash
command:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">cat</span> <span class="o">*.</span><span class="n">top10</span> <span class="o">|</span> <span class="n">tr</span> <span class="s">&quot; &quot;</span> <span class="s">&quot;\n&quot;</span> <span class="o">|</span> <span class="n">sort</span> <span class="o">-</span><span class="n">u</span> <span class="o">&gt;</span> <span class="nc">$allTopicTerms</span>
</span></code></pre></td></tr></table></div></figure>


<p>The <a href="https://github.com/fozziethebeat/TopicModelComparison/blob/master/src/main/scala/edu/ucla/sspace/ExtractUCIStats.scala">ExtractUCIStats</a> class will do just as it says, extract the raw scores
needed for the UCI metric, i.e. Pointwise Mutual Information scores between each
topic word as they appear within a sliding window of K words in an external
corpus.  We use a sliding window of 20 words and we use the <a href="http://wacky.sslmit.unibo.it/doku.php?id=corpora">Wackypedia</a>
corpus as our external dataset.  Similarly, <a href="https://github.com/fozziethebeat/TopicModelComparison/blob/master/src/main/scala/edu/ucla/sspace/ExtractUMassStats.scala">ExtractUMass</a> will extract the
raw scores needed for the UMass metric, i.e. document co-occurence counts for
topic words as they appear in the New York Times corpus.  These two commands
will run theses classes as desired:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">scala</span> <span class="n">edu</span><span class="o">.</span><span class="n">ucla</span><span class="o">.</span><span class="n">sspace</span><span class="o">.</span><span class="nc">ExtractUCIStats</span> <span class="nc">$allTopicTerms</span> <span class="nc">$uciStatsMatrix</span> <span class="nc">$externalCorpusFile</span>
</span><span class='line'><span class="n">scala</span> <span class="n">edu</span><span class="o">.</span><span class="n">ucla</span><span class="o">.</span><span class="n">sspace</span><span class="o">.</span><span class="nc">ExtractUMassStats</span> <span class="nc">$allTopicTerms</span> <span class="nc">$umassStatsMatrix</span> <span class="nc">$oneDocFile</span>
</span></code></pre></td></tr></table></div></figure>


<p>Then, for each metric, we startup an <a href="http://avro.apache.org/">Avro</a> based <a href="https://github.com/fozziethebeat/TopicModelComparison/blob/master/src/main/scala/edu/ucla/sspace/CoherenceServer.scala">CoherenceServer</a> that
will compute the coherence of a topic using the raw scores computed between
individual words.  This server works the same with both sets of scores computed
above, the only change is the input matrix.  We then query the server for each
topic and record the computed coherence.  A key argument for computing the
coherence is the <code>epsilon</code> value used to smooth the coherence scores such that
they remain real valued.  These two commands will start the server and query the
server for a set of topics:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">scala</span> <span class="n">edu</span><span class="o">.</span><span class="n">ucla</span><span class="o">.</span><span class="n">sspace</span><span class="o">.</span><span class="nc">CoherenceServer</span> <span class="nc">$uciStatsMatrix</span> <span class="nc">$allTopicTerms</span> <span class="nc">$port</span> <span class="o">&amp;</span>
</span><span class='line'><span class="n">scala</span> <span class="n">edu</span><span class="o">.</span><span class="n">ucla</span><span class="o">.</span><span class="n">sspace</span><span class="o">.</span><span class="nc">SenseCoherence</span> <span class="nc">$port</span> <span class="nc">$top10File</span> <span class="nc">$epsilon</span> <span class="s">&quot;$model $numTopics $metricName&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p>The port value needs to be the same for both commands and you must wait for the
server to full start before running the second command.  <code>$top10File</code>
corresponds to the list of top 10 words per topic computed in the previous
section, the third argument is a set of values to be printed for each coherence
score reported, and lastly <code>$epsilon</code> is some positive non-zero number.  After
running <code>SenseCoherence</code> as above, it should report lines with this format:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="nc">LDA</span> <span class="mi">10</span> <span class="nc">UCI</span> <span class="mi">1</span> <span class="o">&lt;</span><span class="n">score</span><span class="o">&gt;</span>
</span><span class='line'><span class="nc">LDA</span> <span class="mi">10</span> <span class="nc">UCI</span> <span class="mi">2</span> <span class="o">&lt;</span><span class="n">score</span><span class="o">&gt;</span>
</span><span class='line'><span class="nc">LDA</span> <span class="mi">10</span> <span class="nc">UCI</span> <span class="mi">3</span> <span class="o">&lt;</span><span class="n">score</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>With each line corresponding to a topic with a given id computed by LDA using 10
topics and evaluated by the UCI measure.  For our experiments, we considered
using 1.0 and 1E-12 for epsilon.</p>

<h2>Comparing Word Similarities with Semantic Judgements</h2>

<p>We compared the reduced word representations against two standard sets of
semantic similarity judgements as our second experiment.  We&#8217;re including the
sets of semantic similarity judgements with this repository since they are both
publicly available.  The processing steps remains the same for both sets of
judgements and each topic model.</p>

<p>We use the <a href="http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient">Spearman Rank Correlation</a> between humans semantic judgements and
the cosine similarity between latent word representations as the key metric.  A
higher rank correlation, or any other correlation measure, indicates that the
latent feature space better captures relations observed by humans judges.  The
<a href="https://github.com/fozziethebeat/TopicModelComparison/blob/master/src/main/scala/edu/ucla/sspace/ComputeCorrelation.scala">ComputeCorrelation</a> class will perform these calculations for a single set of
semantic judgments and a term by topic matrix.  We run this with</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">scala</span> <span class="n">edu</span><span class="o">.</span><span class="n">ucla</span><span class="o">.</span><span class="n">sspace</span> <span class="nc">$topTokens</span> <span class="n">nyt03_LDA_10</span><span class="o">-</span><span class="n">ws</span><span class="o">.</span><span class="n">dat</span> <span class="n">data</span><span class="o">/</span><span class="n">wordSim65pairs</span><span class="o">.</span><span class="n">tab</span> <span class="s">&quot;LDA 10&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Which computes the correlation between a LDA based topic model using 10 topics
and the Rubenstein and Goodenough dataset and again reports some tag information
when printing the correlation value.  We do this for all topic models computed
and the <code>data/wordSim65pairs.tab</code> and <code>data/combined.tab</code> semantic judgements
files which correspond to the <a href="http://dl.acm.org/citation.cfm?id=365657">Rubenstein and Goodenough</a> dataset and the
<a href="http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/">WordSim 353</a> dataset, repsectively.</p>

<h2>Computing Classifier accuracy using latent feature spaces</h2>

<p>As our last experiment, we test the each topic model&#8217;s ability to distinguish
different documents.  Since each represented New York Times document has a broad
semantic label, the section of the paper it was printed in, we evaluate the
representations by training and testing a classifier using the document by topic
features learned by each model.  This evaluation requires just two steps:</p>

<ol>
<li>Forming a training/testing split.</li>
<li>Learning a classifier on the training data and testing it with the test data.</li>
</ol>


<p>We used 10 fold stratified cross fold validation, which insoles splitting the
document by topic space into 10 evenly sized portions which each contain the
same proportion of each New York Times section, for example each fold should
have 60% Opinion documents and 40% World News documents.</p>

<p>The <a href="https://github.com/fozziethebeat/TopicModelComparison/blob/master/src/main/scala/edu/ucla/sspace/FormFolds.scala">FormFolds</a> command will produce these even stratified folds and
simultaneously drop any documents that have more than 1 section label or a
section label that is applied to fewer than 2000 documents.  We drop these
documents to limit the classification task to well represented and unambiguous
section labellings.  We run this command once with</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">cat</span> <span class="nc">$oneDocFile</span> <span class="o">|</span> <span class="n">cut</span> <span class="o">-</span><span class="n">f</span> <span class="mi">1</span> <span class="o">&gt;</span> <span class="nc">$docLabels</span>
</span><span class='line'><span class="n">scala</span> <span class="n">edu</span><span class="o">.</span><span class="n">ucla</span><span class="o">.</span><span class="n">sspace</span><span class="o">.</span><span class="nc">FormFolds</span> <span class="nc">$docLabels</span> <span class="nc">$numFolds</span> <span class="nc">$minLabelCount</span> <span class="o">&gt;</span> <span class="nc">$classifierFolds</span>
</span></code></pre></td></tr></table></div></figure>


<p>With the folds pre-computed, we then build a classifier for each topic model
over all the folds and compute the average accuracy for the topic model across
all folds.  <a href="https://github.com/fozziethebeat/TopicModelComparison/blob/master/src/main/scala/edu/ucla/sspace/StratifiedSchiselClassify.scala">StratifiedSchiselClassify</a> will do this for a classifier type and a topic
model.  We run this with:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">scala</span> <span class="n">edu</span><span class="o">.</span><span class="n">ucla</span><span class="o">.</span><span class="n">sspace</span><span class="o">.</span><span class="nc">StratifiedSchiselClassify</span> <span class="o">-</span><span class="n">d</span> <span class="nc">$classifier</span> <span class="nc">$ds</span> <span class="nc">$docLabels</span> <span class="o">\</span>
</span><span class='line'>                                      <span class="nc">$classifierFolds</span> <span class="s">&quot;$model $numTopics $classifier&quot;</span> <span class="o">\</span>
</span></code></pre></td></tr></table></div></figure>


<p><code>$classifier</code> can be nb, c45, dt, or me, corresponding to a Naive Bayes, C4.5
tree, ID3 Decision tree, or Maximum Entropy classifier as provided by
<a href="http://mallet.cs.umass.edu/">Mallet</a>.  Note that in our original experiments, we used the <a href="http://morden.csee.usf.edu/avatar/">Avatar</a>
project.  A writeup on how to use this will be forth coming.</p>

<h2>Using the automated script</h2>

<p>The writeup so far has described the steps we used to compute each experiment
in more detail than provided in the original paper.  However, to make this even
easier to replicate, we&#8217;ve provided a <a href="https://github.com/fozziethebeat/TopicModelComparison/blob/master/run.sh">run</a> script that automates the process
as much as possible.  This section describes the minimum number of steps needed
to setup the script and do the processing.  However, since many of the
computations are embarrassingly parallel, we didn&#8217;t use this <strong>exact</strong> script to
do our processing.  Where noted, we used the same scala class files and inputs,
but parallelized the large number of runs using <a href="http://wiki.apache.org/hadoop/HadoopStreaming">Hadoop Streaming</a>.  Since
<a href="http://wiki.apache.org/hadoop/HadoopStreaming">Hadoop Streaming</a> can be highly frustrating and finicky, we leave that
parallelizing up to you.</p>

<p>Before using the script, you need to download and prepare a few key files that
we cannot distribute:</p>

<ol>
<li>The <a href="http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2008T19">New York Times Annotated Corpus</a> for the <a href="http://www.ldc.upenn.edu/">Linguistics Data Consortium</a>.<br/>
After downloading this, unzip the 2003 portion of the corpus.  Then set the
<code>nytCorpusDir</code> variable to point to that directory.  If you&#8217;ve set it up
properly it should have a subdirectory for each month, each of which has
subdirectories for each day of that month which holds the articles written in
that month.</li>
<li>Download a <a href="http://opennlp.apache.org/">OpenNlp</a> <a href="http://opennlp.sourceforge.net/models-1.5/">Maximum Entropy</a> tokenize model from <a href="http://opennlp.sourceforge.net/models-1.5/en-token.bin">here</a>.  Set
the <code>tokenizerModel</code> variable to the location of this file.</li>
<li>Download a stop word file.  We used the <a href="https://github.com/fozziethebeat/S-Space/blob/dev-wordsi/data/english-stop-words-large.txt">english-stop-words-large.txt</a> file
provided by the <a href="https://github.com/fozziethebeat/S-Space">S-Space</a> package.  Set the <code>stopWords</code> variable to the
location of this file.</li>
<li>Download the <a href="http://wacky.sslmit.unibo.it/doku.php?id=corpora">Wackypedia</a> corpus and set the <code>externalCorpusDir</code> variable
to it&#8217;s location.</li>
</ol>


<p>Once those variables have been set and the data has been downloaded, the script
should run using the same parameters we used in our experiments.  It will
eventually produce a large number of word by topic matrices, document by topic
matrices, list of top words per topic files, and a series of data files for each
experiment that can easily be plotted using R.</p>

<p>If you wish to change some variables, here&#8217;s the meaning of each one:</p>

<ul>
<li><code>numTopTokens</code>: The number of words in the corpus to represent (not including
words in the semantic similarity judgements).</li>
<li><code>numTopWordsPerTopic</code>: The number of top words to report for each topic</li>
<li><code>transform</code>: The transformation type used for building the Term Document
Matrix used by LSA.</li>
<li><code>topicSequence</code>: A sequence of numbers indicating how many topics to learn for
each model</li>
<li><code>lastTopc</code>: The largest number of topics requested</li>
<li><code>exponents</code>: A sequence of numbers indicating the exponent corresponding to
each value of epsilon used by the coherence metrics.</li>
<li><code>numFolds</code>: The number of stratified folds to compute for the classifier
evaluation</li>
<li><code>minLabelCount</code>: The minimum number of times each section label should occur</li>
<li><code>port</code>: The port number for the coherence server.</li>
</ul>


<p>All other variables used indicate location and names of files generated by the
run script.  If space is a concern, set <code>topicDir</code> to a location with a large
amount of disk space, most of the generated results will be stored in that
location.  All other location parameters can be similarly changed as needed
without affecting the run script (please notify us by filing an issue if this
statement turns out to be wrong).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sonatype and Scala: A relationship of trickery]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/06/02/sonatype-and-scala-a-relationship-of-trickery/"/>
    <updated>2012-06-02T22:28:00+09:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/06/02/sonatype-and-scala-a-relationship-of-trickery</id>
    <content type="html"><![CDATA[<p>Open source projects are amazing.  They let you easily share the work you do
with others so that they can iterate and improve upon your ideas, an especially
invaluable quality when you later move on to other things.  <a href="http://maven.apache.org/">Maven</a> in a very
similar fashion makes sharing open source (or even closed source) java projects
super easy.  If you structure your codebase in a <a href="http://maven.apache.org/">Maven</a> way and register an
account with <a href="https://oss.sonatype.org/">Sonatype</a>, you can get your project&#8217;s jars distributed
worldwide so that anyone and everyone can used your stuff without having to
download and compile your code.</p>

<p>For pure java projects, this process is utterly <a href="https://docs.sonatype.org/display/Repository/Sonatype+OSS+Maven+Repository+Usage+Guide">trivial</a>, just follow some
instructions and it all works out beautifully.  But java is verbose and often
prefer to use <a href="http://www.scala-lang.org/">Scala</a>, especially for small prototype systems.  But how do
you deploy a mavenized scala project with <a href="https://oss.sonatype.org/">Sonatype</a>?  Well, it all pretty
much works out except that you need to create a jar of <strong>javadocs</strong> before
<a href="https://docs.sonatype.org/display/Repository/Sonatype+OSS+Maven+Repository+Usage+Guide#SonatypeOSSMavenRepositoryUsageGuide-6.CentralSyncRequirement">Sonatype</a> will let you publish anything.  Not being java and all, scala does
not do this out of the box, and maven won&#8217;t either.  But, there&#8217;s a cute hack
you can do if you&#8217;re using maven 3.0.  Just add this snippet to your pom file
and watch the deployment rock:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'>  <span class="nt">&lt;build&gt;</span>
</span><span class='line'>    <span class="nt">&lt;plugins&gt;</span>
</span><span class='line'>      ...
</span><span class='line'>      <span class="nt">&lt;plugin&gt;</span>
</span><span class='line'>        <span class="nt">&lt;groupId&gt;</span>net.alchim31.maven<span class="nt">&lt;/groupId&gt;</span>
</span><span class='line'>        <span class="nt">&lt;artifactId&gt;</span>scala-maven-plugin<span class="nt">&lt;/artifactId&gt;</span>
</span><span class='line'>        <span class="nt">&lt;version&gt;</span>3.0.2<span class="nt">&lt;/version&gt;</span>
</span><span class='line'>        <span class="nt">&lt;executions&gt;</span>
</span><span class='line'>          <span class="nt">&lt;execution&gt;</span>
</span><span class='line'>            <span class="nt">&lt;id&gt;</span>javadoc-jar<span class="nt">&lt;/id&gt;</span>
</span><span class='line'>            <span class="nt">&lt;phase&gt;</span>package<span class="nt">&lt;/phase&gt;</span>
</span><span class='line'>            <span class="nt">&lt;goals&gt;</span>
</span><span class='line'>              <span class="nt">&lt;goal&gt;</span>doc-jar<span class="nt">&lt;/goal&gt;</span>
</span><span class='line'>            <span class="nt">&lt;/goals&gt;</span>
</span><span class='line'>          <span class="nt">&lt;/execution&gt;</span>
</span><span class='line'>        <span class="nt">&lt;/executions&gt;</span>
</span><span class='line'>      <span class="nt">&lt;/plugin&gt;</span>
</span><span class='line'>    <span class="nt">&lt;/plugins&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/build&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>This uses the versitile <a href="http://davidb.github.com/scala-maven-plugin/">scala maven plugin</a> and runs the <code>doc-jar</code> goal to
build scala docs every time you type <code>mvn package</code> or some command that depends
on <code>mvn package</code>, this includes <code>mvn deploy</code>.  The id bit creates a
<code>javadoc.jar</code> containing the scaladoc.  So with just that tidbit, you can deploy
your maven projects to <a href="https://oss.sonatype.org/">Sonatype</a> with no issues at all.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Stepping into the Mixtures with Gibbs Sampling]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/05/17/stepping-into-the-mixtures-with-gibbs-sampling/"/>
    <updated>2012-05-17T16:07:00+09:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/05/17/stepping-into-the-mixtures-with-gibbs-sampling</id>
    <content type="html"><![CDATA[<script src="http://fozziethebeat.github.com/javascripts/raphael-min.js"></script>


<script src="http://fozziethebeat.github.com/javascripts/raphael-extra.js"></script>


<script src="http://fozziethebeat.github.com/javascripts/graphing-resize.js"></script>


<h3>Refresher</h3>

<p><img class="right" src="http://fozziethebeat.github.com/images/empty_pool.jpg" width="300" height="600" title="Empty Pools = Not fun for diving" >
As stated before <a href="blog/2012/05/04/getting-to-the-mixtures-in-dirichlet-process-mixture-models/">here</a> and <a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">elsewhere</a>, mixture models are pretty cool.
But rather than diving head first into a complicated mixture model like the
<a href="http://en.wikipedia.org/wiki/Dirichlet_process">Dirichlet Process Mixture Model</a> or the <a href="http://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process">Hierarchical Dirichlet Process</a>,
we&#8217;re going to ease our way into these models, just like you&#8217;d learn to accept
the horrors of a pool of water before you try deep sea diving.  First, we&#8217;ll
figure out how to apply the <a href="http://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs Sampling Method</a> with the <a href="http://en.wikipedia.org/wiki/Linked_list">Linked List</a>
of machine learning methods, <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a>.  Then, after <a href="http://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs Sampling</a>
feels comfy, we&#8217;ll tackle a <a href="http://en.wikipedia.org/wiki/Mixture_model">Finite Mixture Model</a> with <a href="http://en.wikipedia.org/wiki/Multivariate_normal_distribution">Gaussian</a>
components.  And with the magic of <a href="https://github.com/scalala/Scalala">Scalala</a> and <a href="http://www.scalanlp.org/">Scalanlp</a> the code&#8217;ll be
short, sweet, and easy to understand.</p>

<h3>The Toy of Machine Learning Methods</h3>

<p><img class="left" src="http://fozziethebeat.github.com/images/Single_linked_list.png" width="400" height="200" title="Linked Lists = Tool for Learning" >
Simple but instructive data structures like a <a href="http://en.wikipedia.org/wiki/Linked_list">Linked List</a> work well as an
instructive tool.  Good libraries provide solid and efficient implementations,
but it&#8217;s straightforward enough for beginners to implement in a variety of
manners.  For our goals, <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a> will serve the same purpose.  As simple
as the model is, it&#8217;s reasonably powerful.</p>

<p>So what is the <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a> model?  Let&#8217;s say we want to group a bundle of
emails into two groups: spam and not spam.  And all you have to go on are the
contents of the emails.  In this situation, we&#8217;d like to just use the words in a
single email to decide whether or not it&#8217;s spamtastic or spamfree.  <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a>
assumes that you can make this decision based on each word in the
email individually and then combine these decisions.  It&#8217;s <a href="http://en.wikipedia.org/wiki/Bayes'_rule">Bayesian</a>
because it&#8217;s a simple <a href="http://en.wikipedia.org/wiki/Bayesian_network">Bayesian Model</a> and it&#8217;s Naive because this approach
assumes the words are totally independent and have no statistical relation
whatsoever.</p>

<h3>Model Representations</h3>

<p>Due to it&#8217;s simplicity, and that naive assumption, Naive Bayes makes for an easy
model to understand and describe using a variety of models.  here&#8217;s some of the
most frequent descriptions:</p>

<h4>A Bayesian Network</h4>

<table>
<td>
<div id='naivebayes'></div>
<script type='text/javascript'>
(function() {

  function draw() {
        var paper = Raphael('naivebayes', 200, 300);
        paper.circle(60, 20, 20);
        paper.text(60, 20, "α")
             .attr({"font-size": 25});

        paper.arrow(60, 40, 60, 80, 10);

        paper.rect(10,60, 100, 150);

        paper.circle(60, 100, 20);
        paper.text(60, 100, "K")
             .attr({"font-size": 25});


        paper.circle(150, 100, 20);
        paper.text(150, 100, "γ")
             .attr({"font-size": 25});

        paper.arrow(150, 120, 150, 150, 10);

        paper.rect(120, 140, 60, 60);
        paper.circle(150, 170, 20);
        paper.text(150, 170, "θ")
             .attr({"font-size": 25});

        paper.arrow(130, 170, 80, 170, 10);

        paper.rect(29, 140, 60, 60);

        paper.arrow(60, 120, 60, 150, 10);

        paper.circle(60, 170, 20);
        paper.text(60, 170, "X")
             .attr({"font-size": 25});
    }

    draw();

    $(window).resize(function() {
        draw();
    });

})();
</script>
</td>
<td>

<script type="math/tex; mode=display">
\begin{array}{lll}
\phi & \sim & Dirichlet(\alpha) \\
\theta_{s} & \sim & Dirichlet(\gamma_s) \\
\theta_{n} & \sim & Dirichlet(\gamma_n) \\
K_{j} & \sim & Multinomial(\phi) \\
X_j & \sim & Multinomial(\theta_{K_{j}}) 
\end{array}
</script>

</td>
</table>


<p>These two views of Naive Bayes describe the same model, but different ways
of computing it.  For the model on the right, if we work backwards, it says that
a data point <script type="math/tex"> X_j </script> comes from some multinomial distribution <script type="math/tex"> \theta_{K_j} </script>.  <script type="math/tex"> K_j </script> is a latent variable that encode which
group, i.e. spam or not spam, the data point came from, and that itself comes
from a multinomial <script type="math/tex"> \phi </script>.  The two <script type="math/tex"> \theta_s, \theta_n </script> distributions describe our knowledge about spam and non-spam emails,
respectively.  And finally, all three multinomial distributions come from
<a href="http://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet</a> distributions.  The added magic of the <a href="http://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet</a>
distrubtion is that it <strong>generates</strong> multinomial distributions, this
relationship is more formally known as a <a href="http://en.wikipedia.org/wiki/Conjugate_prior">Conjugate Prior</a>.  The plate
diagram on the left says <strong>nearly</strong> the same story except it <em>collapses</em> out the <script type="math/tex"> \phi </script> distribution and links <script type="math/tex"> \alpha </script> directly to
the compnent labels.  We could also in theory collapse the <script type="math/tex"> \theta </script> distrubtions, but we&#8217;ll not do that
for the moment.</p>

<h4>A Graphical Model</h4>

<table>
<td>
<div id="nb-graph"></div>
</td>
<td>
<div id="nb-factor"></div>
</td>
</table>




<script type='text/javascript'>
(function() {

  function draw() {
        var paper = Raphael('nb-graph', 200, 200);

        paper.circle(100, 30, 20);
        paper.text(100, 30, "Y")
             .attr({"font-size": 20});

        paper.circle(30, 160, 20);
        paper.text(30, 160, "w_1")
             .attr({"font-size": 20});

        paper.circle(80, 160, 20);
        paper.text(80, 160, "w_2")
             .attr({"font-size": 20});

        paper.text(120, 160, "...")
             .attr({"font-size": 25});

        paper.circle(170, 160, 20);
        paper.text(170, 160, "w_v")
             .attr({"font-size": 20});

        paper.arrow(100, 50, 30, 140, 10);
        paper.arrow(100, 50, 80, 140, 10);
        paper.arrow(100, 50, 170, 140, 10);
    }

    draw();

    $(window).resize(function() {
        draw();
    });

})();
</script>




<script type='text/javascript'>
(function() {

  function draw() {
        var paper = Raphael('nb-fail', 200, 200);

        paper.circle(100, 30, 20);
        paper.text(100, 30, "Y")
             .attr({"font-size": 20});

        paper.circle(30, 160, 20);
        paper.text(30, 160, "w_1")
             .attr({"font-size": 20});

        paper.circle(80, 160, 20);
        paper.text(80, 160, "w_2")
             .attr({"font-size": 20});

        paper.text(120, 160, "...")
             .attr({"font-size": 25});

        paper.circle(170, 160, 20);
        paper.text(170, 160, "w_v")
             .attr({"font-size": 20});

        paper.factor(100, 50, 30, 140, 10);
        paper.factor(100, 50, 80, 140, 10);
        paper.factor(100, 50, 170, 140, 10);
    }

    draw();

    $(window).resize(function() {
        draw();
    });

})();
</script>


<p>This graphical view tells an alternative story.  Instead of probability
distrubutions making other distributions and then finally making data points.
this says that the class of a data point, <script type="math/tex"> Y </script> generates the
features of the data point, <script type="math/tex">, w_1, w_2, \cdots, w_v </script>.  This is
where the naive part comes from, each &#8220;feature&#8221;, i.e. word in the email, is
completely disconnected from the other words and only depends on the class of
the email.</p>

<p><img class="right" src="http://fozziethebeat.github.com/images/thomas_bayes.gif" title="Thomas Bayes, not so Naive" ></p>

<h4>A Probability Distribution</h4>

<p>Finally, there&#8217;s the written way to understand Naive Bayes, as a conditional
probability distrubtion.  Let&#8217;s write out the distribution for just the spam
class:</p>

<script type="math/tex; mode=display">
p(spam | email) \propto p(spam) \prod_{word \in email} p(word | spam)
</script>


<p>This looks pretty much the same for non-spam.  Simple, no? If we combine
together the different views above with this written form, then we can think of
the class <script type="math/tex">spam</script> as a <a href="http://en.wikipedia.org/wiki/Multinomial_distribution">Multinomial</a> distribution over words and the
probability of a word given the class spam is really just the probability of
picking the word based on how many times it&#8217;s shown up in spammy emails.
Similarly, the probability of seeing spam is just the number of times you&#8217;ve
seen a delcious piece of spam, be it on your phone, at home, or in the frying
pan.  Using that intuition and some Bayesian math, the above definition gets
boild down to:</p>

<script type="math/tex; mode=display">
p(s|e) \propto \frac{C_{s} + \alpha_s - 1}{N + \alpha_s + \alpha_n- 1}
                \prod_{w \in e} \theta_{s,w}^{C_{w,e}}
</script>


<p>Let&#8217;s get some definitions out of the way.</p>

<script type="math/tex; mode=display">
\begin{array}{lll}
C_s &=& \text{number of times spam's been seen} \\
C_{w,e} &=& \text{number of times seeing word w in email e} \\
N &=& \text{number of data points seen} \\
\alpha_{s,n} &=& \text{priors for each class} \\
\theta_{s,w} &=& \text{the wight given to word w in class spam} \\
\end{array}
</script>


<p>Let&#8217;s tease this appart into two pieces, <script type="math/tex"> p(spam) </script> and <script type="math/tex"> p(email|spam) </script>.  The first part is:</p>

<script type="math/tex; mode=display">
p(s) \propto \frac{C_{s} + \alpha_s - 1}{N + \alpha_s + \alpha_n- 1}
</script>


<p>This basically says that the probability of spam is the number of time&#8217;s we&#8217;ve
seen it, plus some smoothing factors so that both span and non-spam have some
positive possibility of existing.  Amazingly! this is also equivalent to the
<em>collapsed</em> <script type="math/tex">\phi</script> distribution we mentioned in the bayesian
description of Naive Bayes, by doing just a little bit of math and realizing
that the Dirichlet is a conjugate prior of a multinomial, we get the above
distribution for each class.</p>

<p>Next,</p>

<script type="math/tex; mode=display">
p(email|s) = \prod_{w \in e} \theta_{s,w}^{C_{w,e}}
</script>


<p>If <script type="math/tex">\theta_s</script> describes the probability of each word occuring in a
spammy email, then <script type="math/tex">\theta_{w,s}<sup>{C_{w,e}}</script></sup> becomes the likelihood of
seeing that word as many times as observed in the email.  By computing the
product of all these words, we <em>naively</em> get the probability of the full email.
Slam the two distributions together and we get the full likelihood of the email
given the class spam.</p>

<h3>Don&#8217;t forget Gibbs!</h3>

<p>Now that Naive Bayes makes some more sense in a variety of descriptive flavours
<img class="right" src="http://fozziethebeat.github.com/images/Josiah_Willard_Gibbs_-from_MMS-.jpg" width="300" height="600" title="The man who made sampling cool before HipHop" >
and we&#8217;ve defined our distrubutions and hyper parameters, it&#8217;s time to throw in
gibbs sampling.  To quickly refresh our memories, the recipe for gibbs at this
stage is:</p>

<ol>
<li>for each data point <script type="math/tex"> X_j </script>

<ol>
<li>Forget about <script type="math/tex"> X_j </script></li>
<li>Compute the likelihood that each component, <script type="math/tex">\theta_*</script>, generated <script type="math/tex">X_j</script></li>
<li>Sample and jot down a new component, <script type="math/tex">K_j</script>, for <script type="math/tex">X_J</script></li>
<li>Remember <script type="math/tex"> X_J </script></li>
</ol>
</li>
<li>Restimate our component parameters, <script type="math/tex">\theta_{*} </script></li>
<li>Repeat</li>
</ol>


<p>You might be wondering &#8220;why forget about the current data point?&#8221;  The reason is
that we&#8217;re trying to decide what to do with a <strong>new data point</strong>.  Gibbs
sampling only really works because the distrubtions we&#8217;re working with are
<a href="http://en.wikipedia.org/wiki/Exchangeable_random_variables">exchangeable</a>, i.e. the ordering of our data doesn&#8217;t matter.  So we can
just pretend any data point is new by just forgetting it ever existed, Note,
this is why <script type="math/tex">p(spam)</script> has a <script type="math/tex">-1</script> in the denominator, that count
has been &#8220;forgotten&#8221;.  Other than that tidbit, we just pretend everything else
is the same.</p>

<h3>Writing out the code</h3>

<p><img class="left" src="http://fozziethebeat.github.com/images/spam_sampling.jpg" width="150" height="150" title="Tasty Samples of Spam" >
With Gibbs sampling fresh in our minds, and our probability distributions clear,
let&#8217;s see what some real code looks like.  To do this, I&#8217;m going to use
<a href="https://github.com/scalala/Scalala">Scalala</a> and <a href="http://www.scalanlp.org/">Scalanlp</a>, which take care of the linear algebra and
probability distrubtions for us really concisely.  And while it may not be the
fastest library around, it&#8217;s super easy to read and understand.  For faster
implementations, you can just translate this code into whatever beast you desire
and optimize away.</p>

<h4>Setting up before sampling</h4>

<p>Before we go on a spam sampling binge, we gotta setup our initial groupings.
Gibbs sampling works pretty well with a random starting point, so let&#8217;s go with
that:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">def</span> <span class="n">sampleThetas</span><span class="o">(</span><span class="n">labelStats</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">DenseVectorRow</span><span class="o">[</span><span class="kt">Double</span><span class="o">]])</span> <span class="k">=</span>
</span><span class='line'>    <span class="n">labelStats</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">n_c</span> <span class="k">=&gt;</span> <span class="k">new</span> <span class="nc">Dirichlet</span><span class="o">(</span><span class="n">n_c</span><span class="o">+</span><span class="n">gamma</span><span class="o">).</span><span class="n">sample</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="k">val</span> <span class="n">k</span> <span class="k">=</span> <span class="n">numComponents</span>
</span><span class='line'><span class="k">val</span> <span class="n">v</span> <span class="k">=</span> <span class="n">numFeatures</span>
</span><span class='line'><span class="k">val</span> <span class="n">n</span> <span class="k">=</span> <span class="n">numDataPoints</span>
</span><span class='line'><span class="k">var</span> <span class="n">labelStats</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">.</span><span class="n">fill</span><span class="o">(</span><span class="n">k</span><span class="o">)(</span><span class="k">new</span> <span class="nc">DenseVectorRow</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span>
</span><span class='line'>    <span class="nc">Array</span><span class="o">.</span><span class="n">fill</span><span class="o">(</span><span class="n">v</span><span class="o">)(</span><span class="mi">0</span><span class="n">d</span><span class="o">)))</span>
</span><span class='line'><span class="k">var</span> <span class="n">thetas</span> <span class="k">=</span> <span class="n">sampleThetas</span><span class="o">(</span><span class="n">labelStats</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="k">val</span> <span class="n">labels</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Multinomial</span><span class="o">(</span><span class="k">new</span> <span class="nc">Dirichlet</span><span class="o">(</span><span class="n">alpha</span><span class="o">).</span><span class="n">sample</span><span class="o">.</span><span class="n">data</span><span class="o">).</span><span class="n">sample</span><span class="o">(</span><span class="n">n</span><span class="o">).</span><span class="n">toArray</span>
</span><span class='line'><span class="k">var</span> <span class="n">labelCounts</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DenseVectorRow</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="nc">Array</span><span class="o">.</span><span class="n">fill</span><span class="o">(</span><span class="n">k</span><span class="o">)(</span><span class="mi">0</span><span class="n">d</span><span class="o">))</span>
</span><span class='line'><span class="n">data</span><span class="o">.</span><span class="n">zip</span><span class="o">(</span><span class="n">labels</span><span class="o">).</span><span class="n">foreach</span><span class="o">{</span>
</span><span class='line'>    <span class="k">case</span> <span class="o">(</span><span class="n">x_j</span><span class="o">,</span> <span class="n">j</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">{</span><span class="n">labelStats</span><span class="o">(</span><span class="n">j</span><span class="o">)</span> <span class="o">+=</span> <span class="n">x_j</span><span class="o">;</span> <span class="n">labelCounts</span><span class="o">(</span><span class="n">j</span><span class="o">)</span> <span class="o">+=</span> <span class="mi">1</span><span class="o">}</span> <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>So what&#8217;s going on here?  Well, first, we just get some basic stats about our
data such as the size, the number of features and the number of components we
want, in this case, 2.  Then we make a data structure that will hold the number
of times we see each feature within each component, <code>labelStats</code>; the
number of times we&#8217;ve seen each component overall <code>labelCounts</code>; and the
parameters for each compnent, <code>thetas</code>.  Finally, randomly assign each point to
a random component and update the <code>labelStats</code> and <code>labelCounts</code>.   Note that we
get the <code>thetas</code> from a Dirichlet distribution that uses the <code>labelStats</code> and a
smoothing parameter <code>gamma</code>, just like we noted in the Bayesian Plate diagram
above.</p>

<p>Now for the real sampling meat:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">for</span> <span class="o">(</span><span class="n">i</span> <span class="k">&lt;-</span> <span class="mi">0</span> <span class="n">until</span> <span class="n">numIterations</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">for</span> <span class="o">(</span> <span class="o">(</span><span class="n">x_j</span><span class="o">,</span> <span class="n">j</span><span class="o">)</span> <span class="k">&lt;-</span> <span class="n">data</span><span class="o">.</span><span class="n">zipWithIndex</span> <span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">label</span> <span class="k">=</span> <span class="n">labels</span><span class="o">(</span><span class="n">j</span><span class="o">)</span>
</span><span class='line'>        <span class="n">labelStats</span><span class="o">(</span><span class="n">label</span><span class="o">)</span> <span class="o">-=</span> <span class="n">x_j</span>
</span><span class='line'>        <span class="n">labelCounts</span><span class="o">(</span><span class="n">label</span><span class="o">)</span> <span class="o">-=</span> <span class="mi">1</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">val</span> <span class="n">prior</span> <span class="k">=</span> <span class="n">labelCounts</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="o">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="o">+</span><span class="n">alpha</span><span class="o">.</span><span class="n">sum</span><span class="o">)</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">likelihood</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DenseVectorRow</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="n">thetas</span><span class="o">.</span><span class="n">map</span><span class="o">(</span> <span class="n">theta</span> <span class="k">=&gt;</span>
</span><span class='line'>            <span class="o">(</span><span class="n">theta</span> <span class="o">:^</span> <span class="n">x_j</span><span class="o">).</span><span class="n">iterator</span><span class="o">.</span><span class="n">product</span><span class="o">).</span><span class="n">toArray</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">val</span> <span class="n">probs</span> <span class="k">=</span> <span class="n">prior</span> <span class="o">:*</span> <span class="n">likelihood</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">new_label</span><span class="k">=</span> <span class="k">new</span> <span class="nc">Multinomial</span><span class="o">(</span><span class="n">probs</span> <span class="o">/</span> <span class="n">probs</span><span class="o">.</span><span class="n">sum</span><span class="o">).</span><span class="n">sample</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">labelCounts</span><span class="o">(</span><span class="n">new_label</span><span class="o">)</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class='line'>        <span class="n">labelStats</span><span class="o">(</span><span class="n">new_label</span><span class="o">)</span> <span class="o">+=</span> <span class="n">x_j</span>
</span><span class='line'>        <span class="n">labels</span><span class="o">(</span><span class="n">j</span><span class="o">)</span> <span class="k">=</span> <span class="n">new_label</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="n">thetas</span> <span class="k">=</span> <span class="n">sampleThetas</span><span class="o">(</span><span class="n">labelStats</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The first three lines in the loop get the current label for <code>x_j</code> and then
forget about it&#8217;s data.  <code>prior</code> represents the likelihood of selecting each
component just based on it&#8217;s frequency, i.e. <script type="math/tex">p(spam)</script>.  <code>likelihood</code>
represents the likelihood of each component generating <code>x_j</code>, i.e. <script type="math/tex"> p(e|s)</script>.
To finish off the sampling procedure, we sample a new label by turning these
likelihoods into a multinomial distrubtion and sample from it.  With the new
label we then add in the data for <code>x_j</code> to the that component&#8217;s stats.  Once
we&#8217;ve made a full sweep through the data set, we update our parmeters for each
component by resampling from a Dirichlet distribution by using the our
<code>labelStats</code>.</p>

<p>And Voila! We have a simple gibbs sampler for Naive Bayes.</p>

<h3>Running it on some data</h3>

<p>Let&#8217;s see how this unsupervised version of Naive Bayes handles a really generic
looking dataset: globs of data points from three different Gaussian
distributions, each with a different mean and variance.  And let&#8217;s start off
with just trying to find 2 groups, as we&#8217;ve been working along with so far.</p>

<p><img class="right" src="http://fozziethebeat.github.com/images/test.nb.groups.png" title="Splitting apart some simple groups" ></p>

<p>Looks pretty neat huh?  Even with a terrible and completely random starting
point, our learner manages to split up the data in a sensible way by finding a
dividing plane between two groups.  What&#8217;s even cooler, is that notice how after
iteration 1 completes, nearly all the data is assigned to one class.  By somehow
by iteration 10, we&#8217;ve managed to finagle our way out of that terrible solution
and into a <strong>significantly</strong> better solution.  And by the 100th iteration, we&#8217;ve
stayed there in a stable state.</p>

<p>Now what happens if we try finding 3 components in the data set?</p>

<p><img class="right" src="http://fozziethebeat.github.com/images/test.nb.3.groups.png" title="Splitting apart some simple groups" ></p>

<p>Here we get a similar split, but not what we&#8217;re looking for.  Our mixture model
gets all befuddled with the group on the bottom left and tries to flailingly
split into two smaller groups.  If you run this a couple times, you&#8217;ll see that
it does the same thing with the two green groups, it can&#8217;t split them evenly as
you&#8217;d expect.</p>

<h3>The Secret no one mentions</h3>

<p>This simple model i&#8217;ve walked through is the unsupervised equivalent to the
supervised Naive Bayes model.  It does a pretty reasonable job of splitting
apart some groups of data, but there are clearly datasets it has some issues
dealing with.  But what&#8217;s not been said so far is that this model is actually
not just Naive Bayes, it&#8217;s a <a href="http://en.wikipedia.org/wiki/Mixture_model">Finite Mixture Model</a> with Multionomial
components.  The code, probabilities, and graphs i&#8217;ve layed out all work for
more than two components.  So next, i&#8217;ll show two cool ways to extend this
model:</p>

<ol>
<li>With Gaussian components, which can better handle this toy data set</li>
<li>With an infinite number of components, which figure out that paramter <strong>for
you</strong> like a <em>magician</em>.</li>
</ol>


<p><img class="center" src="http://fozziethebeat.github.com/images/magician.jpg" title="Magicians figure out their own parameters" ></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting to the mixtures in Dirichlet Process Mixture Models]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/05/04/getting-to-the-mixtures-in-dirichlet-process-mixture-models/"/>
    <updated>2012-05-04T13:01:00+09:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/05/04/getting-to-the-mixtures-in-dirichlet-process-mixture-models</id>
    <content type="html"><![CDATA[<p>The <a href="http://en.wikipedia.org/wiki/Dirichlet_process">Dirichlet Process</a> is pretty sweet as Edwin Chen and numerous others
have <a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">pointed out</a>.  When combined with a mixture of Bayesian models, it&#8217;s
really effective for finding an accurate number of models needed to represent
your bundle of data, especially useful when you have no idea how many bundles of
related words you may have.  Edwin Chen does a really great job of
<a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">explaining</a> how the Dirichlet Process works, how to implement it, and how it
can break down the McDonald&#8217;s menu, but he leaves out details on how to combine
it with mixture models, which is by far the coolest part.</p>

<p>The short description for how to merge the Dirichlet Process with <a href="http://en.wikipedia.org/wiki/Mixture_model">Nonparametric
Mixture Modeling</a> (and infer the model) looks like this:</p>

<ol>
<li>Define probability distributions for your mixture model</li>
<li>Rewrite your distributions with Bayes rule so that parameters depend on data
and hyper parameters</li>
<li>Repeat step 2 for hyper parameters as desired (it&#8217;s turtles all the way down, as Kevin
Knight <a href="http://www.isi.edu/natural-language/people/bayes-with-tears.pdf">said</a>)</li>
<li>For each data point

<ol>
<li>Forget about the current data point</li>
<li>Compute the likelihood that each mixture made this data point</li>
<li>Sampling a new label for this data point using these likelihoods</li>
<li>Jot down this new label</li>
<li>Remember this data point</li>
</ol>
</li>
<li>After seeing all the data, re-estimate all your parameters based on the new
assignments</li>
<li>Repeat steps 4 and 5 ad nausea.</li>
</ol>


<p>In short, this is the <a href="http://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs Sampling</a> approach.  If you already know what
your distributions look like, and this paltry description made perfect sense,
Hurray! Please go bake a cake for everyone that feels a little underwhelmed with
this description.</p>

<p><img class="center" src="http://fozziethebeat.github.com/images/cake.jpg" title="Delicious cake for people still reading" ></p>

<p>Now that someone is off baking us cake, let&#8217;s dive down into what the
distributions look like.  Taking a look at a number of papers describing
variations of the Dirichlet Process Mixture Model, like <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.9111">The Infinite Gaussian Mixture Model</a>
and <a href="http://www.cs.princeton.edu/~blei/papers/BleiJordan2004.pdf">Variational Inference for Dirichlet Process Mixture Models</a>, they first
make things seem pretty straightforward.  To start, they generally throw down
this simple equation:</p>

<script type="math/tex; mode=display">
p(y|\theta_1, \cdots, \theta_k, \pi_1, \cdots, \pi_k) = \sum_j^k \pi_j p(y|\theta_j)
</script>


<p>Simple, no?  If you want mixtures of <a href="http://www.umiacs.umd.edu/~resnik/pubs/gibbs.pdf">Gaussian</a> models, then <script type="math/tex"> \theta_j</script> has a mean and a variance, we&#8217;ll call these <script type="math/tex"> \mu_j</script> and <script type="math/tex">\sigma_j<sup>2</script>.</sup>   The next step is then to rewrite this equation in a couple of ways so that
you define the parameters, i.e. <script type="math/tex"> \theta_j </script> in terms of the data
and other parameters.  One example: <script type="math/tex"> \mu_j </script> looks like this:</p>

<script type="math/tex; mode=display">
p(\mu_j|c,y,s_j, \lambda, r) \sim \mathcal{N}\left( \frac{\bar{y}n_js_j + \lambda
r}{n_js_j + r},\frac{1}{n_js_j+r} \right)
</script>


<p>Not <em>too</em> bad after reading through the definition for all the parameters; the
first bundle describes the mean of the mixture and the second bundle describes
the variance of the mixture.  You may have noticed that our means for each
mixture come from a Gaussian distribution themselves (I did say it was turtles
all the way down).  If you&#8217;re using a nice math coding environment, like
<a href="https://github.com/scalala/Scalala">Scalala</a> and <a href="http://www.scalanlp.org/">Scalanlp</a>, you can easily sample from this distribution like
this</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">theta_j</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Gamma</span><span class="o">((</span><span class="n">y_mean</span><span class="o">*</span><span class="n">n_j</span><span class="o">*</span><span class="n">s_j</span> <span class="o">+</span> <span class="n">lambda</span><span class="o">*</span><span class="n">r</span><span class="o">)/(</span><span class="n">n_j</span><span class="o">*</span><span class="n">s_j</span><span class="o">+</span><span class="n">r</span><span class="o">),</span> <span class="mi">1</span><span class="n">d</span><span class="o">/(</span><span class="n">n_j</span><span class="o">*</span><span class="n">s_j</span> <span class="o">+</span><span class="n">r</span><span class="o">)).</span><span class="n">sample</span>
</span></code></pre></td></tr></table></div></figure>


<p>This goes on for most of the parameters for the model until you get to one of
the core tear inducing rewrites.  This lovely gem describes the probably
distribution function for one of the hyper parameters in the <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.9111">Infinite Gaussian Mixture Model</a>:</p>

<script type="math/tex; mode=display">
p(\alpha|k,n) \propto \frac{\alpha^{k-3/2} exp(-1/(2\alpha))\Gamma(\alpha)} 
                           {\Gamma(\alpha / k)}
</script>


<p><em>This is not easy to sample from</em> especially if you&#8217;re new to sophisticated
sampling methods.  And sadly, just about every academic publication describing
these kinds of models gets to a point where they assume you know what they&#8217;re
talking about and can throw down <a href="http://en.wikipedia.org/wiki/Mixture_model">Monte Carlo Markov Chain</a> sampling
procedures like they&#8217;re rice at a wedding.</p>

<p><img class="center" src="http://fozziethebeat.github.com/images/wedding-rice.jpg" title="MCMC sampling methods for the newlyweds" ></p>

<p>So what&#8217;s a lowly non-statistician data modeller to do? Start with a well
written description of a similar, but significantly simpler model.  In this
case, <a href="http://www.umiacs.umd.edu/~resnik/pubs/gibbs.pdf">Gibbs Sampling for the Uninitiated</a>, which describes how to apply
Gibbs to the Naive Bayes model.  I&#8217;ll summarize this awesome paper in the next
part of this series and then tweak it a little to make a <a href="http://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model">Finite Gaussian Mixture Model</a>.<br/>
After that&#8217;s said and done, I&#8217;ll show how to extend the finite version into the
Dirichlet Process Mixture Model (with some simplifications).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Puzzle Time: Transforming Names]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/04/23/puzzle-time-transforming-names/"/>
    <updated>2012-04-23T22:18:00+09:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/04/23/puzzle-time-transforming-names</id>
    <content type="html"><![CDATA[<p>This post starts what will hopefully be a new trend: computational solutions to
<a href="http://www.npr.org/2012/04/22/151120151/a-puzzle-worthy-of-don-draper">NPR&#8217;s Sunday Puzzle</a>.  In this weeks puzzle, we have to</p>

<blockquote><p>Think of a common man&#8217;s name in four letters, one syllable. Move each letter exactly halfway around the alphabet. For example, A would become N, N would become A, and B would become O. The result will be a common woman&#8217;s name in two syllables. What names are these?</p></blockquote>


<p>Word puzzles can often be pretty hard for automated computer programs.  This is
in part what made <a href="http://www-03.ibm.com/innovation/us/watson/index.html">IBM&#8217;s Watson</a> so darn cool.  But thankfully this week&#8217;s
puzzle is amazingly simple for a program, in fact, i&#8217;d even say it&#8217;s ideal for a
program.  To solve it, just need two simple steps:</p>

<ol>
<li>Gather a bunch of male and female names</li>
<li>Process the male names as noted to see if we have any matching female names</li>
</ol>


<p>Step 1 is perhaps the hardest since it requires gathering not only names, but
common names in english.  However, <a href="http://names.mongabay.com">this</a> handy site has done the work for
us.  Here they have listings of common English names based on a variety of
categories: region, ranking, gender, alphabetical, etc.  For our purposes, we&#8217;ll
just grab the a long list of the most frequent male and female names from
<a href="http://names.mongabay.com/male_names.htm">here</a> and <a href="http://names.mongabay.com/female_names.htm">here</a>.  All the names are nicely laid out in a table with a
pretty regular format.  In fact the format is so regular we can write a <a href="http://en.wikipedia.org/wiki/Regular_expression"><em>regular expression</em></a> for it.  Here&#8217;s an example of the table:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='html'><span class='line'><span class="nt">&lt;table</span> <span class="na">align=</span><span class="s">&quot;left&quot;</span> <span class="na">CELLPADDING=</span><span class="s">&quot;3&quot;</span> <span class="na">CELLSPACING=</span><span class="s">&quot;3&quot;</span> <span class="na">style=</span><span class="s">&quot;table1&quot;</span><span class="nt">&gt;</span>
</span><span class='line'><span class="nt">&lt;tr</span> <span class="na">bgcolor=</span><span class="s">&quot;F5FDD9&quot;</span><span class="nt">&gt;&lt;td&gt;&lt;b&gt;</span>Name<span class="ni">&amp;nbsp;&amp;nbsp;</span><span class="nt">&lt;/td&gt;&lt;td&gt;&lt;b&gt;</span>% Frequency<span class="ni">&amp;nbsp;&amp;nbsp;</span><span class="nt">&lt;/td&gt;&lt;td&gt;&lt;b&gt;</span>Approx Number<span class="ni">&amp;nbsp;&amp;nbsp;</span><span class="nt">&lt;/td&gt;&lt;td&gt;&lt;b&gt;</span>Rank<span class="ni">&amp;nbsp;&amp;nbsp;</span><span class="nt">&lt;/td&gt;&lt;/tr&gt;</span>
</span><span class='line'><span class="nt">&lt;tr</span> <span class="na">bgcolor=</span><span class="s">&quot;white&quot;</span><span class="nt">&gt;&lt;td&gt;</span>AARON<span class="nt">&lt;/td&gt;&lt;td&gt;</span>0.24<span class="nt">&lt;/td&gt;&lt;td&gt;</span> 350,151 <span class="nt">&lt;/td&gt;&lt;td&gt;</span>77<span class="nt">&lt;/td&gt;&lt;/tr&gt;</span>
</span><span class='line'><span class="nt">&lt;tr</span> <span class="na">bgcolor=</span><span class="s">&quot;white&quot;</span><span class="nt">&gt;&lt;td&gt;</span>ABDUL<span class="nt">&lt;/td&gt;&lt;td&gt;</span>0.007<span class="nt">&lt;/td&gt;&lt;td&gt;</span> 10,213 <span class="nt">&lt;/td&gt;&lt;td&gt;</span>831<span class="nt">&lt;/td&gt;&lt;/tr&gt;</span>
</span><span class='line'><span class="nt">&lt;tr</span> <span class="na">bgcolor=</span><span class="s">&quot;F5FDD9&quot;</span><span class="nt">&gt;&lt;td&gt;</span>ABE<span class="nt">&lt;/td&gt;&lt;td&gt;</span>0.006<span class="nt">&lt;/td&gt;&lt;td&gt;</span> 8,754 <span class="nt">&lt;/td&gt;&lt;td&gt;</span>854<span class="nt">&lt;/td&gt;&lt;/tr&gt;</span>
</span><span class='line'><span class="nt">&lt;tr</span> <span class="na">bgcolor=</span><span class="s">&quot;white&quot;</span><span class="nt">&gt;&lt;td&gt;</span>ABEL<span class="nt">&lt;/td&gt;&lt;td&gt;</span>0.019<span class="nt">&lt;/td&gt;&lt;td&gt;</span> 27,720 <span class="nt">&lt;/td&gt;&lt;td&gt;</span>485<span class="nt">&lt;/td&gt;&lt;/tr&gt;</span>
</span><span class='line'><span class="nt">&lt;tr</span> <span class="na">bgcolor=</span><span class="s">&quot;white&quot;</span><span class="nt">&gt;&lt;td&gt;</span>ABRAHAM<span class="nt">&lt;/td&gt;&lt;td&gt;</span>0.035<span class="nt">&lt;/td&gt;&lt;td&gt;</span> 51,064 <span class="nt">&lt;/td&gt;&lt;td&gt;</span>347<span class="nt">&lt;/td&gt;&lt;/tr&gt;</span>
</span><span class='line'><span class="nt">&lt;tr</span> <span class="na">bgcolor=</span><span class="s">&quot;white&quot;</span><span class="nt">&gt;&lt;td&gt;</span>ABRAM<span class="nt">&lt;/td&gt;&lt;td&gt;</span>0.005<span class="nt">&lt;/td&gt;&lt;td&gt;</span> 7,295 <span class="nt">&lt;/td&gt;&lt;td&gt;</span>1053<span class="nt">&lt;/td&gt;&lt;/tr&gt;</span>
</span><span class='line'><span class="nt">&lt;tr</span> <span class="na">bgcolor=</span><span class="s">&quot;F5FDD9&quot;</span><span class="nt">&gt;&lt;td&gt;</span>ADALBERTO<span class="nt">&lt;/td&gt;&lt;td&gt;</span>0.005<span class="nt">&lt;/td&gt;&lt;td&gt;</span> 7,295 <span class="nt">&lt;/td&gt;&lt;td&gt;</span>1040<span class="nt">&lt;/td&gt;&lt;/tr&gt;</span>
</span><span class='line'><span class="nt">&lt;tr</span> <span class="na">bgcolor=</span><span class="s">&quot;white&quot;</span><span class="nt">&gt;&lt;td&gt;</span>ADAM<span class="nt">&lt;/td&gt;&lt;td&gt;</span>0.259<span class="nt">&lt;/td&gt;&lt;td&gt;</span> 377,871 <span class="nt">&lt;/td&gt;&lt;td&gt;</span>69<span class="nt">&lt;/td&gt;&lt;/tr&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Regular right?  It&#8217;s so darn simple we can use some simple command line tools to
clean it up:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='html'><span class='line'>cat female_names_alpha.htm | grep &#39;<span class="nt">&lt;tr</span> <span class="na">bgcolor=</span><span class="s">&quot;&#39; | sed &quot;</span><span class="na">s</span><span class="err">/.*&lt;</span><span class="na">td</span><span class="nt">&gt;</span>\([A-Z]\+\).*/\1/&quot; | tail -n +2 &gt; female_names.txt
</span></code></pre></td></tr></table></div></figure>


<p>If we downloaded the female names into <code>female_names_alpha.htm</code>, the first part
will simply print out the contents of the html page to standard out.  The second
part, the grep command, will catch all rows in the table, which thankfully have
the same start prefix.  Part three extracts the names nestled in a row element.
And finally the tail part eliminates the first row of the table, e.g. the head
row, which grep accidentally catches.  This&#8217;ll grab us a ton of names.</p>

<p>So what&#8217;s next?  Well, we have a bunch of male and female names.  But we&#8217;ve got
too many, namely we have names that clearly won&#8217;t work as they have too many or
too few letters.  So let&#8217;s so some more cleaning, but this time in scala:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">maleNames</span> <span class="k">=</span> <span class="nc">Source</span><span class="o">.</span><span class="n">fromFile</span><span class="o">(</span><span class="n">args</span><span class="o">(</span><span class="mi">0</span><span class="o">)).</span><span class="n">getLines</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">_</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">4</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>This dandy line reads the text from the file and filters out any lines that have
more than four characters.  Since our initial processing placed each name on a
line, this in effect removes any names with more than four letters.  If we do
this with the female names, we&#8217;ll have lists of common four letter names for the
two genders.  For later use, we&#8217;ll also turn the female names into a set by
calling <code>.toSet</code> on the list.</p>

<p>Last, we can do the transformation on each male name to see if it&#8217;s a valid
female name and then print out any combinations that match.  The next three
lines do this slickly:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">maleNames</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">w</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">w</span><span class="o">,</span> <span class="n">w</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">l</span> <span class="k">=&gt;</span> <span class="o">(((</span><span class="n">l</span><span class="o">-</span><span class="-Symbol">&#39;A</span><span class="err">&#39;</span><span class="o">+</span><span class="mi">13</span><span class="o">)</span> <span class="o">%</span> <span class="mi">26</span><span class="o">)</span> <span class="o">+</span> <span class="-Symbol">&#39;A</span><span class="err">&#39;</span><span class="o">).</span><span class="n">toChar</span><span class="o">).</span><span class="n">toString</span><span class="o">))</span>
</span><span class='line'>         <span class="o">.</span><span class="n">filter</span><span class="o">(</span> <span class="n">bg</span> <span class="k">=&gt;</span> <span class="n">femaleNames</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="n">bg</span><span class="o">.</span><span class="n">_2</span><span class="o">))</span>
</span><span class='line'>         <span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>This works in three steps.  First, the <code>map</code> call transforms each male name into
a <code>tuple</code> which consists of the original male name, and then a <em>potential</em>
female name by sliding up each letter in the name by 13 letters.  With ascii
characters, this is done just by first grounding the letter to 0, by subtracting
the char value for <code>A</code>, adding 13, and then rolling over the value to be back
in the range of the 26 letter characters by taking the modulo (<code>%</code>) and finally
bumping the character value into the range of real ascii characters by adding
<code>A</code>.   That&#8217;s all done in the first line.  Step two is to simply throw out any
generated female names that don&#8217;t show up in our list of female names, done by
line 3.  And the final line just prints out any matches we get.  With our name
lists, this turns out to be:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="o">(</span><span class="nc">ERIN</span><span class="o">,</span><span class="nc">REVA</span><span class="o">)</span>
</span><span class='line'><span class="o">(</span><span class="nc">EVAN</span><span class="o">,</span><span class="nc">RINA</span><span class="o">)</span>
</span><span class='line'><span class="o">(</span><span class="nc">GLEN</span><span class="o">,</span><span class="nc">TYRA</span><span class="o">)</span>
</span><span class='line'><span class="o">(</span><span class="nc">IVAN</span><span class="o">,</span><span class="nc">VINA</span><span class="o">)</span>
</span><span class='line'><span class="o">(</span><span class="nc">RYAN</span><span class="o">,</span><span class="nc">ELNA</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>The rules of the game stipulated that the male name had to have one syllable and
the female name has to have two.  We didn&#8217;t code for this at all since that
parts slightly more complicated, and since our pre-processing reduced the set of
options down to five, we can easily pick out a valid answer.  In this case,
&#8220;Glen&#8221; and &#8220;Tyra&#8221; or &#8220;Ryan&#8221; and &#8220;Elna&#8221; look like valid responses.  I&#8217;d vote for
the first pair since I haven&#8217;t heard &#8220;Elna&#8221; in forever.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Making Agglomerative Clustering run in N^2 Time]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/03/16/making-agglomerative-clustering-run-in-n-2-time/"/>
    <updated>2012-03-16T17:21:00+09:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/03/16/making-agglomerative-clustering-run-in-n-2-time</id>
    <content type="html"><![CDATA[<p>Let&#8217;s say you&#8217;re an amateur zoologist and you&#8217;ve got a bunch of data describing
<a href="http://en.wikipedia.org/wiki/Owl">parliaments of owls</a>, <a href="http://en.wikipedia.org/wiki/Goose">gaggles of geese</a>, <a href="http://en.wikipedia.org/wiki/Chicken">peeps of chickens</a>, and a
<a href="http://en.wikipedia.org/wiki/Jackdaw">train of jackdaws</a> but you don&#8217;t really know that you have these bird types.
All you really have are descriptive features describing each bird like feature
type, beak type, conservation status, feeding preferences, etc.   Using just
this data, you&#8217;d like to find out how many bird species you have and how similar
each group is to the others in a nice graphical fashion like down below.  How
would you do it?</p>

<p><img class="center" src="http://fozziethebeat.github.com/images/circularDendrogram.png"></p>

<p>The classic solution to the problem would be to use <a href="http://en.wikipedia.org/wiki/Hierarchical_clustering">Hierarchical Agglomerative
Clustering</a>.  In this case, Agglomerative Clustering would group together
birds that have similar features into hopefully distinct species of birds.  And
there&#8217;s a lot of packages out there that kind of do this for you like <a href="http://www.cs.waikato.ac.nz/ml/weka/">Weka</a>,
<a href="http://scikit-learn.org/stable/">Scikit-Learn</a>, or plain old <a href="http://www.statmethods.net/advstats/cluster.html">R</a>.  However, you&#8217;ve got a <em><em>lot</em></em> of birds
to deal with and these standard packages are just taking <em><em>way</em></em> too long.  Why
are they taking way too long?  Because they&#8217;re doing agglomerative clustering
the slow ways.</p>

<p>So what do they slow ways look like?  Well, pretty much all ways of doing
agglomerative clustering start by building an Affinity Matrix that simply
measures how similar two birds are to each other:</p>

<figure class='code'><figcaption><span>Building the Affinity Matrix</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">m</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">.</span><span class="n">fill</span><span class="o">(</span><span class="n">numBirds</span><span class="o">,</span> <span class="n">numBirds</span><span class="o">)(</span><span class="mi">0</span><span class="o">)</span>
</span><span class='line'><span class="k">for</span> <span class="o">((</span><span class="n">bird1</span><span class="o">,</span> <span class="n">i</span><span class="o">)</span> <span class="k">&lt;-</span> <span class="n">birds</span><span class="o">.</span><span class="n">zipWithIndex</span><span class="o">;</span>
</span><span class='line'>     <span class="o">(</span><span class="n">bird2</span><span class="o">,</span> <span class="n">j</span><span class="o">)</span> <span class="k">&lt;-</span> <span class="n">birds</span><span class="o">.</span><span class="n">zipWithIndex</span><span class="o">)</span>
</span><span class='line'>    <span class="n">m</span><span class="o">(</span><span class="n">i</span><span class="o">)(</span><span class="n">j</span><span class="o">)</span> <span class="k">=</span> <span class="n">similarityBetween</span><span class="o">(</span><span class="n">bird1</span><span class="o">,</span> <span class="n">bird2</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>This little snippet just compares each bird against all other birds and stores
how &#8220;similar&#8221; they are to each other in terms of their descriptive features.
What next?  Well, you need another data structure to keep track of your bird
groups.  We&#8217;ll do this with just a map from group identifiers to sets of bird
identifiers.  And since this algorithm is named &#8220;agglomerative&#8221;, we gotta
agglomerate things, so we&#8217;ll start by putting every bird in their own bird
group:</p>

<figure class='code'><figcaption><span>Starting off the bird groups</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>    <span class="c1">// First get each bird and it&#39;s id, then create a tuple holding the bird id</span>
</span><span class='line'>    <span class="c1">// and a set with the bird as the only element.  Then turn this list of</span>
</span><span class='line'>    <span class="c1">// tuples into a map.</span>
</span><span class='line'>    <span class="k">var</span> <span class="n">groupMap</span> <span class="k">=</span> <span class="n">birds</span><span class="o">.</span><span class="n">zipWithIndex</span><span class="o">.</span><span class="n">map</span><span class="o">(</span> <span class="n">birdIndex</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">birdIndex</span><span class="o">.</span><span class="n">_2</span><span class="o">,</span> <span class="nc">Set</span><span class="o">(</span><span class="n">birdIndex</span><span class="o">.</span><span class="n">_1</span><span class="o">))).</span><span class="n">toMap</span>
</span></code></pre></td></tr></table></div></figure>


<p>So that was simple.  Now comes the complicated bits that either lead you to a
slow version, a fairly slow version, or a <em>super</em> slow version of agglomerative
clustering.  Before we do either of these options, let&#8217;s make two
simplifications: let&#8217;s assume we just want the final groups that our bird show
up in and we know how many bird groups we want to find.  Agglomerative
Clustering can give you not only this information, but a whole tree showing how
birds are linked together, but the book-keeping for doing this is tricky and
doesn&#8217;t really affect the issues we&#8217;re focusing on here.  With that out of the
way, what does the <em>super</em> slow method look like?</p>

<figure class='code'><figcaption><span>The Super Slow Agglomerative Clustering</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">while</span> <span class="o">(</span><span class="n">groupMap</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="n">numClusters</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">groupSet</span> <span class="k">=</span> <span class="n">groupMap</span><span class="o">.</span><span class="n">toSet</span>
</span><span class='line'>    <span class="c1">// Find the two most similar groups in our map.</span>
</span><span class='line'>    <span class="k">var</span> <span class="n">bestSim</span> <span class="k">=</span> <span class="mf">0.0</span>
</span><span class='line'>    <span class="k">var</span> <span class="n">bestGroups</span> <span class="k">=</span> <span class="o">(-</span><span class="mi">1</span><span class="o">,</span> <span class="o">-</span><span class="mi">1</span><span class="o">)</span>
</span><span class='line'>    <span class="k">for</span> <span class="o">(</span><span class="nc">Array</span><span class="o">((</span><span class="n">id1</span><span class="o">,</span> <span class="n">birds1</span><span class="o">),</span> <span class="o">(</span><span class="n">id2</span><span class="o">,</span> <span class="n">birds2</span><span class="o">))</span> <span class="k">&lt;-</span> <span class="n">groupSet</span><span class="o">.</span><span class="n">subsets</span><span class="o">(</span><span class="mi">2</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">_</span><span class="o">.</span><span class="n">toArray</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>        <span class="c1">// Get the similarity between the two bird groups using the raw</span>
</span><span class='line'>        <span class="c1">// values in m.</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">sim</span> <span class="k">=</span> <span class="n">groupSim</span><span class="o">(</span><span class="n">id1</span><span class="o">,</span> <span class="n">birds1</span><span class="o">,</span> <span class="n">id2</span><span class="o">,</span> <span class="n">birds2</span><span class="o">,</span> <span class="n">m</span><span class="o">)</span>
</span><span class='line'>        <span class="k">if</span> <span class="o">(</span><span class="n">sim</span> <span class="o">&gt;</span> <span class="n">bestSim</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>            <span class="n">bestSim</span> <span class="k">=</span> <span class="n">sim</span>
</span><span class='line'>            <span class="n">bestGroups</span> <span class="k">=</span> <span class="o">(</span><span class="n">id1</span><span class="o">,</span> <span class="n">id2</span><span class="o">)</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// Now merge the two groups together into a new group</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">newGroup</span> <span class="k">=</span> <span class="n">groupMap</span><span class="o">(</span><span class="n">bestGroups</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span> <span class="o">++</span> <span class="n">groupMap</span><span class="o">(</span><span class="n">bestGroups</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
</span><span class='line'>    <span class="c1">// Now remove the two groups from the map</span>
</span><span class='line'>    <span class="n">groupMap</span> <span class="k">=</span> <span class="n">groupMap</span> <span class="o">-</span> <span class="n">bestGroups</span><span class="o">.</span><span class="n">_1</span>
</span><span class='line'>    <span class="n">groupMap</span> <span class="k">=</span> <span class="n">groupMap</span> <span class="o">-</span> <span class="n">bestGroups</span><span class="o">.</span><span class="n">_2</span>
</span><span class='line'>    <span class="c1">// Update the map to store the new group.  Since the old id&#39;s are</span>
</span><span class='line'>    <span class="c1">// removed, we can just re-use one of them without any change.</span>
</span><span class='line'>    <span class="n">groupMap</span> <span class="k">=</span> <span class="n">groupMap</span> <span class="o">++</span> <span class="nc">Map</span><span class="o">((</span><span class="n">bestGroups</span><span class="o">.</span><span class="n">_1</span> <span class="o">-&gt;</span> <span class="n">newGroup</span><span class="o">))</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>That&#8217;s it!  It&#8217;s <em>super</em> simple and <em>super</em> slow.  Why is it so slow?  Well, in
general, you&#8217;ll do the big while loop <code>O(N)</code> times, where N is your number of
birds.  Then the next block of code compares each possible pairing of bird
groups.  Since in general the number of groups is proportional to the number of
birds, this will be <code>O(N^2)</code> comparisons.  Throw these two bits together
and you get a runtime of <code>O(N3)</code>!  When you have 100,000 birds,
<em>that&#8217;s</em> <strong>super</strong> slow.</p>

<p>So what can you do to hasten that up?  Well, the main goal of the big loop
through pairs of groups is to find the most <em>similar</em> pair of bird groups, so an
obvious choice would be to create a priority queue, so let&#8217;s see how that looks:</p>

<figure class='code'><figcaption><span>Agglomerative Clustering, Priority Queue Style</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// Create a new priority queue that tracks the similarity of two groups and</span>
</span><span class='line'><span class="c1">// their id&#39;s</span>
</span><span class='line'><span class="k">val</span> <span class="n">groupQueue</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">PriorityQueue</span><span class="o">[(</span><span class="kt">double</span>, <span class="kt">int</span>, <span class="kt">int</span><span class="o">)]()</span>
</span><span class='line'><span class="k">val</span> <span class="n">groupSet</span> <span class="k">=</span> <span class="n">groupMap</span><span class="o">.</span><span class="n">toSet</span>
</span><span class='line'><span class="k">for</span> <span class="o">(</span><span class="nc">Array</span><span class="o">((</span><span class="n">id1</span><span class="o">,</span> <span class="n">birds1</span><span class="o">),</span> <span class="o">(</span><span class="n">id2</span><span class="o">,</span> <span class="n">birds2</span><span class="o">))</span> <span class="k">&lt;-</span> <span class="n">groupMap</span><span class="o">.</span><span class="n">subsets</span><span class="o">(</span><span class="mi">2</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">_</span><span class="o">.</span><span class="n">toArray</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>    <span class="c1">// Get the similarity between the two bird groups using the raw</span>
</span><span class='line'>    <span class="c1">// values in m.</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">sim</span> <span class="k">=</span> <span class="n">groupSim</span><span class="o">(</span><span class="n">id1</span><span class="o">,</span> <span class="n">birds1</span><span class="o">,</span> <span class="n">id2</span><span class="o">,</span> <span class="n">birds2</span><span class="o">,</span> <span class="n">m</span><span class="o">)</span>
</span><span class='line'>    <span class="n">groupQueue</span><span class="o">.</span><span class="n">enque</span><span class="o">((</span><span class="n">sim</span><span class="o">,</span> <span class="n">id1</span><span class="o">,</span> <span class="n">id2</span><span class="o">))</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="k">var</span> <span class="n">nextId</span> <span class="k">=</span> <span class="n">groupMap</span><span class="o">.</span><span class="n">size</span>
</span><span class='line'><span class="k">while</span> <span class="o">(</span><span class="n">groupMap</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="n">numClusters</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">var</span> <span class="n">best</span> <span class="k">=</span> <span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="o">-</span><span class="mi">1</span><span class="o">,</span> <span class="o">-</span><span class="mi">1</span><span class="o">)</span>
</span><span class='line'>    <span class="k">do</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">best</span> <span class="k">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dequeue</span>
</span><span class='line'>    <span class="o">}</span> <span class="k">while</span> <span class="o">(</span><span class="n">groupMap</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="n">best</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span> <span class="o">&amp;&amp;</span> <span class="n">groupMap</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="n">best</span><span class="o">.</span><span class="n">_3</span><span class="o">))</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// Now merge the two groups together into a new group</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">newGroup</span> <span class="k">=</span> <span class="n">groupMap</span><span class="o">(</span><span class="n">best</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span> <span class="o">++</span> <span class="n">groupMap</span><span class="o">(</span><span class="n">best</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
</span><span class='line'>    <span class="c1">// Now remove the two groups from the map</span>
</span><span class='line'>    <span class="n">groupMap</span> <span class="k">=</span> <span class="n">groupMap</span> <span class="o">-</span> <span class="n">best</span><span class="o">.</span><span class="n">_1</span>
</span><span class='line'>    <span class="n">groupMap</span> <span class="k">=</span> <span class="n">groupMap</span> <span class="o">-</span> <span class="n">best</span><span class="o">.</span><span class="n">_2</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// Create a new id for this group.</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">newId</span> <span class="k">=</span> <span class="n">nextId</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// Next, add in the similarity between the new group and all existing</span>
</span><span class='line'>    <span class="c1">// groups to the queue.</span>
</span><span class='line'>    <span class="k">for</span> <span class="o">(</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">group</span><span class="o">)</span> <span class="k">&lt;-</span> <span class="n">groupMap</span> <span class="o">)</span>
</span><span class='line'>        <span class="n">groupQueue</span><span class="o">.</span><span class="n">enqueue</span><span class="o">((</span><span class="n">groupSim</span><span class="o">(</span><span class="n">newId</span><span class="o">,</span> <span class="n">newGroup</span><span class="o">,</span> <span class="n">id</span><span class="o">,</span> <span class="n">group</span><span class="o">,</span> <span class="n">m</span><span class="o">),</span>
</span><span class='line'>                            <span class="n">newId</span><span class="o">,</span> <span class="n">id</span><span class="o">))</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// Finally, update the map to store the new group.</span>
</span><span class='line'>    <span class="n">nextId</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class='line'>    <span class="n">groupMap</span> <span class="k">=</span> <span class="n">groupMap</span> <span class="o">++</span> <span class="o">(</span><span class="n">newId</span><span class="o">-&gt;</span> <span class="n">newGroup</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now this approach is definitely <em>faster</em> but it&#8217;s also horribly memory
inefficient.  Even if you have an array based priority queue that doesn&#8217;t
allocate any extra memory for each entry other than the tuple being stored, this
approach has one major problem: after every merge step <code>O(N)</code> elements
immediately become invalidated.  Since the two merged groups no longer exist,
any comparison between them and other groups is moot.  However the priority
queue doesn&#8217;t easily permit removing them so they just float around.  This is
precisely why lines 14-16 are in a do while loop that ensures the returned
groups exist.  It&#8217;s highly likely that they wont.  I think we can do better,
both in terms of speed and in terms of memory.  So let&#8217;s blaze through a faster
method in couple of steps.</p>

<p>First, we need to change the setup of the algorithm.  The previous two method
just depended on some simple data structures.  This approach requires two
additional structures for bookkeeping: a way to track chains of nearest
neighbors and a way to track the set of clusters not already a part of the
chain.</p>

<figure class='code'><figcaption><span>Agglomerative Clustering, Blazingly Fast Style Step 1</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// A mapping from cluster id&#39;s to their point sets.</span>
</span><span class='line'><span class="k">val</span> <span class="n">clusterMap</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">HashMap</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">HashSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]]()</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// A set of clusters to be considered for merging.</span>
</span><span class='line'><span class="k">val</span> <span class="n">remaining</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">HashSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]()</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Create a cluster for every data point and add it to the cluster map</span>
</span><span class='line'><span class="c1">// and to the examine set.</span>
</span><span class='line'><span class="k">for</span> <span class="o">(</span><span class="n">r</span> <span class="k">&lt;-</span> <span class="mi">0</span> <span class="n">until</span> <span class="n">adj</span><span class="o">.</span><span class="n">size</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">remaining</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">r</span><span class="o">)</span>
</span><span class='line'>    <span class="n">clusterMap</span><span class="o">(</span><span class="n">r</span><span class="o">)</span> <span class="k">=</span> <span class="nc">HashSet</span><span class="o">(</span><span class="n">r</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Create a stack to represent the nearest neighbor.  The real source of</span>
</span><span class='line'><span class="c1">// matic</span>
</span><span class='line'><span class="k">val</span> <span class="n">chain</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Stack</span><span class="o">[(</span><span class="kt">Double</span>, <span class="kt">Int</span><span class="o">)]()</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Add in a random node from remaining to start the neighbor chain.</span>
</span><span class='line'><span class="n">initializeChain</span><span class="o">(</span><span class="n">chain</span><span class="o">,</span> <span class="n">remaining</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>The <code>clusterMap</code> structure replaces our <code>birdMap</code> but does the same thing, but
the remaining set and the chain stack hold the crux of this approach.  Instead
of trying to find the best link between two clusters, we&#8217;re going to depend on
chains of nearest neighbors, so the first node in the chain could be anything,
but the second node is simply the nearest neighbor to the first node.  To add
the third node, we find the nearest neighbor out of any nodes not in the chain.
We&#8217;ll keep doing this until two nodes in the chain represent reciprocal nearest
neighbors, that is two nodes that are most similar to each other, and no other
nodes in or outside of the chain.  Upon finding these two nodes, we merge them,
immediately.  Then we just repeat the process until we have the desired number
of clusters.  In scala, this turns out to be pretty simple to do:</p>

<figure class='code'><figcaption><span>Agglomerative Clustering, Blazingly Fast Style Step 2</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// Find the nearest neighbors and merge as soon as recursive nearest</span>
</span><span class='line'><span class="c1">// neighbors are found.</span>
</span><span class='line'><span class="k">while</span> <span class="o">(</span><span class="n">clusterMap</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="n">numClusters</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="c1">// Get the last link in the chain.</span>
</span><span class='line'>    <span class="k">val</span> <span class="o">(</span><span class="n">parentSim</span><span class="o">,</span> <span class="n">current</span><span class="o">)</span> <span class="k">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">top</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// Find the nearest neighbor using the clusters not in the chain</span>
</span><span class='line'>    <span class="c1">// already.</span>
</span><span class='line'>    <span class="k">val</span> <span class="o">(</span><span class="n">linkSim</span><span class="o">,</span> <span class="n">next</span><span class="o">)</span> <span class="k">=</span> <span class="n">findBest</span><span class="o">(</span><span class="n">remaining</span><span class="o">,</span> <span class="n">adj</span><span class="o">,</span> <span class="n">current</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// Check the similarity for the best neighbor and compare it to that of</span>
</span><span class='line'>    <span class="c1">// the current node in the chain.  If the neighbor sim is larger, then</span>
</span><span class='line'>    <span class="c1">// the current node and it&#39;s parent aren&#39;t RNNs.  Otherwise, the current</span>
</span><span class='line'>    <span class="c1">// node is RNNs with it&#39;s parent.</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(</span><span class="n">linkSim</span> <span class="o">&gt;</span> <span class="n">parentSim</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="c1">// The current node to push is more similar to the last node in the</span>
</span><span class='line'>        <span class="c1">// chain, so the last node and the next to last nodes can&#39;t be</span>
</span><span class='line'>        <span class="c1">// reciprocal nearest neighbors.</span>
</span><span class='line'>        <span class="n">chain</span><span class="o">.</span><span class="n">push</span><span class="o">((</span><span class="n">linkSim</span><span class="o">,</span> <span class="n">next</span><span class="o">))</span>
</span><span class='line'>        <span class="n">remaining</span><span class="o">.</span><span class="n">remove</span><span class="o">(</span><span class="n">next</span><span class="o">)</span>
</span><span class='line'>    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
</span><span class='line'>        <span class="c1">// The current node is less similar to the last node than the last</span>
</span><span class='line'>        <span class="c1">// node is to it&#39;s predecesor in the chain, so the last two nodes in</span>
</span><span class='line'>        <span class="c1">// the chain are just the kind of nodes we&#39;re looking to merge.</span>
</span><span class='line'>
</span><span class='line'>        <span class="c1">// Pop the current node from the top. </span>
</span><span class='line'>        <span class="n">chain</span><span class="o">.</span><span class="n">pop</span>
</span><span class='line'>        <span class="c1">// Pop the parent of the best node.</span>
</span><span class='line'>        <span class="k">val</span> <span class="o">(</span><span class="n">_</span><span class="o">,</span> <span class="n">parent</span><span class="o">)</span> <span class="k">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">pop</span>
</span><span class='line'>
</span><span class='line'>        <span class="c1">// These are the two nodes we&#39;ll be merging.  The node we</span>
</span><span class='line'>        <span class="c1">// found above is left in the remaining set and is essentially</span>
</span><span class='line'>        <span class="c1">// forgotten.</span>
</span><span class='line'>
</span><span class='line'>        <span class="c1">// Remove the current and parent clusters from the cluster map</span>
</span><span class='line'>        <span class="c1">// and extract the sizes.</span>
</span><span class='line'>        <span class="k">val</span> <span class="o">(</span><span class="n">c1Points</span><span class="o">,</span> <span class="n">c1Size</span><span class="o">)</span> <span class="k">=</span> <span class="n">removeCluster</span><span class="o">(</span><span class="n">clusterMap</span><span class="o">,</span> <span class="n">current</span><span class="o">)</span>
</span><span class='line'>        <span class="k">val</span> <span class="o">(</span><span class="n">c2Points</span><span class="o">,</span> <span class="n">c2Size</span><span class="o">)</span> <span class="k">=</span> <span class="n">removeCluster</span><span class="o">(</span><span class="n">clusterMap</span><span class="o">,</span> <span class="n">parent</span><span class="o">)</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">total</span> <span class="k">=</span> <span class="n">c1Size</span> <span class="o">+</span> <span class="n">c2Size</span>
</span><span class='line'>
</span><span class='line'>        <span class="c1">// Update the similarity between the new merged cluster and all</span>
</span><span class='line'>        <span class="c1">// other existing clusters.  </span>
</span><span class='line'>        <span class="k">for</span> <span class="o">(</span><span class="n">key</span> <span class="k">&lt;-</span> <span class="n">clusterMap</span><span class="o">.</span><span class="n">keys</span><span class="o">)</span>
</span><span class='line'>            <span class="n">adj</span><span class="o">(</span><span class="n">current</span><span class="o">)(</span><span class="n">key</span><span class="o">)</span> <span class="k">=</span> <span class="n">updatedSimilarity</span><span class="o">(</span>
</span><span class='line'>                <span class="n">adj</span><span class="o">,</span> <span class="n">current</span><span class="o">,</span> <span class="n">c1Size</span><span class="o">,</span> <span class="n">parent</span><span class="o">,</span> <span class="n">c2Size</span><span class="o">,</span> <span class="n">key</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>        <span class="c1">// Replace the mapping from current to now point to the merged</span>
</span><span class='line'>        <span class="c1">// cluster and add current back into the set of remaining</span>
</span><span class='line'>        <span class="c1">// clusters so that it&#39;s compared to nodes in the chain.</span>
</span><span class='line'>        <span class="n">clusterMap</span><span class="o">(</span><span class="n">current</span><span class="o">)</span> <span class="k">=</span> <span class="n">c1Points</span> <span class="o">++</span> <span class="n">c2Points</span>
</span><span class='line'>        <span class="n">remaining</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">current</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>        <span class="c1">// If the chain is now empty, re-initialize it.</span>
</span><span class='line'>        <span class="k">if</span> <span class="o">(</span><span class="n">chain</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="o">)</span>
</span><span class='line'>            <span class="n">initializeChain</span><span class="o">(</span><span class="n">chain</span><span class="o">,</span> <span class="n">remaining</span><span class="o">)</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>And that&#8217;s it!  By focusing on reciprocal nearest neighbors, the algorithm
merges together clusters that will always be merged, no matter how you find
them.  Furthermore, it&#8217;s remarkably easy and fast to find these nodes.  By
building the chain, the number of things that can go on the chain gets smaller
and smaller.</p>

<p>Oh, but there&#8217;s one other magic trick to making this super fast, and it depends
on how you compute <code>updateSimilarity</code>.  The silly way to do it would be to
traverse all the pairings between nodes in the new cluster and the each other
remaining cluster, but that in itself gets really slow as the clusters grow.
But rather than doing that, <a href="http://onlinelibrary.wiley.com/doi/10.1002/widm.53/full">these folks</a> suggest some recurrence relations
that can be computed in <em>constant time</em>, for any of the existing agglomerative
criteria methods.  Crazy right?  But just crazy enough to work correctly and be
too fast to believe.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Moral Dilemmas]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/03/04/moral-dilemmas/"/>
    <updated>2012-03-04T19:03:00+09:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/03/04/moral-dilemmas</id>
    <content type="html"><![CDATA[<p>What is the nature of a moral dilemma?  How moral can any dilemma be?  Are some
dilemmas inherently moral free or can any problem be cast into some level of a
moral quandary? And if that&#8217;s true, how do you get to this quandary state and
where do you go from there?</p>

<p>These questions have been bubbling around in my mind for the past couple months,
ever since I read a <a href="http://www.nytimes.com/2011/09/13/opinion/if-it-feels-right.html">New York Times Op-Ed post</a> about moral dilemmas and a new
book called <a href="http://www.amazon.com/Lost-Transition-Dark-Emerging-Adulthood/dp/0199828024">&#8220;Lost in Transition&#8221;</a>.  In the NYTimes post, the David Brooks notes
that the authors of &#8220;Lost in Transition&#8221; found that modern high school students
are unequipped to think about things in moral terms.  For instance, The authors
posed some questions to students and said the following:</p>

<blockquote><p>Not many of them have previously given much or any thought to many of the kinds of questions about morality that we asked.</p></blockquote>


<p>I haven&#8217;t read the book in detail, so I can&#8217;t really comment on their full
findings, proposals, or methodology, so I&#8217;m not about to comment on that now.
Instead, I got to asking myself the many questions up above.  In particular, I
wanted to know how I could pose daily activities as moral dilemmas.  I don&#8217;t
often put things into directly moral terms, but I was a Teaching Assistant for
a class focused on Ethics in Engineering, so I <em>should</em> be equipped to do some
pretty solid moral analysis if I apply myself.</p>

<p>So, I&#8217;d like to try and break down some seemingly non-moral problems in totally
moral terms.  And to start, let&#8217;s go with <em>Doing the dishes</em>.</p>

<p>Now, normally, you might look at the task of dishes and think that it&#8217;s just
something you have to do.  Often it&#8217;s a little boring, or even a little
unpleasant based on your cooking forays, but it&#8217;s not too often that people feel
morally <em>conflicted</em> about when and whether or not they do their dishes.  But if
you live with other people and share a common cooking space, I would argue that
it is inherently a moral issue.  Namely, it&#8217;s an issue about acting with empathy.</p>

<p>According to Wikipedia, the world&#8217;s coolest encyclopedia, <em>empathy</em> is defined
as</p>

<blockquote><p>the capacity to recognize and, to some extent, share feelings (such as<br/>    sadness or happiness) that are being experienced by another sentient or<br/>    semi-sentient being</p></blockquote>


<p>So how does empathy relate <em>at all</em> to doing the dishes? Well, by living in an
apartment shared with other people, your actions implicitly affect the
livelihood and wellbeing of others.  So let&#8217;s break down the task of dishes into
two aspects: your choice of actions and the possible affects it can have on
others.   When it comes to doing dishes, there&#8217;s really two choices you can
make:</p>

<ol>
<li>do the dishes</li>
<li>don&#8217;t do the dishes</li>
</ol>


<p>And everyone in the house will likely have one of two reactions to dirty dishes:</p>

<ol>
<li>dirty dishes are not a problem</li>
<li>dirty dishes are frustrating</li>
</ol>


<p>With regards to clean dishes, it&#8217;s fairly safe to assume that everyone enjoys
and appreciates clean dishes, i&#8217;ve certainly not met anyone bothered by a clean
plate or glass.  So how do these reactions connect to whether or not you
should make those dishes sparkly clean?  Well, by leaving dirty dishes you&#8217;re
doing one of two things:</p>

<ol>
<li>you&#8217;re assuming that everyone is ok with dirty dishes</li>
<li>you don&#8217;t about how others feel about dirty dishes</li>
</ol>


<p>I would argue that both cases are a failure in empathy.  By assuming that others
share your same reaction to doing the dishes, you&#8217;re failing to recognize that
others in the house may feel otherwise (first half of our empathy definition).
If for instance, a roommate has a horrible case of OCD and emotionally breakdown
upon the sight of food encrusted dishes, failing to do the dishes may make this
particular person deeply wounded.  And by simply ignoring how other&#8217;s feel about
dishes (the second, sharing, half of empathy), even if you recognize that they
may feel differently, you&#8217;re actively choosing not to share those feelings.</p>

<p>What happens if you <em>do</em> clean the dishes and leave things sparkly, fresh, and
lemon scented?  You&#8217;re still making an assumption about how other&#8217;s feel about
dishes, but you&#8217;re making the most conservative assumption you can; most people
prefer clean dishes so by cleaning you&#8217;re <em>less</em> likely to cause any emotional
breakdowns, or in more normal cases, mild frustration.  Assuming that you like
to be happy, doing the dishes is the only <em>empathy</em> filled action available, it
makes the fewest and most conservative assumptions about what makes others happy.</p>

<p>So that&#8217;s one way to break down the task of dishes into a moral dilemma.  So the
next time you make a mess in the kitchen and ask yourself &#8220;should I clean
those?&#8221;  You can make a handy debate about it.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Morphological Analysis made easy with Scala]]></title>
    <link href="http://fozziethebeat.github.com/blog/2012/02/29/morphological-analysis-made-easy-with-scala/"/>
    <updated>2012-02-29T19:49:00+09:00</updated>
    <id>http://fozziethebeat.github.com/blog/2012/02/29/morphological-analysis-made-easy-with-scala</id>
    <content type="html"><![CDATA[<p>Ever since I got involved with distributional semantics, i&#8217;ve been perplexed
about how to handle morphed words, which happens to be just about every noun and
verb in English.  What is a morphed word in English you ask?  It&#8217;s pretty much
any word that&#8217;s been changed to reflect things like past tense, plurality,
ownership, and all those things.  They&#8217;re conjugated verbs and more!  But they
always pose a massive problem in distributional semantics.</p>

<p>Think about it for a second, what&#8217;s the big difference between &#8220;cat&#8221; and &#8220;cats&#8221;?
Not too much, the second is simply saying there are multiple occurrences of a
&#8220;cat&#8221;.  But what do people typically do for distributional semantics?  One of
two bad options: leave the two words as separate things and thus split the
information gained about &#8220;cat&#8221; across two words or stem every word and throw
away something important like the multitude of &#8220;cat&#8221;.  Both seem totally wrong
and unsatisfactory.</p>

<p>So on and off I&#8217;ve searched for a good tool to do morphological analysis for
English.  Ideally, the analyzer could recognize the word &#8220;cats&#8221; and split into
two things: <code>cat</code> and <code>&lt;p&gt;</code> that way you can retain all your information about a
<code>cat</code> and still know that there were a multitude of them when you see &#8220;cats&#8221;.
And for verbs?  You should see the same thing, <code>ran</code> could then become something
like <code>run</code> and <code>&lt;past&gt;</code>&#8221; so that again, you still know everything about running and that
it happened in the past.  But until today, I&#8217;ve never found a tool that does
this both quickly and easily in a usable language.</p>

<p>That all changes today.  Today I found the <a href="http://wiki.apertium.org/wiki/Lttoolbox-java">Lltool-box for Java</a>.  It creates
a large finite state machine for recognizing morphed words and figuring out the
root word, i.e. &#8220;run&#8221; in &#8220;ran&#8221; and &#8220;cat&#8221; in &#8220;cats&#8221;, and how the word was
morphed, i.e. &#8220;run&#8221; for &#8220;ran&#8221; and &#8220;cat&#8221; for &#8220;cats&#8221;.  All you have to do is get a
listing of how words are morphed, load it up until the <a href="http://wiki.apertium.org/wiki/Lttoolbox-java">Lttoolbox-java</a>
system and analyze away your sentences.</p>

<p>So let&#8217;s take a spin on how to do this.  First, you read the wiki instructions
for <a href="http://wiki.apertium.org/wiki/Lttoolbox-java">Lttoolbox-java</a>, download it, and compile it.  Compiling is easy:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mvn deploy -DskipTests</span></code></pre></td></tr></table></div></figure>


<p>The <code>-DskipTests</code> part seems needed since their unit tests don&#8217;t pass.  But
after that, you can start using the code in your favorite jvm based language.
my personal fave is Scala, so let&#8217;s run with that.  So what next?  Now you
create a <a href="http://en.wikipedia.org/wiki/Finite_state_transducer">Finite State Transducer</a> using a dictionary file and tell it to
analyze words:</p>

<figure class='code'><figcaption><span>Setting up the Finite State Transducer </span><a href='https://gist.github.com/1772752'>Source Gist </a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// Create the Finite State Transducer processor.</span>
</span><span class='line'><span class="k">val</span> <span class="n">fstp</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">FSTProcessor</span><span class="o">()</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Load the finite state transducer with the compiled dictionary file.  </span>
</span><span class='line'><span class="n">fstp</span><span class="o">.</span><span class="n">load</span><span class="o">(</span><span class="n">openInFileStream</span><span class="o">(</span><span class="n">args</span><span class="o">(</span><span class="mi">0</span><span class="o">)))</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Setup the trandsducer to do morphological analysis and make sure it&#39;s valid.</span>
</span><span class='line'><span class="n">fstp</span><span class="o">.</span><span class="n">initAnalysis</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now that you have a transducer ready to analyze words, you just feed it stuff like this:</p>

<figure class='code'><figcaption><span>Loading the transducer with input and output </span><a href='https://gist.github.com/1772752'>Source Gist </a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">in</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StringReader</span><span class="o">(</span><span class="s">&quot;cats, dogs and blubber all running quickly!&quot;</span><span class="o">)</span>
</span><span class='line'><span class="k">val</span> <span class="n">out</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StringWriter</span><span class="o">()</span>
</span><span class='line'><span class="c1">// Do the analysis.</span>
</span><span class='line'><span class="n">fstp</span><span class="o">.</span><span class="n">analysis</span><span class="o">(</span><span class="n">in</span><span class="o">,</span> <span class="n">out</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>You gotta create a reader and writer for their transducer interface.  It&#8217;s
funky, but it still leaves you with a pretty flexible interface.  So now what?
What does the output look like?  If you feed it &#8220;cats, dogs and blubber all
running quickly!&#8221;, the output is pretty ugly at first:</p>

<figure class='code'><figcaption><span>This will never look pretty</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='html'><span class='line'>^cats/cat<span class="nt">&lt;n&gt;&lt;pl&gt;</span>$^,/,<span class="nt">&lt;cm&gt;</span>$
</span><span class='line'>^dogs/dog<span class="nt">&lt;n&gt;&lt;pl&gt;</span>$
</span><span class='line'>^and/and<span class="nt">&lt;cnjcoo&gt;</span>$
</span><span class='line'>^blubber/*blubber$
</span><span class='line'>^all/all<span class="nt">&lt;adj&gt;</span>/all<span class="nt">&lt;adv&gt;</span>/all<span class="nt">&lt;predet&gt;&lt;sp&gt;</span>/all<span class="nt">&lt;det&gt;&lt;qnt&gt;&lt;pl&gt;</span>/all<span class="nt">&lt;det&gt;&lt;qnt&gt;&lt;sp&gt;</span>/all<span class="nt">&lt;prn&gt;&lt;qnt&gt;&lt;mf&gt;&lt;sp&gt;</span>$
</span><span class='line'>^running/run<span class="nt">&lt;vblex&gt;&lt;ger&gt;</span>/run<span class="nt">&lt;vblex&gt;&lt;pprs&gt;</span>/run<span class="nt">&lt;vblex&gt;&lt;subs&gt;</span>/running<span class="nt">&lt;adj&gt;</span>/running<span class="nt">&lt;n&gt;&lt;sg&gt;</span>$
</span><span class='line'>^quickly/quickly<span class="nt">&lt;adv&gt;</span>$
</span></code></pre></td></tr></table></div></figure>


<p>Beastly no? So i figure that it&#8217;s best to write some regular expressions to
handle all of this.  You&#8217;ll need a couple: one to split up words, one to match
analysed words, one for unrecognized words, and one to split up the tags.  In
scala you can do this pretty easily like this:</p>

<figure class='code'><figcaption><span>Regular Expressions for handling output from Lttoolbox-java </span><a href='https://gist.github.com/1772752'>Source Gist </a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// 1: Recognize a fully analyzed word so that they can be tokenized.  In the</span>
</span><span class='line'><span class="c1">// above test case, &quot;cats,&quot; will not be separated by white space so we require</span>
</span><span class='line'><span class="c1">// this more complicated splitting method.</span>
</span><span class='line'><span class="k">val</span> <span class="n">parseRegex</span> <span class="k">=</span> <span class="s">&quot;&quot;&quot;\^.*?\$&quot;&quot;&quot;</span><span class="o">.</span><span class="n">r</span>
</span><span class='line'><span class="c1">// 2: Recognize a word with morphological tags.</span>
</span><span class='line'><span class="k">val</span> <span class="n">morphredRegex</span> <span class="k">=</span> <span class="s">&quot;&quot;&quot;\^(.+?)/(.+?)(&lt;[0-9a-z&lt;&gt;]+&gt;).*\$&quot;&quot;&quot;</span>
</span><span class='line'><span class="c1">// 3: Recognize a word that could not be recognized.  The transducer prepends</span>
</span><span class='line'><span class="c1">// &amp;quot;*&amp;quot; to unrecognized tokens, so we match and eliminate it.</span>
</span><span class='line'><span class="k">val</span> <span class="n">unknownRegex</span> <span class="k">=</span> <span class="s">&quot;&quot;&quot;\^(.+)/\*(.+?)\$&quot;&quot;&quot;</span>
</span><span class='line'><span class="c1">// 4: A regular expression for matching morphological tags.  This is simpler</span>
</span><span class='line'><span class="c1">// than writing a splitting rule.</span>
</span><span class='line'><span class="k">val</span> <span class="n">featureRegex</span> <span class="k">=</span> <span class="s">&quot;&quot;&quot;&lt;.*?&gt;&quot;&quot;&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Then all you need to do is run through the analyzed sentence and split it up
into separate tokens, some for root words and some for morphed features.  You
can do that like this:</p>

<figure class='code'><figcaption><span>Parsing that business Lttoolbox-java </span><a href='https://gist.github.com/1772752'>Source Gist </a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">tokens</span> <span class="k">=</span> <span class="n">parseRegex</span><span class="o">.</span><span class="n">findAllIn</span><span class="o">(</span><span class="n">out</span><span class="o">.</span><span class="n">toString</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">parseMatch</span> <span class="o">=&amp;</span><span class="n">gt</span><span class="o">;</span>
</span><span class='line'><span class="c1">// Match the current analyzed word as being morphed or unknown.  For morphed</span>
</span><span class='line'><span class="c1">// words, create a list of the lemma and the tags.  For unknown words just</span>
</span><span class='line'><span class="c1">// create a list of the lemma.</span>
</span><span class='line'><span class="n">parseMatch</span><span class="o">.</span><span class="n">toString</span> <span class="k">match</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">case</span> <span class="n">morphredRegex</span><span class="o">(</span><span class="n">surface</span><span class="o">,</span> <span class="n">lemma</span><span class="o">,</span> <span class="n">tags</span><span class="o">)</span> <span class="o">=&amp;</span><span class="n">gt</span><span class="o">;</span>
</span><span class='line'>        <span class="n">lemma</span> <span class="o">::</span> <span class="n">featureRegex</span><span class="o">.</span><span class="n">findAllIn</span><span class="o">(</span><span class="n">tags</span><span class="o">).</span><span class="n">toList</span>
</span><span class='line'>    <span class="k">case</span> <span class="n">unknownRegex</span><span class="o">(</span><span class="n">surface</span><span class="o">,</span> <span class="n">lemma</span><span class="o">)</span> <span class="o">=&amp;</span><span class="n">gt</span><span class="o">;</span>
</span><span class='line'>        <span class="nc">List</span><span class="o">(</span><span class="n">lemma</span><span class="o">)</span>
</span><span class='line'><span class="o">}).</span><span class="n">reduceLeft</span><span class="o">(</span><span class="n">_++_</span><span class="o">).</span><span class="n">filter</span><span class="o">(!</span><span class="n">rejectFeatures</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="n">_</span><span class="o">))</span>
</span></code></pre></td></tr></table></div></figure>


<p>This bit of code&#8217;s pretty sweet.  You first iterate over each analyzed word with
the first matcher.  Then you match each word with the two word level regular
expressions: one for fully analyzed words and one for unrecognized words.  After
that it&#8217;s easy smeesy, you just split the tags up with the last regular
expression and turn it all into a list.  The last two bits at the end turn the
whole thing into a single list and lets you filter out tags or tokens you don&#8217;t
want.</p>

<p>So with that, you can now do simple and fast morphological analysis in Java,
Scala, or even Clojure (but who&#8217;d do something silly like that?)!-</p>
]]></content>
  </entry>
  
</feed>
